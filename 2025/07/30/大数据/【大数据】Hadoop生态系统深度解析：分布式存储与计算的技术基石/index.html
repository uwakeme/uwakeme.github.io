<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>【大数据】Hadoop生态系统深度解析：分布式存储与计算的技术基石 | Uwakeme</title><meta name="author" content="Wake"><meta name="copyright" content="Wake"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="一、Hadoop概述与架构（一）Hadoop简介 什么是Hadoop  Apache Hadoop：开源分布式存储和计算框架 设计目标：处理大规模数据集的存储和分析 核心理念：移动计算而非移动数据 容错性：硬件故障是常态而非异常 可扩展性：从单机扩展到数千台机器   Hadoop发展历程  2003年：Google发布GFS和MapReduce论文 2006年：Doug Cutting创建Hado">
<meta property="og:type" content="article">
<meta property="og:title" content="【大数据】Hadoop生态系统深度解析：分布式存储与计算的技术基石">
<meta property="og:url" content="https://uwakeme.tech/2025/07/30/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E3%80%90%E5%A4%A7%E6%95%B0%E6%8D%AE%E3%80%91Hadoop%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%EF%BC%9A%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E4%B8%8E%E8%AE%A1%E7%AE%97%E7%9A%84%E6%8A%80%E6%9C%AF%E5%9F%BA%E7%9F%B3/index.html">
<meta property="og:site_name" content="Uwakeme">
<meta property="og:description" content="一、Hadoop概述与架构（一）Hadoop简介 什么是Hadoop  Apache Hadoop：开源分布式存储和计算框架 设计目标：处理大规模数据集的存储和分析 核心理念：移动计算而非移动数据 容错性：硬件故障是常态而非异常 可扩展性：从单机扩展到数千台机器   Hadoop发展历程  2003年：Google发布GFS和MapReduce论文 2006年：Doug Cutting创建Hado">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png">
<meta property="article:published_time" content="2025-07-29T16:00:00.000Z">
<meta property="article:modified_time" content="2025-07-31T00:57:45.045Z">
<meta property="article:author" content="Wake">
<meta property="article:tag" content="Hadoop">
<meta property="article:tag" content="大数据">
<meta property="article:tag" content="分布式系统">
<meta property="article:tag" content="HDFS">
<meta property="article:tag" content="MapReduce">
<meta property="article:tag" content="YARN">
<meta property="article:tag" content="数据处理">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "【大数据】Hadoop生态系统深度解析：分布式存储与计算的技术基石",
  "url": "https://uwakeme.tech/2025/07/30/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E3%80%90%E5%A4%A7%E6%95%B0%E6%8D%AE%E3%80%91Hadoop%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%EF%BC%9A%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E4%B8%8E%E8%AE%A1%E7%AE%97%E7%9A%84%E6%8A%80%E6%9C%AF%E5%9F%BA%E7%9F%B3/",
  "image": "https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png",
  "datePublished": "2025-07-29T16:00:00.000Z",
  "dateModified": "2025-07-31T00:57:45.045Z",
  "author": [
    {
      "@type": "Person",
      "name": "Wake",
      "url": "https://uwakeme.tech/about/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/avatar.jpeg"><link rel="canonical" href="https://uwakeme.tech/2025/07/30/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E3%80%90%E5%A4%A7%E6%95%B0%E6%8D%AE%E3%80%91Hadoop%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%EF%BC%9A%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E4%B8%8E%E8%AE%A1%E7%AE%97%E7%9A%84%E6%8A%80%E6%9C%AF%E5%9F%BA%E7%9F%B3/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?f96f39b60e7cc4fbcbd971a88f91f326";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
btf.addGlobalFn('pjaxComplete', () => {
  _hmt.push(['_trackPageview',window.location.pathname])
}, 'baidu_analytics')
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/local-search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: true,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '【大数据】Hadoop生态系统深度解析：分布式存储与计算的技术基石',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"><script>function loadCss(l){var d=document,h=d.head,s=d.createElement('link');s.rel='stylesheet';s.href=l;!function e(f){if (d.body)return f();setTimeout(function(){e(f)})}(function(){h.appendChild(s);});}loadCss('/style.css');loadCss('https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css');loadCss('https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/sharejs/dist/css/share.min.css');</script><noscript><link rel="stylesheet" href="/style.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/sharejs/dist/css/share.min.css"></noscript></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/img/avatar.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">217</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">597</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">26</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Uwakeme</span></a><a class="nav-page-title" href="/"><span class="site-name">【大数据】Hadoop生态系统深度解析：分布式存储与计算的技术基石</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">【大数据】Hadoop生态系统深度解析：分布式存储与计算的技术基石</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-07-29T16:00:00.000Z" title="发表于 2025-07-30 00:00:00">2025-07-30</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-07-31T00:57:45.045Z" title="更新于 2025-07-31 08:57:45">2025-07-31</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">4.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>20分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2025/07/30/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E3%80%90%E5%A4%A7%E6%95%B0%E6%8D%AE%E3%80%91Hadoop%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%EF%BC%9A%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E4%B8%8E%E8%AE%A1%E7%AE%97%E7%9A%84%E6%8A%80%E6%9C%AF%E5%9F%BA%E7%9F%B3/#post-comment"><span class="gitalk-comment-count"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><div id="post-outdate-notice" data="{&quot;limitDay&quot;:1000,&quot;messagePrev&quot;:&quot;本文最后更新于&quot;,&quot;messageNext&quot;:&quot;天前，文中内容可能已过时，请谨慎参考。&quot;,&quot;postUpdate&quot;:&quot;2025-07-31 08:57:45&quot;}" hidden=""></div><h1 id="一、Hadoop概述与架构"><a href="#一、Hadoop概述与架构" class="headerlink" title="一、Hadoop概述与架构"></a>一、Hadoop概述与架构</h1><h2 id="（一）Hadoop简介"><a href="#（一）Hadoop简介" class="headerlink" title="（一）Hadoop简介"></a>（一）Hadoop简介</h2><ul>
<li><p><strong>什么是Hadoop</strong></p>
<ul>
<li>Apache Hadoop：开源分布式存储和计算框架</li>
<li>设计目标：处理大规模数据集的存储和分析</li>
<li>核心理念：移动计算而非移动数据</li>
<li>容错性：硬件故障是常态而非异常</li>
<li>可扩展性：从单机扩展到数千台机器</li>
</ul>
</li>
<li><p><strong>Hadoop发展历程</strong></p>
<ul>
<li>2003年：Google发布GFS和MapReduce论文</li>
<li>2006年：Doug Cutting创建Hadoop项目</li>
<li>2008年：Hadoop成为Apache顶级项目</li>
<li>2012年：Hadoop 2.0引入YARN架构</li>
<li>现在：Hadoop 3.x版本持续演进</li>
</ul>
</li>
<li><p><strong>Hadoop应用场景</strong></p>
<ul>
<li>大数据存储：PB级数据存储和管理</li>
<li>批处理计算：离线数据分析和处理</li>
<li>数据仓库：企业级数据仓库建设</li>
<li>日志分析：网站访问日志、系统日志分析</li>
<li>机器学习：大规模机器学习数据预处理</li>
</ul>
</li>
</ul>
<h2 id="（二）Hadoop核心组件"><a href="#（二）Hadoop核心组件" class="headerlink" title="（二）Hadoop核心组件"></a>（二）Hadoop核心组件</h2><ul>
<li><p><strong>HDFS（Hadoop分布式文件系统）</strong></p>
<ul>
<li>分布式存储：数据分布在多台机器上</li>
<li>高容错性：数据自动备份，故障自动恢复</li>
<li>高吞吐量：适合大文件的顺序读写</li>
<li>流式数据访问：一次写入，多次读取</li>
<li>商用硬件：运行在普通x86服务器上</li>
</ul>
</li>
<li><p><strong>MapReduce（分布式计算框架）</strong></p>
<ul>
<li>编程模型：Map阶段和Reduce阶段</li>
<li>自动并行化：框架自动处理并行执行</li>
<li>容错处理：任务失败自动重试</li>
<li>数据本地性：计算向数据移动</li>
<li>简化编程：隐藏分布式计算复杂性</li>
</ul>
</li>
<li><p><strong>YARN（资源管理器）</strong></p>
<ul>
<li>资源管理：集群资源统一管理和调度</li>
<li>多框架支持：支持MapReduce、Spark等</li>
<li>容器化：应用运行在容器中</li>
<li>高可用性：ResourceManager高可用</li>
<li>资源隔离：CPU、内存资源隔离</li>
</ul>
</li>
</ul>
<h2 id="（三）Hadoop生态系统"><a href="#（三）Hadoop生态系统" class="headerlink" title="（三）Hadoop生态系统"></a>（三）Hadoop生态系统</h2><ul>
<li><p><strong>数据存储层</strong></p>
<ul>
<li>HDFS：分布式文件系统</li>
<li>HBase：NoSQL列式数据库</li>
<li>Kudu：实时分析存储引擎</li>
<li>Alluxio：内存分布式存储系统</li>
</ul>
</li>
<li><p><strong>数据处理层</strong></p>
<ul>
<li>MapReduce：批处理计算框架</li>
<li>Spark：内存计算框架</li>
<li>Flink：流处理计算框架</li>
<li>Storm：实时流处理系统</li>
</ul>
</li>
<li><p><strong>数据管理层</strong></p>
<ul>
<li>Hive：数据仓库软件，SQL查询</li>
<li>Pig：数据流语言和执行环境</li>
<li>Sqoop：关系数据库数据导入导出</li>
<li>Flume：日志收集系统</li>
</ul>
</li>
</ul>
<h1 id="二、HDFS分布式文件系统"><a href="#二、HDFS分布式文件系统" class="headerlink" title="二、HDFS分布式文件系统"></a>二、HDFS分布式文件系统</h1><h2 id="（一）HDFS架构设计"><a href="#（一）HDFS架构设计" class="headerlink" title="（一）HDFS架构设计"></a>（一）HDFS架构设计</h2><ul>
<li><p><strong>主从架构</strong></p>
<ul>
<li>NameNode：主节点，管理文件系统元数据</li>
<li>DataNode：从节点，存储实际数据块</li>
<li>Secondary NameNode：辅助NameNode，定期合并元数据</li>
<li>客户端：文件系统访问接口</li>
</ul>
</li>
<li><p><strong>数据存储机制</strong></p>
<ul>
<li>块存储：文件分割成固定大小的块（默认128MB）</li>
<li>副本机制：每个块默认存储3个副本</li>
<li>副本放置策略：机架感知，提高可靠性和性能</li>
<li>数据完整性：校验和机制检测数据损坏</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># HDFS基本操作命令</span></span><br><span class="line"><span class="comment"># 查看HDFS文件系统</span></span><br><span class="line">hdfs dfs -<span class="built_in">ls</span> /</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建目录</span></span><br><span class="line">hdfs dfs -<span class="built_in">mkdir</span> /user/data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 上传文件到HDFS</span></span><br><span class="line">hdfs dfs -put localfile.txt /user/data/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从HDFS下载文件</span></span><br><span class="line">hdfs dfs -get /user/data/hdfsfile.txt ./</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看文件内容</span></span><br><span class="line">hdfs dfs -<span class="built_in">cat</span> /user/data/file.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除文件</span></span><br><span class="line">hdfs dfs -<span class="built_in">rm</span> /user/data/file.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看文件系统状态</span></span><br><span class="line">hdfs dfsadmin -report</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查文件系统健康状态</span></span><br><span class="line">hdfs fsck /</span><br></pre></td></tr></tbody></table></figure>

<h2 id="（二）NameNode详解"><a href="#（二）NameNode详解" class="headerlink" title="（二）NameNode详解"></a>（二）NameNode详解</h2><ul>
<li><p><strong>元数据管理</strong></p>
<ul>
<li>文件系统树：目录结构和文件信息</li>
<li>块映射：文件块到DataNode的映射关系</li>
<li>内存存储：元数据全部加载到内存中</li>
<li>持久化：FSImage和EditLog文件</li>
</ul>
</li>
<li><p><strong>NameNode高可用（HA）</strong></p>
<ul>
<li>Active/Standby模式：主备NameNode</li>
<li>共享存储：QJM（Quorum Journal Manager）</li>
<li>自动故障转移：ZKFC（ZooKeeper Failover Controller）</li>
<li>数据同步：实时同步元数据变更</li>
</ul>
</li>
</ul>
<figure class="highlight xml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- NameNode HA配置示例 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 配置NameNode集群ID --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">&lt;!-- 配置NameNode节点 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.mycluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">&lt;!-- 配置NameNode RPC地址 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.mycluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>namenode1:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">&lt;!-- 配置共享存储 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://journal1:8485;journal2:8485;journal3:8485/mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure>

<h2 id="（三）DataNode详解"><a href="#（三）DataNode详解" class="headerlink" title="（三）DataNode详解"></a>（三）DataNode详解</h2><ul>
<li><p><strong>数据存储</strong></p>
<ul>
<li>块存储：将文件块存储在本地文件系统</li>
<li>多目录：支持多个存储目录，提高I/O性能</li>
<li>数据校验：定期检查数据块完整性</li>
<li>心跳机制：定期向NameNode报告状态</li>
</ul>
</li>
<li><p><strong>数据读写流程</strong></p>
<ul>
<li>写入流程：客户端→NameNode→DataNode管道写入</li>
<li>读取流程：客户端→NameNode获取位置→直接从DataNode读取</li>
<li>数据本地性：优先读取本地或同机架的数据</li>
<li>负载均衡：自动平衡各DataNode的存储负载</li>
</ul>
</li>
</ul>
<figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// HDFS Java API使用示例</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HDFSExample</span> {</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception {</span><br><span class="line">        <span class="comment">// 创建配置对象</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://namenode:8020"</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 获取文件系统对象</span></span><br><span class="line">        <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(conf);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 创建文件并写入数据</span></span><br><span class="line">        <span class="type">Path</span> <span class="variable">outputPath</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">"/user/data/output.txt"</span>);</span><br><span class="line">        <span class="type">FSDataOutputStream</span> <span class="variable">out</span> <span class="operator">=</span> fs.create(outputPath);</span><br><span class="line">        out.writeUTF(<span class="string">"Hello Hadoop HDFS!"</span>);</span><br><span class="line">        out.close();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 读取文件数据</span></span><br><span class="line">        <span class="type">FSDataInputStream</span> <span class="variable">in</span> <span class="operator">=</span> fs.open(outputPath);</span><br><span class="line">        <span class="type">String</span> <span class="variable">content</span> <span class="operator">=</span> in.readUTF();</span><br><span class="line">        System.out.println(<span class="string">"文件内容: "</span> + content);</span><br><span class="line">        in.close();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 关闭文件系统</span></span><br><span class="line">        fs.close();</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<h1 id="三、MapReduce计算框架"><a href="#三、MapReduce计算框架" class="headerlink" title="三、MapReduce计算框架"></a>三、MapReduce计算框架</h1><h2 id="（一）MapReduce编程模型"><a href="#（一）MapReduce编程模型" class="headerlink" title="（一）MapReduce编程模型"></a>（一）MapReduce编程模型</h2><ul>
<li><p><strong>Map阶段</strong></p>
<ul>
<li>输入分片：将输入数据分割成独立的块</li>
<li>Map函数：处理键值对，输出中间结果</li>
<li>分区：根据key将Map输出分配到不同Reducer</li>
<li>排序：对Map输出按key排序</li>
<li>合并：可选的Combiner减少网络传输</li>
</ul>
</li>
<li><p><strong>Reduce阶段</strong></p>
<ul>
<li>Shuffle：从Map任务获取中间结果</li>
<li>排序：对相同key的值进行分组</li>
<li>Reduce函数：处理分组后的数据</li>
<li>输出：将最终结果写入HDFS</li>
</ul>
</li>
</ul>
<figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// WordCount MapReduce示例</span></span><br><span class="line"><span class="comment">// Mapper类</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, IntWritable&gt; {</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="type">IntWritable</span> <span class="variable">one</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">word</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Context context)</span> </span><br><span class="line">            <span class="keyword">throws</span> IOException, InterruptedException {</span><br><span class="line">        <span class="comment">// 将输入文本转换为小写并分割成单词</span></span><br><span class="line">        String[] words = value.toString().toLowerCase().split(<span class="string">"\\s+"</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 为每个单词输出 (word, 1)</span></span><br><span class="line">        <span class="keyword">for</span> (String w : words) {</span><br><span class="line">            word.set(w);</span><br><span class="line">            context.write(word, one);</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// Reducer类</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt; {</span><br><span class="line">    <span class="keyword">private</span> <span class="type">IntWritable</span> <span class="variable">result</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span></span><br><span class="line">            <span class="keyword">throws</span> IOException, InterruptedException {</span><br><span class="line">        <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 计算每个单词的总数</span></span><br><span class="line">        <span class="keyword">for</span> (IntWritable value : values) {</span><br><span class="line">            sum += value.get();</span><br><span class="line">        }</span><br><span class="line">        </span><br><span class="line">        result.set(sum);</span><br><span class="line">        context.write(key, result);</span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// Driver类</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountDriver</span> {</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception {</span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf, <span class="string">"word count"</span>);</span><br><span class="line">        </span><br><span class="line">        job.setJarByClass(WordCountDriver.class);</span><br><span class="line">        job.setMapperClass(WordCountMapper.class);</span><br><span class="line">        job.setCombinerClass(WordCountReducer.class);  <span class="comment">// 使用Reducer作为Combiner</span></span><br><span class="line">        job.setReducerClass(WordCountReducer.class);</span><br><span class="line">        </span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line">        </span><br><span class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]));</span><br><span class="line">        </span><br><span class="line">        System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<h2 id="（二）MapReduce执行流程"><a href="#（二）MapReduce执行流程" class="headerlink" title="（二）MapReduce执行流程"></a>（二）MapReduce执行流程</h2><ul>
<li><p><strong>作业提交</strong></p>
<ul>
<li>客户端提交作业到ResourceManager</li>
<li>ResourceManager分配ApplicationMaster</li>
<li>ApplicationMaster向ResourceManager申请资源</li>
<li>启动Map和Reduce任务</li>
</ul>
</li>
<li><p><strong>任务执行</strong></p>
<ul>
<li>Map任务：读取输入分片，执行Map函数</li>
<li>Shuffle阶段：Map输出传输到Reducer</li>
<li>Reduce任务：执行Reduce函数，输出结果</li>
<li>任务监控：跟踪任务进度和状态</li>
</ul>
</li>
</ul>
<h2 id="（三）MapReduce优化技术"><a href="#（三）MapReduce优化技术" class="headerlink" title="（三）MapReduce优化技术"></a>（三）MapReduce优化技术</h2><ul>
<li><p><strong>输入优化</strong></p>
<ul>
<li>文件格式：使用SequenceFile、Avro等高效格式</li>
<li>压缩：启用输入数据压缩减少I/O</li>
<li>分片大小：调整输入分片大小优化并行度</li>
<li>数据本地性：优化数据分布提高本地性</li>
</ul>
</li>
<li><p><strong>执行优化</strong></p>
<ul>
<li>Combiner：减少Map输出数据量</li>
<li>压缩：启用Map输出和最终输出压缩</li>
<li>内存调优：调整JVM堆大小和缓冲区</li>
<li>推测执行：启用推测执行处理慢任务</li>
</ul>
</li>
</ul>
<figure class="highlight xml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- MapReduce性能优化配置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 启用Map输出压缩 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.output.compress<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">&lt;!-- 设置Map输出压缩编解码器 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.output.compress.codec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.io.compress.SnappyCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">&lt;!-- 调整Map任务内存 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">&lt;!-- 调整Reduce任务内存 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>4096<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">&lt;!-- 启用推测执行 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure>

<h1 id="四、YARN资源管理"><a href="#四、YARN资源管理" class="headerlink" title="四、YARN资源管理"></a>四、YARN资源管理</h1><h2 id="（一）YARN架构组件"><a href="#（一）YARN架构组件" class="headerlink" title="（一）YARN架构组件"></a>（一）YARN架构组件</h2><ul>
<li><p><strong>ResourceManager（RM）</strong></p>
<ul>
<li>全局资源管理：管理整个集群的资源</li>
<li>应用管理：接收作业提交，启动ApplicationMaster</li>
<li>调度器：根据策略分配资源给应用</li>
<li>高可用：支持Active/Standby模式</li>
</ul>
</li>
<li><p><strong>NodeManager（NM）</strong></p>
<ul>
<li>节点资源管理：管理单个节点的资源</li>
<li>容器管理：启动和监控容器</li>
<li>健康检查：监控节点健康状态</li>
<li>日志管理：收集和管理应用日志</li>
</ul>
</li>
<li><p><strong>ApplicationMaster（AM）</strong></p>
<ul>
<li>应用协调：协调应用的执行</li>
<li>资源申请：向ResourceManager申请资源</li>
<li>任务调度：在分配的容器中启动任务</li>
<li>容错处理：处理任务失败和重试</li>
</ul>
</li>
<li><p><strong>Container（容器）</strong></p>
<ul>
<li>资源封装：封装CPU、内存等资源</li>
<li>任务执行：应用任务的执行环境</li>
<li>资源隔离：提供资源隔离保证</li>
<li>生命周期管理：容器的创建、运行、销毁</li>
</ul>
</li>
</ul>
<h2 id="（二）YARN调度器"><a href="#（二）YARN调度器" class="headerlink" title="（二）YARN调度器"></a>（二）YARN调度器</h2><ul>
<li><p><strong>FIFO调度器</strong></p>
<ul>
<li>先进先出：按提交顺序执行作业</li>
<li>简单实现：适合小规模集群</li>
<li>资源利用率低：大作业会阻塞小作业</li>
<li>不支持优先级：无法区分作业重要性</li>
</ul>
</li>
<li><p><strong>容量调度器（Capacity Scheduler）</strong></p>
<ul>
<li>队列管理：支持多个队列，队列间资源隔离</li>
<li>容量保证：每个队列有最小资源保证</li>
<li>弹性资源：空闲资源可被其他队列使用</li>
<li>层次队列：支持队列嵌套，细粒度管理</li>
</ul>
</li>
<li><p><strong>公平调度器（Fair Scheduler）</strong></p>
<ul>
<li>公平共享：所有应用公平共享资源</li>
<li>抢占机制：支持资源抢占保证公平性</li>
<li>多种策略：支持FIFO、Fair、DRF等策略</li>
<li>动态配置：支持运行时配置修改</li>
</ul>
</li>
</ul>
<figure class="highlight xml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 容量调度器配置示例 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 启用容量调度器 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">&lt;!-- 配置队列 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.queues<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>default,production,development<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">&lt;!-- 配置队列容量 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.default.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>40<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.production.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>40<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.development.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>20<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure>

<h2 id="（三）YARN应用开发"><a href="#（三）YARN应用开发" class="headerlink" title="（三）YARN应用开发"></a>（三）YARN应用开发</h2><ul>
<li><p><strong>应用提交流程</strong></p>
<ul>
<li>客户端向ResourceManager提交应用</li>
<li>ResourceManager分配容器启动ApplicationMaster</li>
<li>ApplicationMaster向ResourceManager注册</li>
<li>ApplicationMaster申请资源启动任务</li>
<li>任务完成后ApplicationMaster注销</li>
</ul>
</li>
<li><p><strong>编程接口</strong></p>
<ul>
<li>Client API：应用提交和监控</li>
<li>ApplicationMaster API：资源申请和任务管理</li>
<li>Container API：容器生命周期管理</li>
<li>Timeline Service：应用历史信息存储</li>
</ul>
</li>
</ul>
<figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// YARN应用客户端示例</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">YarnClient</span> {</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception {</span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">YarnClient</span> <span class="variable">yarnClient</span> <span class="operator">=</span> YarnClient.createYarnClient();</span><br><span class="line">        yarnClient.init(conf);</span><br><span class="line">        yarnClient.start();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 创建应用</span></span><br><span class="line">        <span class="type">YarnClientApplication</span> <span class="variable">app</span> <span class="operator">=</span> yarnClient.createApplication();</span><br><span class="line">        <span class="type">ApplicationSubmissionContext</span> <span class="variable">appContext</span> <span class="operator">=</span> app.getApplicationSubmissionContext();</span><br><span class="line">        <span class="type">ApplicationId</span> <span class="variable">appId</span> <span class="operator">=</span> appContext.getApplicationId();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 设置应用信息</span></span><br><span class="line">        appContext.setApplicationName(<span class="string">"MyYarnApp"</span>);</span><br><span class="line">        appContext.setApplicationType(<span class="string">"MAPREDUCE"</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 设置ApplicationMaster</span></span><br><span class="line">        <span class="type">ContainerLaunchContext</span> <span class="variable">amContainer</span> <span class="operator">=</span> ContainerLaunchContext.newInstance(</span><br><span class="line">            <span class="literal">null</span>, <span class="literal">null</span>, <span class="literal">null</span>, <span class="literal">null</span>, <span class="literal">null</span>, <span class="literal">null</span>);</span><br><span class="line">        appContext.setAMContainerSpec(amContainer);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 设置资源需求</span></span><br><span class="line">        <span class="type">Resource</span> <span class="variable">capability</span> <span class="operator">=</span> Resource.newInstance(<span class="number">1024</span>, <span class="number">1</span>);</span><br><span class="line">        appContext.setResource(capability);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 提交应用</span></span><br><span class="line">        yarnClient.submitApplication(appContext);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 监控应用状态</span></span><br><span class="line">        <span class="type">ApplicationReport</span> <span class="variable">appReport</span> <span class="operator">=</span> yarnClient.getApplicationReport(appId);</span><br><span class="line">        <span class="type">YarnApplicationState</span> <span class="variable">appState</span> <span class="operator">=</span> appReport.getYarnApplicationState();</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> (appState != YarnApplicationState.FINISHED &amp;&amp; </span><br><span class="line">               appState != YarnApplicationState.KILLED &amp;&amp; </span><br><span class="line">               appState != YarnApplicationState.FAILED) {</span><br><span class="line">            Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">            appReport = yarnClient.getApplicationReport(appId);</span><br><span class="line">            appState = appReport.getYarnApplicationState();</span><br><span class="line">        }</span><br><span class="line">        </span><br><span class="line">        yarnClient.close();</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<h1 id="五、Hadoop生态系统组件"><a href="#五、Hadoop生态系统组件" class="headerlink" title="五、Hadoop生态系统组件"></a>五、Hadoop生态系统组件</h1><h2 id="（一）Hive数据仓库"><a href="#（一）Hive数据仓库" class="headerlink" title="（一）Hive数据仓库"></a>（一）Hive数据仓库</h2><ul>
<li><p><strong>Hive概述</strong></p>
<ul>
<li>SQL接口：提供类SQL查询语言HiveQL</li>
<li>元数据管理：存储表结构和分区信息</li>
<li>数据存储：数据存储在HDFS上</li>
<li>执行引擎：支持MapReduce、Spark、Tez</li>
</ul>
</li>
<li><p><strong>Hive架构</strong></p>
<ul>
<li>Hive CLI：命令行接口</li>
<li>HiveServer2：JDBC/ODBC服务</li>
<li>Metastore：元数据存储服务</li>
<li>Driver：查询编译和优化</li>
<li>执行引擎：查询执行引擎</li>
</ul>
</li>
</ul>
<figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- Hive SQL示例</span></span><br><span class="line"><span class="comment">-- 创建数据库</span></span><br><span class="line"><span class="keyword">CREATE</span> DATABASE IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> sales_db;</span><br><span class="line">USE sales_db;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 创建外部表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> sales_data (</span><br><span class="line">    order_id STRING,</span><br><span class="line">    customer_id STRING,</span><br><span class="line">    product_id STRING,</span><br><span class="line">    quantity <span class="type">INT</span>,</span><br><span class="line">    price <span class="type">DECIMAL</span>(<span class="number">10</span>,<span class="number">2</span>),</span><br><span class="line">    order_date STRING</span><br><span class="line">)</span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (<span class="keyword">year</span> <span class="type">INT</span>, <span class="keyword">month</span> <span class="type">INT</span>)</span><br><span class="line">STORED <span class="keyword">AS</span> TEXTFILE</span><br><span class="line">LOCATION <span class="string">'/user/data/sales/'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 添加分区</span></span><br><span class="line"><span class="keyword">ALTER TABLE</span> sales_data <span class="keyword">ADD</span> <span class="keyword">PARTITION</span> (<span class="keyword">year</span><span class="operator">=</span><span class="number">2023</span>, <span class="keyword">month</span><span class="operator">=</span><span class="number">12</span>)</span><br><span class="line">LOCATION <span class="string">'/user/data/sales/2023/12/'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 查询数据</span></span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    customer_id,</span><br><span class="line">    <span class="built_in">SUM</span>(quantity <span class="operator">*</span> price) <span class="keyword">as</span> total_amount</span><br><span class="line"><span class="keyword">FROM</span> sales_data</span><br><span class="line"><span class="keyword">WHERE</span> <span class="keyword">year</span> <span class="operator">=</span> <span class="number">2023</span> <span class="keyword">AND</span> <span class="keyword">month</span> <span class="operator">=</span> <span class="number">12</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> customer_id</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> total_amount <span class="keyword">DESC</span></span><br><span class="line">LIMIT <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 创建内部表并插入数据</span></span><br><span class="line"><span class="keyword">CREATE TABLE</span> customer_summary <span class="keyword">AS</span></span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    customer_id,</span><br><span class="line">    <span class="built_in">COUNT</span>(<span class="operator">*</span>) <span class="keyword">as</span> order_count,</span><br><span class="line">    <span class="built_in">SUM</span>(quantity <span class="operator">*</span> price) <span class="keyword">as</span> total_spent,</span><br><span class="line">    <span class="built_in">AVG</span>(quantity <span class="operator">*</span> price) <span class="keyword">as</span> avg_order_value</span><br><span class="line"><span class="keyword">FROM</span> sales_data</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> customer_id;</span><br></pre></td></tr></tbody></table></figure>

<h2 id="（二）HBase-NoSQL数据库"><a href="#（二）HBase-NoSQL数据库" class="headerlink" title="（二）HBase NoSQL数据库"></a>（二）HBase NoSQL数据库</h2><ul>
<li><p><strong>HBase特点</strong></p>
<ul>
<li>列式存储：按列族存储数据</li>
<li>实时读写：支持随机实时读写</li>
<li>自动分片：Region自动分割和负载均衡</li>
<li>强一致性：提供强一致性保证</li>
<li>水平扩展：支持线性扩展</li>
</ul>
</li>
<li><p><strong>HBase数据模型</strong></p>
<ul>
<li>表（Table）：数据存储的逻辑单元</li>
<li>行键（Row Key）：唯一标识一行数据</li>
<li>列族（Column Family）：列的逻辑分组</li>
<li>列限定符（Column Qualifier）：列的具体名称</li>
<li>时间戳（Timestamp）：数据版本标识</li>
</ul>
</li>
</ul>
<figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// HBase Java API示例</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.HBaseConfiguration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.Bytes;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HBaseExample</span> {</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception {</span><br><span class="line">        <span class="comment">// 创建配置</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> HBaseConfiguration.create();</span><br><span class="line">        conf.set(<span class="string">"hbase.zookeeper.quorum"</span>, <span class="string">"zk1,zk2,zk3"</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 创建连接</span></span><br><span class="line">        <span class="type">Connection</span> <span class="variable">connection</span> <span class="operator">=</span> ConnectionFactory.createConnection(conf);</span><br><span class="line">        <span class="type">Table</span> <span class="variable">table</span> <span class="operator">=</span> connection.getTable(TableName.valueOf(<span class="string">"user_profile"</span>));</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 插入数据</span></span><br><span class="line">        <span class="type">Put</span> <span class="variable">put</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Put</span>(Bytes.toBytes(<span class="string">"user001"</span>));</span><br><span class="line">        put.addColumn(Bytes.toBytes(<span class="string">"info"</span>), Bytes.toBytes(<span class="string">"name"</span>), Bytes.toBytes(<span class="string">"张三"</span>));</span><br><span class="line">        put.addColumn(Bytes.toBytes(<span class="string">"info"</span>), Bytes.toBytes(<span class="string">"age"</span>), Bytes.toBytes(<span class="string">"25"</span>));</span><br><span class="line">        put.addColumn(Bytes.toBytes(<span class="string">"contact"</span>), Bytes.toBytes(<span class="string">"email"</span>), Bytes.toBytes(<span class="string">"zhangsan@example.com"</span>));</span><br><span class="line">        table.put(put);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 查询数据</span></span><br><span class="line">        <span class="type">Get</span> <span class="variable">get</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Get</span>(Bytes.toBytes(<span class="string">"user001"</span>));</span><br><span class="line">        <span class="type">Result</span> <span class="variable">result</span> <span class="operator">=</span> table.get(get);</span><br><span class="line">        </span><br><span class="line">        <span class="type">String</span> <span class="variable">name</span> <span class="operator">=</span> Bytes.toString(result.getValue(Bytes.toBytes(<span class="string">"info"</span>), Bytes.toBytes(<span class="string">"name"</span>)));</span><br><span class="line">        <span class="type">String</span> <span class="variable">age</span> <span class="operator">=</span> Bytes.toString(result.getValue(Bytes.toBytes(<span class="string">"info"</span>), Bytes.toBytes(<span class="string">"age"</span>)));</span><br><span class="line">        </span><br><span class="line">        System.out.println(<span class="string">"姓名: "</span> + name + <span class="string">", 年龄: "</span> + age);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 扫描数据</span></span><br><span class="line">        <span class="type">Scan</span> <span class="variable">scan</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scan</span>();</span><br><span class="line">        scan.addFamily(Bytes.toBytes(<span class="string">"info"</span>));</span><br><span class="line">        <span class="type">ResultScanner</span> <span class="variable">scanner</span> <span class="operator">=</span> table.getScanner(scan);</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> (Result r : scanner) {</span><br><span class="line">            <span class="type">String</span> <span class="variable">rowKey</span> <span class="operator">=</span> Bytes.toString(r.getRow());</span><br><span class="line">            <span class="type">String</span> <span class="variable">userName</span> <span class="operator">=</span> Bytes.toString(r.getValue(Bytes.toBytes(<span class="string">"info"</span>), Bytes.toBytes(<span class="string">"name"</span>)));</span><br><span class="line">            System.out.println(<span class="string">"用户ID: "</span> + rowKey + <span class="string">", 姓名: "</span> + userName);</span><br><span class="line">        }</span><br><span class="line">        </span><br><span class="line">        scanner.close();</span><br><span class="line">        table.close();</span><br><span class="line">        connection.close();</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<h2 id="（三）Spark计算引擎"><a href="#（三）Spark计算引擎" class="headerlink" title="（三）Spark计算引擎"></a>（三）Spark计算引擎</h2><ul>
<li><p><strong>Spark优势</strong></p>
<ul>
<li>内存计算：数据缓存在内存中，提高性能</li>
<li>多语言支持：支持Scala、Java、Python、R</li>
<li>统一平台：批处理、流处理、机器学习、图计算</li>
<li>易用性：丰富的高级API和算子</li>
<li>容错性：RDD血缘关系提供容错能力</li>
</ul>
</li>
<li><p><strong>Spark核心概念</strong></p>
<ul>
<li>RDD：弹性分布式数据集</li>
<li>DataFrame：结构化数据抽象</li>
<li>Dataset：类型安全的数据抽象</li>
<li>Spark SQL：结构化数据处理</li>
<li>Spark Streaming：流数据处理</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># PySpark示例</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建SparkSession</span></span><br><span class="line">spark = SparkSession.builder \</span><br><span class="line">    .appName(<span class="string">"SparkExample"</span>) \</span><br><span class="line">    .config(<span class="string">"spark.sql.adaptive.enabled"</span>, <span class="string">"true"</span>) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line">df = spark.read.option(<span class="string">"header"</span>, <span class="string">"true"</span>).csv(<span class="string">"hdfs://namenode:8020/user/data/sales.csv"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据处理</span></span><br><span class="line">result = df.groupBy(<span class="string">"customer_id"</span>) \</span><br><span class="line">    .agg(</span><br><span class="line">        count(<span class="string">"order_id"</span>).alias(<span class="string">"order_count"</span>),</span><br><span class="line">        <span class="built_in">sum</span>(<span class="string">"amount"</span>).alias(<span class="string">"total_amount"</span>),</span><br><span class="line">        avg(<span class="string">"amount"</span>).alias(<span class="string">"avg_amount"</span>)</span><br><span class="line">    ) \</span><br><span class="line">    .<span class="built_in">filter</span>(col(<span class="string">"order_count"</span>) &gt; <span class="number">5</span>) \</span><br><span class="line">    .orderBy(desc(<span class="string">"total_amount"</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">result.show(<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 写入结果</span></span><br><span class="line">result.write \</span><br><span class="line">    .mode(<span class="string">"overwrite"</span>) \</span><br><span class="line">    .option(<span class="string">"header"</span>, <span class="string">"true"</span>) \</span><br><span class="line">    .csv(<span class="string">"hdfs://namenode:8020/user/output/customer_analysis"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 停止SparkSession</span></span><br><span class="line">spark.stop()</span><br></pre></td></tr></tbody></table></figure>

<h1 id="六、Hadoop集群部署与管理"><a href="#六、Hadoop集群部署与管理" class="headerlink" title="六、Hadoop集群部署与管理"></a>六、Hadoop集群部署与管理</h1><h2 id="（一）集群规划与部署"><a href="#（一）集群规划与部署" class="headerlink" title="（一）集群规划与部署"></a>（一）集群规划与部署</h2><ul>
<li><p><strong>硬件规划</strong></p>
<ul>
<li>NameNode：高内存、SSD存储、双网卡</li>
<li>DataNode：大容量存储、多磁盘、高网络带宽</li>
<li>ResourceManager：中等配置、高可用部署</li>
<li>网络：万兆以太网、交换机配置</li>
</ul>
</li>
<li><p><strong>软件部署</strong></p>
<ul>
<li>操作系统：CentOS、Ubuntu LTS版本</li>
<li>Java环境：OpenJDK 8或11</li>
<li>Hadoop安装：二进制包部署或编译安装</li>
<li>配置文件：core-site.xml、hdfs-site.xml、yarn-site.xml</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment"># Hadoop集群部署脚本</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置环境变量</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/java-8-openjdk</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/opt/hadoop</span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=<span class="variable">$HADOOP_HOME</span>/etc/hadoop</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建Hadoop用户</span></span><br><span class="line">useradd -m hadoop</span><br><span class="line">usermod -aG <span class="built_in">sudo</span> hadoop</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置SSH免密登录</span></span><br><span class="line">su - hadoop -c <span class="string">"ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa"</span></span><br><span class="line">su - hadoop -c <span class="string">"cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys"</span></span><br><span class="line">su - hadoop -c <span class="string">"chmod 600 ~/.ssh/authorized_keys"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载并解压Hadoop</span></span><br><span class="line"><span class="built_in">cd</span> /opt</span><br><span class="line">wget https://downloads.apache.org/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz</span><br><span class="line">tar -xzf hadoop-3.3.4.tar.gz</span><br><span class="line"><span class="built_in">mv</span> hadoop-3.3.4 hadoop</span><br><span class="line"><span class="built_in">chown</span> -R hadoop:hadoop hadoop</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置Hadoop环境变量</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">'export HADOOP_HOME=/opt/hadoop'</span> &gt;&gt; /home/hadoop/.bashrc</span><br><span class="line"><span class="built_in">echo</span> <span class="string">'export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop'</span> &gt;&gt; /home/hadoop/.bashrc</span><br><span class="line"><span class="built_in">echo</span> <span class="string">'export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin'</span> &gt;&gt; /home/hadoop/.bashrc</span><br><span class="line"></span><br><span class="line"><span class="comment"># 格式化NameNode（仅在NameNode节点执行）</span></span><br><span class="line">su - hadoop -c <span class="string">"<span class="variable">$HADOOP_HOME</span>/bin/hdfs namenode -format -force"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动Hadoop集群</span></span><br><span class="line">su - hadoop -c <span class="string">"<span class="variable">$HADOOP_HOME</span>/sbin/start-dfs.sh"</span></span><br><span class="line">su - hadoop -c <span class="string">"<span class="variable">$HADOOP_HOME</span>/sbin/start-yarn.sh"</span></span><br></pre></td></tr></tbody></table></figure>

<h2 id="（二）集群监控与管理"><a href="#（二）集群监控与管理" class="headerlink" title="（二）集群监控与管理"></a>（二）集群监控与管理</h2><ul>
<li><p><strong>Web界面监控</strong></p>
<ul>
<li>NameNode Web UI：<a target="_blank" rel="noopener" href="http://namenode:9870/">http://namenode:9870</a></li>
<li>ResourceManager Web UI：<a target="_blank" rel="noopener" href="http://resourcemanager:8088/">http://resourcemanager:8088</a></li>
<li>DataNode Web UI：<a target="_blank" rel="noopener" href="http://datanode:9864/">http://datanode:9864</a></li>
<li>NodeManager Web UI：<a target="_blank" rel="noopener" href="http://nodemanager:8042/">http://nodemanager:8042</a></li>
</ul>
</li>
<li><p><strong>命令行监控</strong></p>
<ul>
<li>集群状态：hdfs dfsadmin -report</li>
<li>节点状态：yarn node -list</li>
<li>应用状态：yarn application -list</li>
<li>日志查看：yarn logs -applicationId app_id</li>
</ul>
</li>
<li><p><strong>第三方监控工具</strong></p>
<ul>
<li>Ambari：集群管理和监控平台</li>
<li>Cloudera Manager：企业级集群管理</li>
<li>Ganglia：分布式监控系统</li>
<li>Nagios：网络监控系统</li>
<li>Prometheus + Grafana：现代监控方案</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Hadoop集群健康检查脚本</span></span><br><span class="line"><span class="comment">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"=== Hadoop集群健康检查 ==="</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查HDFS状态</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"1. HDFS文件系统状态："</span></span><br><span class="line">hdfs dfsadmin -report | grep -E <span class="string">"Live datanodes|Dead datanodes|DFS Used%"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查HDFS文件系统完整性</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"2. HDFS文件系统完整性检查："</span></span><br><span class="line">hdfs fsck / -files -blocks -locations | <span class="built_in">tail</span> -10</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查YARN集群状态</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"3. YARN集群状态："</span></span><br><span class="line">yarn node -list -all | grep -E <span class="string">"RUNNING|UNHEALTHY|LOST"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查运行中的应用</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"4. 运行中的应用："</span></span><br><span class="line">yarn application -list -appStates RUNNING</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查集群资源使用情况</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"5. 集群资源使用情况："</span></span><br><span class="line">yarn top</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查关键服务进程</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"6. 关键服务进程检查："</span></span><br><span class="line">jps | grep -E <span class="string">"NameNode|DataNode|ResourceManager|NodeManager"</span></span><br></pre></td></tr></tbody></table></figure>

<h2 id="（三）性能调优与故障排除"><a href="#（三）性能调优与故障排除" class="headerlink" title="（三）性能调优与故障排除"></a>（三）性能调优与故障排除</h2><ul>
<li><p><strong>HDFS性能调优</strong></p>
<ul>
<li>块大小优化：根据文件大小调整块大小</li>
<li>副本数量：根据可靠性需求调整副本数</li>
<li>压缩配置：启用数据压缩减少存储和网络开销</li>
<li>缓存配置：配置HDFS缓存提高读性能</li>
</ul>
</li>
<li><p><strong>YARN性能调优</strong></p>
<ul>
<li>内存配置：合理配置容器内存大小</li>
<li>CPU配置：启用CPU资源调度</li>
<li>调度器优化：选择合适的调度器和配置</li>
<li>本地化：提高数据本地性减少网络传输</li>
</ul>
</li>
<li><p><strong>常见故障排除</strong></p>
<ul>
<li>NameNode故障：检查内存、磁盘空间、网络</li>
<li>DataNode故障：检查磁盘健康、网络连接</li>
<li>作业失败：检查日志、资源配置、数据格式</li>
<li>性能问题：分析瓶颈、优化配置、硬件升级</li>
</ul>
</li>
</ul>
<hr>
<p><strong>总结</strong>：Hadoop作为大数据处理的基础平台，提供了可靠的分布式存储和计算能力。HDFS解决了大规模数据存储问题，MapReduce提供了简单易用的分布式计算模型，YARN实现了资源的统一管理和调度。</p>
<p>随着大数据技术的发展，Hadoop生态系统不断丰富，Spark、Flink等新一代计算引擎在某些场景下提供了更好的性能。但Hadoop作为大数据的基石，其稳定性、可靠性和成熟的生态系统仍然使其在企业级大数据应用中占据重要地位。</p>
<p>学习Hadoop需要理解分布式系统的基本概念，掌握HDFS、MapReduce、YARN的核心原理，熟悉生态系统中各组件的使用。在实际应用中，要根据业务需求选择合适的技术栈，合理规划集群架构，做好性能调优和运维管理。掌握Hadoop技术，将为您在大数据领域的发展奠定坚实的基础。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://uwakeme.tech/about/">Wake</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://uwakeme.tech/2025/07/30/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E3%80%90%E5%A4%A7%E6%95%B0%E6%8D%AE%E3%80%91Hadoop%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%EF%BC%9A%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E4%B8%8E%E8%AE%A1%E7%AE%97%E7%9A%84%E6%8A%80%E6%9C%AF%E5%9F%BA%E7%9F%B3/">https://uwakeme.tech/2025/07/30/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E3%80%90%E5%A4%A7%E6%95%B0%E6%8D%AE%E3%80%91Hadoop%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%EF%BC%9A%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E4%B8%8E%E8%AE%A1%E7%AE%97%E7%9A%84%E6%8A%80%E6%9C%AF%E5%9F%BA%E7%9F%B3/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://uwakeme.tech" target="_blank">Uwakeme</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Hadoop/">Hadoop</a><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a><a class="post-meta__tags" href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/">分布式系统</a><a class="post-meta__tags" href="/tags/HDFS/">HDFS</a><a class="post-meta__tags" href="/tags/MapReduce/">MapReduce</a><a class="post-meta__tags" href="/tags/YARN/">YARN</a><a class="post-meta__tags" href="/tags/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/">数据处理</a></div><div class="post-share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/sharejs/dist/js/social-share.min.js" defer=""></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/reward/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/img/reward/wechat.jpg" alt="微信"></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/reward/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/img/reward/alipay.jpg" alt="支付宝"></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/08/05/%E5%8D%9A%E5%AE%A2/%E3%80%90%E5%8D%9A%E5%AE%A2%E3%80%91%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BAHexo%E5%8D%9A%E5%AE%A2%E7%BD%91%E7%AB%99/" title="【博客】搭建个人Hexo博客网站"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">【博客】搭建个人Hexo博客网站</div></div><div class="info-2"><div class="info-item-1">一、准备环境1、安装node 访问Node.js官网：https://nodejs.org/ 下载LTS (长期支持版本) 安装时保持默认选项即可 安装完成后，打开命令提示符验证安装：1node -v  2、安装npm npm已包含在Node.js安装包中，安装Node.js时会自动安装 打开命令提示符验证安装：1npm -v 更新npm到最新版本（可选）：1npm install -g npm  3、安装hexo 打开命令提示符，以管理员身份运行以下命令 npm install -g hexo hexo -v 1234567891011121314151617# 二、Git仓库准备## 1、新建git仓库+ 在GitHub/Gitee上创建一个新的仓库+ 仓库名称建议设置为：`用户名.github.io`（使用GitHub Pages时）或自定义名称+ 初始化时不需要添加README文件## 2、本地同步git仓库+ 在一个空白文件夹中，右键打开git bash，输入命令，将git仓库拉下来  ```shell  git clone 仓库地址 &gt; 如果使用SSH连接，请确...</div></div></div></a><a class="pagination-related" href="/2025/07/28/%E5%89%8D%E7%AB%AF/%E3%80%90%E5%89%8D%E7%AB%AF%E3%80%91%E8%B7%A8%E5%9F%9F%E9%97%AE%E9%A2%98%E8%AF%A6%E8%A7%A3%EF%BC%9A%E4%BB%8E%E5%90%8C%E6%BA%90%E7%AD%96%E7%95%A5%E5%88%B0%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E7%9A%84%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/" title="【前端】跨域问题详解：从同源策略到解决方案的完整指南"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">【前端】跨域问题详解：从同源策略到解决方案的完整指南</div></div><div class="info-2"><div class="info-item-1">前言跨域问题是前端开发中最常遇到的问题之一，它源于浏览器的同源策略安全机制。随着现代Web应用架构的复杂化，前后端分离、微服务架构的普及，跨域问题变得更加突出。本文将从同源策略的基本概念出发，深入分析跨域问题的本质，并详细介绍各种跨域解决方案的原理、实现方式和适用场景。 一、同源策略基础（一）什么是同源策略同源策略（Same-Origin Policy）是浏览器的一个重要安全机制，它限制了从一个源加载的文档或脚本如何与来自另一个源的资源进行交互。这是一个用于隔离潜在恶意文档的重要安全机制。 1. 源的定义一个源由三个部分组成：  协议（Protocol）：如 http:// 或 https:// 域名（Domain）：如 example.com 或 api.example.com 端口（Port）：如 :80、:443、:3000  1234567891011// 同源示例const currentOrigin = 'https://www.example.com:443';// 以下URL与当前源的对比：const urls = [    'htt...</div></div></div></a></nav><hr class="custom-hr"><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/img/avatar.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"></div><div class="author-info-name">Wake</div><div class="author-info-description">一起学习，一起进步</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">217</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">597</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">26</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/uwakeme"><i class="fab fa-github"></i><span>关注我</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/uwakeme" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:wakemeup2025@126.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到我的博客！这里分享技术知识、学习心得和生活感悟。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E3%80%81Hadoop%E6%A6%82%E8%BF%B0%E4%B8%8E%E6%9E%B6%E6%9E%84"><span class="toc-text">一、Hadoop概述与架构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%B8%80%EF%BC%89Hadoop%E7%AE%80%E4%BB%8B"><span class="toc-text">（一）Hadoop简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%BA%8C%EF%BC%89Hadoop%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6"><span class="toc-text">（二）Hadoop核心组件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%B8%89%EF%BC%89Hadoop%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F"><span class="toc-text">（三）Hadoop生态系统</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E3%80%81HDFS%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F"><span class="toc-text">二、HDFS分布式文件系统</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%B8%80%EF%BC%89HDFS%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1"><span class="toc-text">（一）HDFS架构设计</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%BA%8C%EF%BC%89NameNode%E8%AF%A6%E8%A7%A3"><span class="toc-text">（二）NameNode详解</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%B8%89%EF%BC%89DataNode%E8%AF%A6%E8%A7%A3"><span class="toc-text">（三）DataNode详解</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89%E3%80%81MapReduce%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6"><span class="toc-text">三、MapReduce计算框架</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%B8%80%EF%BC%89MapReduce%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B"><span class="toc-text">（一）MapReduce编程模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%BA%8C%EF%BC%89MapReduce%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B"><span class="toc-text">（二）MapReduce执行流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%B8%89%EF%BC%89MapReduce%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF"><span class="toc-text">（三）MapReduce优化技术</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B%E3%80%81YARN%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86"><span class="toc-text">四、YARN资源管理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%B8%80%EF%BC%89YARN%E6%9E%B6%E6%9E%84%E7%BB%84%E4%BB%B6"><span class="toc-text">（一）YARN架构组件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%BA%8C%EF%BC%89YARN%E8%B0%83%E5%BA%A6%E5%99%A8"><span class="toc-text">（二）YARN调度器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%B8%89%EF%BC%89YARN%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91"><span class="toc-text">（三）YARN应用开发</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94%E3%80%81Hadoop%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F%E7%BB%84%E4%BB%B6"><span class="toc-text">五、Hadoop生态系统组件</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%B8%80%EF%BC%89Hive%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93"><span class="toc-text">（一）Hive数据仓库</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%BA%8C%EF%BC%89HBase-NoSQL%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-text">（二）HBase NoSQL数据库</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%B8%89%EF%BC%89Spark%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E"><span class="toc-text">（三）Spark计算引擎</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%AD%E3%80%81Hadoop%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E4%B8%8E%E7%AE%A1%E7%90%86"><span class="toc-text">六、Hadoop集群部署与管理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%B8%80%EF%BC%89%E9%9B%86%E7%BE%A4%E8%A7%84%E5%88%92%E4%B8%8E%E9%83%A8%E7%BD%B2"><span class="toc-text">（一）集群规划与部署</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%BA%8C%EF%BC%89%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7%E4%B8%8E%E7%AE%A1%E7%90%86"><span class="toc-text">（二）集群监控与管理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%B8%89%EF%BC%89%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E4%B8%8E%E6%95%85%E9%9A%9C%E6%8E%92%E9%99%A4"><span class="toc-text">（三）性能调优与故障排除</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/08/12/Java/%E3%80%90Java%E3%80%91MyBatis%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%EF%BC%9A%E4%BB%8E%E5%8E%9F%E7%94%9F%E6%A1%86%E6%9E%B6%E5%88%B0%E5%A4%9A%E8%A1%A8%E5%85%B3%E8%81%94%E6%9F%A5%E8%AF%A2%E7%9A%84%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/" title="【Java】MyBatis生态系统深度解析：从原生框架到多表关联查询的源码剖析"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【Java】MyBatis生态系统深度解析：从原生框架到多表关联查询的源码剖析"></a><div class="content"><a class="title" href="/2025/08/12/Java/%E3%80%90Java%E3%80%91MyBatis%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%EF%BC%9A%E4%BB%8E%E5%8E%9F%E7%94%9F%E6%A1%86%E6%9E%B6%E5%88%B0%E5%A4%9A%E8%A1%A8%E5%85%B3%E8%81%94%E6%9F%A5%E8%AF%A2%E7%9A%84%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/" title="【Java】MyBatis生态系统深度解析：从原生框架到多表关联查询的源码剖析">【Java】MyBatis生态系统深度解析：从原生框架到多表关联查询的源码剖析</a><time datetime="2025-08-11T16:00:00.000Z" title="发表于 2025-08-12 00:00:00">2025-08-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E3%80%90%E5%AD%A6%E4%B9%A0%E3%80%91C%E8%AF%AD%E8%A8%80%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98%EF%BC%9A%E4%BB%8E%E7%AE%97%E6%B3%95%E5%88%B0%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B/" title="【学习】C语言项目实战：从算法到系统编程"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【学习】C语言项目实战：从算法到系统编程"></a><div class="content"><a class="title" href="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E3%80%90%E5%AD%A6%E4%B9%A0%E3%80%91C%E8%AF%AD%E8%A8%80%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98%EF%BC%9A%E4%BB%8E%E7%AE%97%E6%B3%95%E5%88%B0%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B/" title="【学习】C语言项目实战：从算法到系统编程">【学习】C语言项目实战：从算法到系统编程</a><time datetime="2025-08-11T09:00:00.000Z" title="发表于 2025-08-11 17:00:00">2025-08-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E3%80%90%E5%AD%A6%E4%B9%A0%E3%80%91C%E8%AF%AD%E8%A8%80%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7%E4%B8%8E%E7%8E%B0%E4%BB%A3%E7%BC%96%E7%A8%8B%EF%BC%9A%E6%8F%90%E5%8D%87%E4%BB%A3%E7%A0%81%E8%B4%A8%E9%87%8F/" title="【学习】C语言高级特性与现代编程：提升代码质量"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【学习】C语言高级特性与现代编程：提升代码质量"></a><div class="content"><a class="title" href="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E3%80%90%E5%AD%A6%E4%B9%A0%E3%80%91C%E8%AF%AD%E8%A8%80%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7%E4%B8%8E%E7%8E%B0%E4%BB%A3%E7%BC%96%E7%A8%8B%EF%BC%9A%E6%8F%90%E5%8D%87%E4%BB%A3%E7%A0%81%E8%B4%A8%E9%87%8F/" title="【学习】C语言高级特性与现代编程：提升代码质量">【学习】C语言高级特性与现代编程：提升代码质量</a><time datetime="2025-08-11T08:00:00.000Z" title="发表于 2025-08-11 16:00:00">2025-08-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E3%80%90%E5%AD%A6%E4%B9%A0%E3%80%91C%E8%AF%AD%E8%A8%80%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%81%E4%B9%85%E5%8C%96%EF%BC%9A%E6%9E%84%E5%BB%BA%E5%AE%9E%E7%94%A8%E7%A8%8B%E5%BA%8F/" title="【学习】C语言文件操作与数据持久化：构建实用程序"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【学习】C语言文件操作与数据持久化：构建实用程序"></a><div class="content"><a class="title" href="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E3%80%90%E5%AD%A6%E4%B9%A0%E3%80%91C%E8%AF%AD%E8%A8%80%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%81%E4%B9%85%E5%8C%96%EF%BC%9A%E6%9E%84%E5%BB%BA%E5%AE%9E%E7%94%A8%E7%A8%8B%E5%BA%8F/" title="【学习】C语言文件操作与数据持久化：构建实用程序">【学习】C语言文件操作与数据持久化：构建实用程序</a><time datetime="2025-08-11T07:00:00.000Z" title="发表于 2025-08-11 15:00:00">2025-08-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E3%80%90%E5%AD%A6%E4%B9%A0%E3%80%91C%E8%AF%AD%E8%A8%80%E7%BB%93%E6%9E%84%E4%BD%93%E4%B8%8E%E8%81%94%E5%90%88%E4%BD%93%EF%BC%9A%E5%A4%8D%E6%9D%82%E6%95%B0%E6%8D%AE%E7%9A%84%E7%BB%84%E7%BB%87%E8%89%BA%E6%9C%AF/" title="【学习】C语言结构体与联合体：复杂数据的组织艺术"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【学习】C语言结构体与联合体：复杂数据的组织艺术"></a><div class="content"><a class="title" href="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E3%80%90%E5%AD%A6%E4%B9%A0%E3%80%91C%E8%AF%AD%E8%A8%80%E7%BB%93%E6%9E%84%E4%BD%93%E4%B8%8E%E8%81%94%E5%90%88%E4%BD%93%EF%BC%9A%E5%A4%8D%E6%9D%82%E6%95%B0%E6%8D%AE%E7%9A%84%E7%BB%84%E7%BB%87%E8%89%BA%E6%9C%AF/" title="【学习】C语言结构体与联合体：复杂数据的组织艺术">【学习】C语言结构体与联合体：复杂数据的组织艺术</a><time datetime="2025-08-11T06:00:00.000Z" title="发表于 2025-08-11 14:00:00">2025-08-11</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">©2024 - 2025 By Wake</span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="https://cdn.jsdelivr.net/npm/hexo-theme-butterfly@5.4.2/source/js/utils.min.js"></script><script src="https://cdn.jsdelivr.net/npm/hexo-theme-butterfly@5.4.2/source/js/main.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@19.1.3/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = `%%{init:{ 'theme':'${theme}'}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (true) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid@11.8.0/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const commentCount = n => {
    const isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
    if (isCommentCount) {
      isCommentCount.textContent= n
    }
  }

  const initGitalk = (el, path) => {
    if (isShuoshuo) {
      window.shuoshuoComment.destroyGitalk = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }

    const gitalk = new Gitalk({
      clientID: 'Ov23liBiL0mPM9ywqcV3',
      clientSecret: '99811bbc42de51338a2ef755501651c968f3240c',
      repo: 'uwakeme/gitalk-pool',
      owner: 'uwakeme',
      admin: ['uwakeme'],
      updateCountCallback: commentCount,
      ...option,
      id: isShuoshuo ? path : (option && option.id) || '0697f4c258d15f1c20dc132bb5a9840c'
    })

    gitalk.render('gitalk-container')
  }

  const loadGitalk = async(el, path) => {
    if (typeof Gitalk === 'function') initGitalk(el, path)
    else {
      await btf.getCSS('https://cdn.jsdelivr.net/npm/gitalk@1.8.0/dist/gitalk.min.css')
      await btf.getScript('https://cdn.jsdelivr.net/npm/gitalk@1.8.0/dist/gitalk.min.js')
      initGitalk(el, path)
    }
  }

  if (isShuoshuo) {
    'Gitalk' === 'Gitalk'
      ? window.shuoshuoComment = { loadComment: loadGitalk }
      : window.loadOtherComment = loadGitalk
    return
  }

  if ('Gitalk' === 'Gitalk' || !false) {
    if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
    else loadGitalk()
  } else {
    window.loadOtherComment = loadGitalk
  }
})()</script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/dist/fireworks.min.js"></script><script defer="defer" id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/dist/canvas-fluttering-ribbon.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/dist/canvas-nest.min.js"></script><script async="" data-pjax="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="请输入搜索内容" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/hexo-theme-butterfly@5.4.2/source/js/search/local-search.min.js"></script></div></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script><script src="/bundle.js"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>