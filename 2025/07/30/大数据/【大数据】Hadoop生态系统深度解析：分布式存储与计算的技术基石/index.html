<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>【大数据】Hadoop生态系统深度解析：分布式存储与计算的技术基石 | Uwakeme</title><meta name="author" content="Wake"><meta name="copyright" content="Wake"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="一、Hadoop概述与架构（一）Hadoop简介 什么是Hadoop  Apache Hadoop：开源分布式存储和计算框架 设计目标：处理大规模数据集的存储和分析 核心理念：移动计算而非移动数据 容错性：硬件故障是常态而非异常 可扩展性：从单机扩展到数千台机器   Hadoop发展历程  2003年：Google发布GFS和MapReduce论文 2006年：Doug Cutting创建Hado">
<meta property="og:type" content="article">
<meta property="og:title" content="【大数据】Hadoop生态系统深度解析：分布式存储与计算的技术基石">
<meta property="og:url" content="https://hexo.blog.uwakeme.tech/2025/07/30/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E3%80%90%E5%A4%A7%E6%95%B0%E6%8D%AE%E3%80%91Hadoop%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%EF%BC%9A%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E4%B8%8E%E8%AE%A1%E7%AE%97%E7%9A%84%E6%8A%80%E6%9C%AF%E5%9F%BA%E7%9F%B3/index.html">
<meta property="og:site_name" content="Uwakeme">
<meta property="og:description" content="一、Hadoop概述与架构（一）Hadoop简介 什么是Hadoop  Apache Hadoop：开源分布式存储和计算框架 设计目标：处理大规模数据集的存储和分析 核心理念：移动计算而非移动数据 容错性：硬件故障是常态而非异常 可扩展性：从单机扩展到数千台机器   Hadoop发展历程  2003年：Google发布GFS和MapReduce论文 2006年：Doug Cutting创建Hado">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png">
<meta property="article:published_time" content="2025-07-29T16:00:00.000Z">
<meta property="article:modified_time" content="2026-01-15T08:48:25.199Z">
<meta property="article:author" content="Wake">
<meta property="article:tag" content="大数据">
<meta property="article:tag" content="Hadoop">
<meta property="article:tag" content="分布式系统">
<meta property="article:tag" content="HDFS">
<meta property="article:tag" content="MapReduce">
<meta property="article:tag" content="YARN">
<meta property="article:tag" content="数据处理">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "【大数据】Hadoop生态系统深度解析：分布式存储与计算的技术基石",
  "url": "https://hexo.blog.uwakeme.tech/2025/07/30/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E3%80%90%E5%A4%A7%E6%95%B0%E6%8D%AE%E3%80%91Hadoop%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%EF%BC%9A%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E4%B8%8E%E8%AE%A1%E7%AE%97%E7%9A%84%E6%8A%80%E6%9C%AF%E5%9F%BA%E7%9F%B3/",
  "image": "https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png",
  "datePublished": "2025-07-29T16:00:00.000Z",
  "dateModified": "2026-01-15T08:48:25.199Z",
  "author": [
    {
      "@type": "Person",
      "name": "Wake",
      "url": "https://hexo.blog.uwakeme.tech/about/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/avatar.jpeg"><link rel="canonical" href="https://hexo.blog.uwakeme.tech/2025/07/30/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E3%80%90%E5%A4%A7%E6%95%B0%E6%8D%AE%E3%80%91Hadoop%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%EF%BC%9A%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E4%B8%8E%E8%AE%A1%E7%AE%97%E7%9A%84%E6%8A%80%E6%9C%AF%E5%9F%BA%E7%9F%B3/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?f96f39b60e7cc4fbcbd971a88f91f326";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
btf.addGlobalFn('pjaxComplete', () => {
  _hmt.push(['_trackPageview',window.location.pathname])
}, 'baidu_analytics')
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/local-search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: true,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '【大数据】Hadoop生态系统深度解析：分布式存储与计算的技术基石',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"><script>function loadCss(l){var d=document,h=d.head,s=d.createElement('link');s.rel='stylesheet';s.href=l;!function e(f){if (d.body)return f();setTimeout(function(){e(f)})}(function(){h.appendChild(s);});}loadCss('/style.css');loadCss('https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css');loadCss('https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/sharejs/dist/css/share.min.css');</script><noscript><link rel="stylesheet" href="/style.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/sharejs/dist/css/share.min.css"></noscript></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/img/avatar.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">252</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">688</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">28</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Uwakeme</span></a><a class="nav-page-title" href="/"><span class="site-name">【大数据】Hadoop生态系统深度解析：分布式存储与计算的技术基石</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">【大数据】Hadoop生态系统深度解析：分布式存储与计算的技术基石</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-07-29T16:00:00.000Z" title="发表于 2025-07-30 00:00:00">2025-07-30</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2026-01-15T08:48:25.199Z" title="更新于 2026-01-15 16:48:25">2026-01-15</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">4.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>20分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2025/07/30/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E3%80%90%E5%A4%A7%E6%95%B0%E6%8D%AE%E3%80%91Hadoop%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%EF%BC%9A%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E4%B8%8E%E8%AE%A1%E7%AE%97%E7%9A%84%E6%8A%80%E6%9C%AF%E5%9F%BA%E7%9F%B3/#post-comment"><span class="gitalk-comment-count"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><div id="post-outdate-notice" data="{&quot;limitDay&quot;:1000,&quot;messagePrev&quot;:&quot;本文最后更新于&quot;,&quot;messageNext&quot;:&quot;天前，文中内容可能已过时，请谨慎参考。&quot;,&quot;postUpdate&quot;:&quot;2026-01-15 16:48:25&quot;}" hidden=""></div><h1 id="一、Hadoop概述与架构"><a href="#一、Hadoop概述与架构" class="headerlink" title="一、Hadoop概述与架构"></a>一、Hadoop概述与架构</h1><h2 id="（一）Hadoop简介"><a href="#（一）Hadoop简介" class="headerlink" title="（一）Hadoop简介"></a>（一）Hadoop简介</h2><ul>
<li><p><strong>什么是Hadoop</strong></p>
<ul>
<li>Apache Hadoop：开源分布式存储和计算框架</li>
<li>设计目标：处理大规模数据集的存储和分析</li>
<li>核心理念：移动计算而非移动数据</li>
<li>容错性：硬件故障是常态而非异常</li>
<li>可扩展性：从单机扩展到数千台机器</li>
</ul>
</li>
<li><p><strong>Hadoop发展历程</strong></p>
<ul>
<li>2003年：Google发布GFS和MapReduce论文</li>
<li>2006年：Doug Cutting创建Hadoop项目</li>
<li>2008年：Hadoop成为Apache顶级项目</li>
<li>2012年：Hadoop 2.0引入YARN架构</li>
<li>现在：Hadoop 3.x版本持续演进</li>
</ul>
</li>
<li><p><strong>Hadoop应用场景</strong></p>
<ul>
<li>大数据存储：PB级数据存储和管理</li>
<li>批处理计算：离线数据分析和处理</li>
<li>数据仓库：企业级数据仓库建设</li>
<li>日志分析：网站访问日志、系统日志分析</li>
<li>机器学习：大规模机器学习数据预处理</li>
</ul>
</li>
</ul>
<h2 id="（二）Hadoop核心组件"><a href="#（二）Hadoop核心组件" class="headerlink" title="（二）Hadoop核心组件"></a>（二）Hadoop核心组件</h2><ul>
<li><p><strong>HDFS（Hadoop分布式文件系统）</strong></p>
<ul>
<li>分布式存储：数据分布在多台机器上</li>
<li>高容错性：数据自动备份，故障自动恢复</li>
<li>高吞吐量：适合大文件的顺序读写</li>
<li>流式数据访问：一次写入，多次读取</li>
<li>商用硬件：运行在普通x86服务器上</li>
</ul>
</li>
<li><p><strong>MapReduce（分布式计算框架）</strong></p>
<ul>
<li>编程模型：Map阶段和Reduce阶段</li>
<li>自动并行化：框架自动处理并行执行</li>
<li>容错处理：任务失败自动重试</li>
<li>数据本地性：计算向数据移动</li>
<li>简化编程：隐藏分布式计算复杂性</li>
</ul>
</li>
<li><p><strong>YARN（资源管理器）</strong></p>
<ul>
<li>资源管理：集群资源统一管理和调度</li>
<li>多框架支持：支持MapReduce、Spark等</li>
<li>容器化：应用运行在容器中</li>
<li>高可用性：ResourceManager高可用</li>
<li>资源隔离：CPU、内存资源隔离</li>
</ul>
</li>
</ul>
<h2 id="（三）Hadoop生态系统"><a href="#（三）Hadoop生态系统" class="headerlink" title="（三）Hadoop生态系统"></a>（三）Hadoop生态系统</h2><ul>
<li><p><strong>数据存储层</strong></p>
<ul>
<li>HDFS：分布式文件系统</li>
<li>HBase：NoSQL列式数据库</li>
<li>Kudu：实时分析存储引擎</li>
<li>Alluxio：内存分布式存储系统</li>
</ul>
</li>
<li><p><strong>数据处理层</strong></p>
<ul>
<li>MapReduce：批处理计算框架</li>
<li>Spark：内存计算框架</li>
<li>Flink：流处理计算框架</li>
<li>Storm：实时流处理系统</li>
</ul>
</li>
<li><p><strong>数据管理层</strong></p>
<ul>
<li>Hive：数据仓库软件，SQL查询</li>
<li>Pig：数据流语言和执行环境</li>
<li>Sqoop：关系数据库数据导入导出</li>
<li>Flume：日志收集系统</li>
</ul>
</li>
</ul>
<h1 id="二、HDFS分布式文件系统"><a href="#二、HDFS分布式文件系统" class="headerlink" title="二、HDFS分布式文件系统"></a>二、HDFS分布式文件系统</h1><h2 id="（一）HDFS架构设计"><a href="#（一）HDFS架构设计" class="headerlink" title="（一）HDFS架构设计"></a>（一）HDFS架构设计</h2><ul>
<li><p><strong>主从架构</strong></p>
<ul>
<li>NameNode：主节点，管理文件系统元数据</li>
<li>DataNode：从节点，存储实际数据块</li>
<li>Secondary NameNode：辅助NameNode，定期合并元数据</li>
<li>客户端：文件系统访问接口</li>
</ul>
</li>
<li><p><strong>数据存储机制</strong></p>
<ul>
<li>块存储：文件分割成固定大小的块（默认128MB）</li>
<li>副本机制：每个块默认存储3个副本</li>
<li>副本放置策略：机架感知，提高可靠性和性能</li>
<li>数据完整性：校验和机制检测数据损坏</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># HDFS基本操作命令</span></span><br><span class="line"><span class="comment"># 查看HDFS文件系统</span></span><br><span class="line">hdfs dfs -<span class="built_in">ls</span> /</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建目录</span></span><br><span class="line">hdfs dfs -<span class="built_in">mkdir</span> /user/data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 上传文件到HDFS</span></span><br><span class="line">hdfs dfs -put localfile.txt /user/data/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从HDFS下载文件</span></span><br><span class="line">hdfs dfs -get /user/data/hdfsfile.txt ./</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看文件内容</span></span><br><span class="line">hdfs dfs -<span class="built_in">cat</span> /user/data/file.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除文件</span></span><br><span class="line">hdfs dfs -<span class="built_in">rm</span> /user/data/file.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看文件系统状态</span></span><br><span class="line">hdfs dfsadmin -report</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查文件系统健康状态</span></span><br><span class="line">hdfs fsck /</span><br></pre></td></tr></tbody></table></figure>

<h2 id="（二）NameNode详解"><a href="#（二）NameNode详解" class="headerlink" title="（二）NameNode详解"></a>（二）NameNode详解</h2><ul>
<li><p><strong>元数据管理</strong></p>
<ul>
<li>文件系统树：目录结构和文件信息</li>
<li>块映射：文件块到DataNode的映射关系</li>
<li>内存存储：元数据全部加载到内存中</li>
<li>持久化：FSImage和EditLog文件</li>
</ul>
</li>
<li><p><strong>NameNode高可用（HA）</strong></p>
<ul>
<li>Active/Standby模式：主备NameNode</li>
<li>共享存储：QJM（Quorum Journal Manager）</li>
<li>自动故障转移：ZKFC（ZooKeeper Failover Controller）</li>
<li>数据同步：实时同步元数据变更</li>
</ul>
</li>
</ul>
<figure class="highlight xml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- NameNode HA配置示例 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 配置NameNode集群ID --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">&lt;!-- 配置NameNode节点 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.mycluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">&lt;!-- 配置NameNode RPC地址 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.mycluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>namenode1:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">&lt;!-- 配置共享存储 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://journal1:8485;journal2:8485;journal3:8485/mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure>

<h2 id="（三）DataNode详解"><a href="#（三）DataNode详解" class="headerlink" title="（三）DataNode详解"></a>（三）DataNode详解</h2><ul>
<li><p><strong>数据存储</strong></p>
<ul>
<li>块存储：将文件块存储在本地文件系统</li>
<li>多目录：支持多个存储目录，提高I/O性能</li>
<li>数据校验：定期检查数据块完整性</li>
<li>心跳机制：定期向NameNode报告状态</li>
</ul>
</li>
<li><p><strong>数据读写流程</strong></p>
<ul>
<li>写入流程：客户端→NameNode→DataNode管道写入</li>
<li>读取流程：客户端→NameNode获取位置→直接从DataNode读取</li>
<li>数据本地性：优先读取本地或同机架的数据</li>
<li>负载均衡：自动平衡各DataNode的存储负载</li>
</ul>
</li>
</ul>
<figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// HDFS Java API使用示例</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HDFSExample</span> {</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception {</span><br><span class="line">        <span class="comment">// 创建配置对象</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://namenode:8020"</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 获取文件系统对象</span></span><br><span class="line">        <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(conf);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 创建文件并写入数据</span></span><br><span class="line">        <span class="type">Path</span> <span class="variable">outputPath</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">"/user/data/output.txt"</span>);</span><br><span class="line">        <span class="type">FSDataOutputStream</span> <span class="variable">out</span> <span class="operator">=</span> fs.create(outputPath);</span><br><span class="line">        out.writeUTF(<span class="string">"Hello Hadoop HDFS!"</span>);</span><br><span class="line">        out.close();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 读取文件数据</span></span><br><span class="line">        <span class="type">FSDataInputStream</span> <span class="variable">in</span> <span class="operator">=</span> fs.open(outputPath);</span><br><span class="line">        <span class="type">String</span> <span class="variable">content</span> <span class="operator">=</span> in.readUTF();</span><br><span class="line">        System.out.println(<span class="string">"文件内容: "</span> + content);</span><br><span class="line">        in.close();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 关闭文件系统</span></span><br><span class="line">        fs.close();</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<h1 id="三、MapReduce计算框架"><a href="#三、MapReduce计算框架" class="headerlink" title="三、MapReduce计算框架"></a>三、MapReduce计算框架</h1><h2 id="（一）MapReduce编程模型"><a href="#（一）MapReduce编程模型" class="headerlink" title="（一）MapReduce编程模型"></a>（一）MapReduce编程模型</h2><ul>
<li><p><strong>Map阶段</strong></p>
<ul>
<li>输入分片：将输入数据分割成独立的块</li>
<li>Map函数：处理键值对，输出中间结果</li>
<li>分区：根据key将Map输出分配到不同Reducer</li>
<li>排序：对Map输出按key排序</li>
<li>合并：可选的Combiner减少网络传输</li>
</ul>
</li>
<li><p><strong>Reduce阶段</strong></p>
<ul>
<li>Shuffle：从Map任务获取中间结果</li>
<li>排序：对相同key的值进行分组</li>
<li>Reduce函数：处理分组后的数据</li>
<li>输出：将最终结果写入HDFS</li>
</ul>
</li>
</ul>
<figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// WordCount MapReduce示例</span></span><br><span class="line"><span class="comment">// Mapper类</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, IntWritable&gt; {</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="type">IntWritable</span> <span class="variable">one</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">word</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Context context)</span> </span><br><span class="line">            <span class="keyword">throws</span> IOException, InterruptedException {</span><br><span class="line">        <span class="comment">// 将输入文本转换为小写并分割成单词</span></span><br><span class="line">        String[] words = value.toString().toLowerCase().split(<span class="string">"\\s+"</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 为每个单词输出 (word, 1)</span></span><br><span class="line">        <span class="keyword">for</span> (String w : words) {</span><br><span class="line">            word.set(w);</span><br><span class="line">            context.write(word, one);</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// Reducer类</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt; {</span><br><span class="line">    <span class="keyword">private</span> <span class="type">IntWritable</span> <span class="variable">result</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span></span><br><span class="line">            <span class="keyword">throws</span> IOException, InterruptedException {</span><br><span class="line">        <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 计算每个单词的总数</span></span><br><span class="line">        <span class="keyword">for</span> (IntWritable value : values) {</span><br><span class="line">            sum += value.get();</span><br><span class="line">        }</span><br><span class="line">        </span><br><span class="line">        result.set(sum);</span><br><span class="line">        context.write(key, result);</span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// Driver类</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountDriver</span> {</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception {</span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf, <span class="string">"word count"</span>);</span><br><span class="line">        </span><br><span class="line">        job.setJarByClass(WordCountDriver.class);</span><br><span class="line">        job.setMapperClass(WordCountMapper.class);</span><br><span class="line">        job.setCombinerClass(WordCountReducer.class);  <span class="comment">// 使用Reducer作为Combiner</span></span><br><span class="line">        job.setReducerClass(WordCountReducer.class);</span><br><span class="line">        </span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line">        </span><br><span class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]));</span><br><span class="line">        </span><br><span class="line">        System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<h2 id="（二）MapReduce执行流程"><a href="#（二）MapReduce执行流程" class="headerlink" title="（二）MapReduce执行流程"></a>（二）MapReduce执行流程</h2><ul>
<li><p><strong>作业提交</strong></p>
<ul>
<li>客户端提交作业到ResourceManager</li>
<li>ResourceManager分配ApplicationMaster</li>
<li>ApplicationMaster向ResourceManager申请资源</li>
<li>启动Map和Reduce任务</li>
</ul>
</li>
<li><p><strong>任务执行</strong></p>
<ul>
<li>Map任务：读取输入分片，执行Map函数</li>
<li>Shuffle阶段：Map输出传输到Reducer</li>
<li>Reduce任务：执行Reduce函数，输出结果</li>
<li>任务监控：跟踪任务进度和状态</li>
</ul>
</li>
</ul>
<h2 id="（三）MapReduce优化技术"><a href="#（三）MapReduce优化技术" class="headerlink" title="（三）MapReduce优化技术"></a>（三）MapReduce优化技术</h2><ul>
<li><p><strong>输入优化</strong></p>
<ul>
<li>文件格式：使用SequenceFile、Avro等高效格式</li>
<li>压缩：启用输入数据压缩减少I/O</li>
<li>分片大小：调整输入分片大小优化并行度</li>
<li>数据本地性：优化数据分布提高本地性</li>
</ul>
</li>
<li><p><strong>执行优化</strong></p>
<ul>
<li>Combiner：减少Map输出数据量</li>
<li>压缩：启用Map输出和最终输出压缩</li>
<li>内存调优：调整JVM堆大小和缓冲区</li>
<li>推测执行：启用推测执行处理慢任务</li>
</ul>
</li>
</ul>
<figure class="highlight xml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- MapReduce性能优化配置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 启用Map输出压缩 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.output.compress<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">&lt;!-- 设置Map输出压缩编解码器 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.output.compress.codec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.io.compress.SnappyCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">&lt;!-- 调整Map任务内存 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">&lt;!-- 调整Reduce任务内存 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>4096<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">&lt;!-- 启用推测执行 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure>

<h1 id="四、YARN资源管理"><a href="#四、YARN资源管理" class="headerlink" title="四、YARN资源管理"></a>四、YARN资源管理</h1><h2 id="（一）YARN架构组件"><a href="#（一）YARN架构组件" class="headerlink" title="（一）YARN架构组件"></a>（一）YARN架构组件</h2><ul>
<li><p><strong>ResourceManager（RM）</strong></p>
<ul>
<li>全局资源管理：管理整个集群的资源</li>
<li>应用管理：接收作业提交，启动ApplicationMaster</li>
<li>调度器：根据策略分配资源给应用</li>
<li>高可用：支持Active/Standby模式</li>
</ul>
</li>
<li><p><strong>NodeManager（NM）</strong></p>
<ul>
<li>节点资源管理：管理单个节点的资源</li>
<li>容器管理：启动和监控容器</li>
<li>健康检查：监控节点健康状态</li>
<li>日志管理：收集和管理应用日志</li>
</ul>
</li>
<li><p><strong>ApplicationMaster（AM）</strong></p>
<ul>
<li>应用协调：协调应用的执行</li>
<li>资源申请：向ResourceManager申请资源</li>
<li>任务调度：在分配的容器中启动任务</li>
<li>容错处理：处理任务失败和重试</li>
</ul>
</li>
<li><p><strong>Container（容器）</strong></p>
<ul>
<li>资源封装：封装CPU、内存等资源</li>
<li>任务执行：应用任务的执行环境</li>
<li>资源隔离：提供资源隔离保证</li>
<li>生命周期管理：容器的创建、运行、销毁</li>
</ul>
</li>
</ul>
<h2 id="（二）YARN调度器"><a href="#（二）YARN调度器" class="headerlink" title="（二）YARN调度器"></a>（二）YARN调度器</h2><ul>
<li><p><strong>FIFO调度器</strong></p>
<ul>
<li>先进先出：按提交顺序执行作业</li>
<li>简单实现：适合小规模集群</li>
<li>资源利用率低：大作业会阻塞小作业</li>
<li>不支持优先级：无法区分作业重要性</li>
</ul>
</li>
<li><p><strong>容量调度器（Capacity Scheduler）</strong></p>
<ul>
<li>队列管理：支持多个队列，队列间资源隔离</li>
<li>容量保证：每个队列有最小资源保证</li>
<li>弹性资源：空闲资源可被其他队列使用</li>
<li>层次队列：支持队列嵌套，细粒度管理</li>
</ul>
</li>
<li><p><strong>公平调度器（Fair Scheduler）</strong></p>
<ul>
<li>公平共享：所有应用公平共享资源</li>
<li>抢占机制：支持资源抢占保证公平性</li>
<li>多种策略：支持FIFO、Fair、DRF等策略</li>
<li>动态配置：支持运行时配置修改</li>
</ul>
</li>
</ul>
<figure class="highlight xml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 容量调度器配置示例 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 启用容量调度器 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">&lt;!-- 配置队列 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.queues<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>default,production,development<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">&lt;!-- 配置队列容量 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.default.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>40<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.production.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>40<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.development.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>20<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure>

<h2 id="（三）YARN应用开发"><a href="#（三）YARN应用开发" class="headerlink" title="（三）YARN应用开发"></a>（三）YARN应用开发</h2><ul>
<li><p><strong>应用提交流程</strong></p>
<ul>
<li>客户端向ResourceManager提交应用</li>
<li>ResourceManager分配容器启动ApplicationMaster</li>
<li>ApplicationMaster向ResourceManager注册</li>
<li>ApplicationMaster申请资源启动任务</li>
<li>任务完成后ApplicationMaster注销</li>
</ul>
</li>
<li><p><strong>编程接口</strong></p>
<ul>
<li>Client API：应用提交和监控</li>
<li>ApplicationMaster API：资源申请和任务管理</li>
<li>Container API：容器生命周期管理</li>
<li>Timeline Service：应用历史信息存储</li>
</ul>
</li>
</ul>
<figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// YARN应用客户端示例</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">YarnClient</span> {</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception {</span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">YarnClient</span> <span class="variable">yarnClient</span> <span class="operator">=</span> YarnClient.createYarnClient();</span><br><span class="line">        yarnClient.init(conf);</span><br><span class="line">        yarnClient.start();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 创建应用</span></span><br><span class="line">        <span class="type">YarnClientApplication</span> <span class="variable">app</span> <span class="operator">=</span> yarnClient.createApplication();</span><br><span class="line">        <span class="type">ApplicationSubmissionContext</span> <span class="variable">appContext</span> <span class="operator">=</span> app.getApplicationSubmissionContext();</span><br><span class="line">        <span class="type">ApplicationId</span> <span class="variable">appId</span> <span class="operator">=</span> appContext.getApplicationId();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 设置应用信息</span></span><br><span class="line">        appContext.setApplicationName(<span class="string">"MyYarnApp"</span>);</span><br><span class="line">        appContext.setApplicationType(<span class="string">"MAPREDUCE"</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 设置ApplicationMaster</span></span><br><span class="line">        <span class="type">ContainerLaunchContext</span> <span class="variable">amContainer</span> <span class="operator">=</span> ContainerLaunchContext.newInstance(</span><br><span class="line">            <span class="literal">null</span>, <span class="literal">null</span>, <span class="literal">null</span>, <span class="literal">null</span>, <span class="literal">null</span>, <span class="literal">null</span>);</span><br><span class="line">        appContext.setAMContainerSpec(amContainer);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 设置资源需求</span></span><br><span class="line">        <span class="type">Resource</span> <span class="variable">capability</span> <span class="operator">=</span> Resource.newInstance(<span class="number">1024</span>, <span class="number">1</span>);</span><br><span class="line">        appContext.setResource(capability);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 提交应用</span></span><br><span class="line">        yarnClient.submitApplication(appContext);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 监控应用状态</span></span><br><span class="line">        <span class="type">ApplicationReport</span> <span class="variable">appReport</span> <span class="operator">=</span> yarnClient.getApplicationReport(appId);</span><br><span class="line">        <span class="type">YarnApplicationState</span> <span class="variable">appState</span> <span class="operator">=</span> appReport.getYarnApplicationState();</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> (appState != YarnApplicationState.FINISHED &amp;&amp; </span><br><span class="line">               appState != YarnApplicationState.KILLED &amp;&amp; </span><br><span class="line">               appState != YarnApplicationState.FAILED) {</span><br><span class="line">            Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">            appReport = yarnClient.getApplicationReport(appId);</span><br><span class="line">            appState = appReport.getYarnApplicationState();</span><br><span class="line">        }</span><br><span class="line">        </span><br><span class="line">        yarnClient.close();</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<h1 id="五、Hadoop生态系统组件"><a href="#五、Hadoop生态系统组件" class="headerlink" title="五、Hadoop生态系统组件"></a>五、Hadoop生态系统组件</h1><h2 id="（一）Hive数据仓库"><a href="#（一）Hive数据仓库" class="headerlink" title="（一）Hive数据仓库"></a>（一）Hive数据仓库</h2><ul>
<li><p><strong>Hive概述</strong></p>
<ul>
<li>SQL接口：提供类SQL查询语言HiveQL</li>
<li>元数据管理：存储表结构和分区信息</li>
<li>数据存储：数据存储在HDFS上</li>
<li>执行引擎：支持MapReduce、Spark、Tez</li>
</ul>
</li>
<li><p><strong>Hive架构</strong></p>
<ul>
<li>Hive CLI：命令行接口</li>
<li>HiveServer2：JDBC/ODBC服务</li>
<li>Metastore：元数据存储服务</li>
<li>Driver：查询编译和优化</li>
<li>执行引擎：查询执行引擎</li>
</ul>
</li>
</ul>
<figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- Hive SQL示例</span></span><br><span class="line"><span class="comment">-- 创建数据库</span></span><br><span class="line"><span class="keyword">CREATE</span> DATABASE IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> sales_db;</span><br><span class="line">USE sales_db;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 创建外部表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> sales_data (</span><br><span class="line">    order_id STRING,</span><br><span class="line">    customer_id STRING,</span><br><span class="line">    product_id STRING,</span><br><span class="line">    quantity <span class="type">INT</span>,</span><br><span class="line">    price <span class="type">DECIMAL</span>(<span class="number">10</span>,<span class="number">2</span>),</span><br><span class="line">    order_date STRING</span><br><span class="line">)</span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (<span class="keyword">year</span> <span class="type">INT</span>, <span class="keyword">month</span> <span class="type">INT</span>)</span><br><span class="line">STORED <span class="keyword">AS</span> TEXTFILE</span><br><span class="line">LOCATION <span class="string">'/user/data/sales/'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 添加分区</span></span><br><span class="line"><span class="keyword">ALTER TABLE</span> sales_data <span class="keyword">ADD</span> <span class="keyword">PARTITION</span> (<span class="keyword">year</span><span class="operator">=</span><span class="number">2023</span>, <span class="keyword">month</span><span class="operator">=</span><span class="number">12</span>)</span><br><span class="line">LOCATION <span class="string">'/user/data/sales/2023/12/'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 查询数据</span></span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    customer_id,</span><br><span class="line">    <span class="built_in">SUM</span>(quantity <span class="operator">*</span> price) <span class="keyword">as</span> total_amount</span><br><span class="line"><span class="keyword">FROM</span> sales_data</span><br><span class="line"><span class="keyword">WHERE</span> <span class="keyword">year</span> <span class="operator">=</span> <span class="number">2023</span> <span class="keyword">AND</span> <span class="keyword">month</span> <span class="operator">=</span> <span class="number">12</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> customer_id</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> total_amount <span class="keyword">DESC</span></span><br><span class="line">LIMIT <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 创建内部表并插入数据</span></span><br><span class="line"><span class="keyword">CREATE TABLE</span> customer_summary <span class="keyword">AS</span></span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    customer_id,</span><br><span class="line">    <span class="built_in">COUNT</span>(<span class="operator">*</span>) <span class="keyword">as</span> order_count,</span><br><span class="line">    <span class="built_in">SUM</span>(quantity <span class="operator">*</span> price) <span class="keyword">as</span> total_spent,</span><br><span class="line">    <span class="built_in">AVG</span>(quantity <span class="operator">*</span> price) <span class="keyword">as</span> avg_order_value</span><br><span class="line"><span class="keyword">FROM</span> sales_data</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> customer_id;</span><br></pre></td></tr></tbody></table></figure>

<h2 id="（二）HBase-NoSQL数据库"><a href="#（二）HBase-NoSQL数据库" class="headerlink" title="（二）HBase NoSQL数据库"></a>（二）HBase NoSQL数据库</h2><ul>
<li><p><strong>HBase特点</strong></p>
<ul>
<li>列式存储：按列族存储数据</li>
<li>实时读写：支持随机实时读写</li>
<li>自动分片：Region自动分割和负载均衡</li>
<li>强一致性：提供强一致性保证</li>
<li>水平扩展：支持线性扩展</li>
</ul>
</li>
<li><p><strong>HBase数据模型</strong></p>
<ul>
<li>表（Table）：数据存储的逻辑单元</li>
<li>行键（Row Key）：唯一标识一行数据</li>
<li>列族（Column Family）：列的逻辑分组</li>
<li>列限定符（Column Qualifier）：列的具体名称</li>
<li>时间戳（Timestamp）：数据版本标识</li>
</ul>
</li>
</ul>
<figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// HBase Java API示例</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.HBaseConfiguration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.Bytes;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HBaseExample</span> {</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception {</span><br><span class="line">        <span class="comment">// 创建配置</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> HBaseConfiguration.create();</span><br><span class="line">        conf.set(<span class="string">"hbase.zookeeper.quorum"</span>, <span class="string">"zk1,zk2,zk3"</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 创建连接</span></span><br><span class="line">        <span class="type">Connection</span> <span class="variable">connection</span> <span class="operator">=</span> ConnectionFactory.createConnection(conf);</span><br><span class="line">        <span class="type">Table</span> <span class="variable">table</span> <span class="operator">=</span> connection.getTable(TableName.valueOf(<span class="string">"user_profile"</span>));</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 插入数据</span></span><br><span class="line">        <span class="type">Put</span> <span class="variable">put</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Put</span>(Bytes.toBytes(<span class="string">"user001"</span>));</span><br><span class="line">        put.addColumn(Bytes.toBytes(<span class="string">"info"</span>), Bytes.toBytes(<span class="string">"name"</span>), Bytes.toBytes(<span class="string">"张三"</span>));</span><br><span class="line">        put.addColumn(Bytes.toBytes(<span class="string">"info"</span>), Bytes.toBytes(<span class="string">"age"</span>), Bytes.toBytes(<span class="string">"25"</span>));</span><br><span class="line">        put.addColumn(Bytes.toBytes(<span class="string">"contact"</span>), Bytes.toBytes(<span class="string">"email"</span>), Bytes.toBytes(<span class="string">"zhangsan@example.com"</span>));</span><br><span class="line">        table.put(put);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 查询数据</span></span><br><span class="line">        <span class="type">Get</span> <span class="variable">get</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Get</span>(Bytes.toBytes(<span class="string">"user001"</span>));</span><br><span class="line">        <span class="type">Result</span> <span class="variable">result</span> <span class="operator">=</span> table.get(get);</span><br><span class="line">        </span><br><span class="line">        <span class="type">String</span> <span class="variable">name</span> <span class="operator">=</span> Bytes.toString(result.getValue(Bytes.toBytes(<span class="string">"info"</span>), Bytes.toBytes(<span class="string">"name"</span>)));</span><br><span class="line">        <span class="type">String</span> <span class="variable">age</span> <span class="operator">=</span> Bytes.toString(result.getValue(Bytes.toBytes(<span class="string">"info"</span>), Bytes.toBytes(<span class="string">"age"</span>)));</span><br><span class="line">        </span><br><span class="line">        System.out.println(<span class="string">"姓名: "</span> + name + <span class="string">", 年龄: "</span> + age);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 扫描数据</span></span><br><span class="line">        <span class="type">Scan</span> <span class="variable">scan</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scan</span>();</span><br><span class="line">        scan.addFamily(Bytes.toBytes(<span class="string">"info"</span>));</span><br><span class="line">        <span class="type">ResultScanner</span> <span class="variable">scanner</span> <span class="operator">=</span> table.getScanner(scan);</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> (Result r : scanner) {</span><br><span class="line">            <span class="type">String</span> <span class="variable">rowKey</span> <span class="operator">=</span> Bytes.toString(r.getRow());</span><br><span class="line">            <span class="type">String</span> <span class="variable">userName</span> <span class="operator">=</span> Bytes.toString(r.getValue(Bytes.toBytes(<span class="string">"info"</span>), Bytes.toBytes(<span class="string">"name"</span>)));</span><br><span class="line">            System.out.println(<span class="string">"用户ID: "</span> + rowKey + <span class="string">", 姓名: "</span> + userName);</span><br><span class="line">        }</span><br><span class="line">        </span><br><span class="line">        scanner.close();</span><br><span class="line">        table.close();</span><br><span class="line">        connection.close();</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<h2 id="（三）Spark计算引擎"><a href="#（三）Spark计算引擎" class="headerlink" title="（三）Spark计算引擎"></a>（三）Spark计算引擎</h2><ul>
<li><p><strong>Spark优势</strong></p>
<ul>
<li>内存计算：数据缓存在内存中，提高性能</li>
<li>多语言支持：支持Scala、Java、Python、R</li>
<li>统一平台：批处理、流处理、机器学习、图计算</li>
<li>易用性：丰富的高级API和算子</li>
<li>容错性：RDD血缘关系提供容错能力</li>
</ul>
</li>
<li><p><strong>Spark核心概念</strong></p>
<ul>
<li>RDD：弹性分布式数据集</li>
<li>DataFrame：结构化数据抽象</li>
<li>Dataset：类型安全的数据抽象</li>
<li>Spark SQL：结构化数据处理</li>
<li>Spark Streaming：流数据处理</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># PySpark示例</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建SparkSession</span></span><br><span class="line">spark = SparkSession.builder \</span><br><span class="line">    .appName(<span class="string">"SparkExample"</span>) \</span><br><span class="line">    .config(<span class="string">"spark.sql.adaptive.enabled"</span>, <span class="string">"true"</span>) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line">df = spark.read.option(<span class="string">"header"</span>, <span class="string">"true"</span>).csv(<span class="string">"hdfs://namenode:8020/user/data/sales.csv"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据处理</span></span><br><span class="line">result = df.groupBy(<span class="string">"customer_id"</span>) \</span><br><span class="line">    .agg(</span><br><span class="line">        count(<span class="string">"order_id"</span>).alias(<span class="string">"order_count"</span>),</span><br><span class="line">        <span class="built_in">sum</span>(<span class="string">"amount"</span>).alias(<span class="string">"total_amount"</span>),</span><br><span class="line">        avg(<span class="string">"amount"</span>).alias(<span class="string">"avg_amount"</span>)</span><br><span class="line">    ) \</span><br><span class="line">    .<span class="built_in">filter</span>(col(<span class="string">"order_count"</span>) &gt; <span class="number">5</span>) \</span><br><span class="line">    .orderBy(desc(<span class="string">"total_amount"</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示结果</span></span><br><span class="line">result.show(<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 写入结果</span></span><br><span class="line">result.write \</span><br><span class="line">    .mode(<span class="string">"overwrite"</span>) \</span><br><span class="line">    .option(<span class="string">"header"</span>, <span class="string">"true"</span>) \</span><br><span class="line">    .csv(<span class="string">"hdfs://namenode:8020/user/output/customer_analysis"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 停止SparkSession</span></span><br><span class="line">spark.stop()</span><br></pre></td></tr></tbody></table></figure>

<h1 id="六、Hadoop集群部署与管理"><a href="#六、Hadoop集群部署与管理" class="headerlink" title="六、Hadoop集群部署与管理"></a>六、Hadoop集群部署与管理</h1><h2 id="（一）集群规划与部署"><a href="#（一）集群规划与部署" class="headerlink" title="（一）集群规划与部署"></a>（一）集群规划与部署</h2><ul>
<li><p><strong>硬件规划</strong></p>
<ul>
<li>NameNode：高内存、SSD存储、双网卡</li>
<li>DataNode：大容量存储、多磁盘、高网络带宽</li>
<li>ResourceManager：中等配置、高可用部署</li>
<li>网络：万兆以太网、交换机配置</li>
</ul>
</li>
<li><p><strong>软件部署</strong></p>
<ul>
<li>操作系统：CentOS、Ubuntu LTS版本</li>
<li>Java环境：OpenJDK 8或11</li>
<li>Hadoop安装：二进制包部署或编译安装</li>
<li>配置文件：core-site.xml、hdfs-site.xml、yarn-site.xml</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment"># Hadoop集群部署脚本</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置环境变量</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/java-8-openjdk</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/opt/hadoop</span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=<span class="variable">$HADOOP_HOME</span>/etc/hadoop</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建Hadoop用户</span></span><br><span class="line">useradd -m hadoop</span><br><span class="line">usermod -aG <span class="built_in">sudo</span> hadoop</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置SSH免密登录</span></span><br><span class="line">su - hadoop -c <span class="string">"ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa"</span></span><br><span class="line">su - hadoop -c <span class="string">"cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys"</span></span><br><span class="line">su - hadoop -c <span class="string">"chmod 600 ~/.ssh/authorized_keys"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载并解压Hadoop</span></span><br><span class="line"><span class="built_in">cd</span> /opt</span><br><span class="line">wget https://downloads.apache.org/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz</span><br><span class="line">tar -xzf hadoop-3.3.4.tar.gz</span><br><span class="line"><span class="built_in">mv</span> hadoop-3.3.4 hadoop</span><br><span class="line"><span class="built_in">chown</span> -R hadoop:hadoop hadoop</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置Hadoop环境变量</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">'export HADOOP_HOME=/opt/hadoop'</span> &gt;&gt; /home/hadoop/.bashrc</span><br><span class="line"><span class="built_in">echo</span> <span class="string">'export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop'</span> &gt;&gt; /home/hadoop/.bashrc</span><br><span class="line"><span class="built_in">echo</span> <span class="string">'export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin'</span> &gt;&gt; /home/hadoop/.bashrc</span><br><span class="line"></span><br><span class="line"><span class="comment"># 格式化NameNode（仅在NameNode节点执行）</span></span><br><span class="line">su - hadoop -c <span class="string">"<span class="variable">$HADOOP_HOME</span>/bin/hdfs namenode -format -force"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动Hadoop集群</span></span><br><span class="line">su - hadoop -c <span class="string">"<span class="variable">$HADOOP_HOME</span>/sbin/start-dfs.sh"</span></span><br><span class="line">su - hadoop -c <span class="string">"<span class="variable">$HADOOP_HOME</span>/sbin/start-yarn.sh"</span></span><br></pre></td></tr></tbody></table></figure>

<h2 id="（二）集群监控与管理"><a href="#（二）集群监控与管理" class="headerlink" title="（二）集群监控与管理"></a>（二）集群监控与管理</h2><ul>
<li><p><strong>Web界面监控</strong></p>
<ul>
<li>NameNode Web UI：<a target="_blank" rel="noopener" href="http://namenode:9870/">http://namenode:9870</a></li>
<li>ResourceManager Web UI：<a target="_blank" rel="noopener" href="http://resourcemanager:8088/">http://resourcemanager:8088</a></li>
<li>DataNode Web UI：<a target="_blank" rel="noopener" href="http://datanode:9864/">http://datanode:9864</a></li>
<li>NodeManager Web UI：<a target="_blank" rel="noopener" href="http://nodemanager:8042/">http://nodemanager:8042</a></li>
</ul>
</li>
<li><p><strong>命令行监控</strong></p>
<ul>
<li>集群状态：hdfs dfsadmin -report</li>
<li>节点状态：yarn node -list</li>
<li>应用状态：yarn application -list</li>
<li>日志查看：yarn logs -applicationId app_id</li>
</ul>
</li>
<li><p><strong>第三方监控工具</strong></p>
<ul>
<li>Ambari：集群管理和监控平台</li>
<li>Cloudera Manager：企业级集群管理</li>
<li>Ganglia：分布式监控系统</li>
<li>Nagios：网络监控系统</li>
<li>Prometheus + Grafana：现代监控方案</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Hadoop集群健康检查脚本</span></span><br><span class="line"><span class="comment">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"=== Hadoop集群健康检查 ==="</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查HDFS状态</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"1. HDFS文件系统状态："</span></span><br><span class="line">hdfs dfsadmin -report | grep -E <span class="string">"Live datanodes|Dead datanodes|DFS Used%"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查HDFS文件系统完整性</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"2. HDFS文件系统完整性检查："</span></span><br><span class="line">hdfs fsck / -files -blocks -locations | <span class="built_in">tail</span> -10</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查YARN集群状态</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"3. YARN集群状态："</span></span><br><span class="line">yarn node -list -all | grep -E <span class="string">"RUNNING|UNHEALTHY|LOST"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查运行中的应用</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"4. 运行中的应用："</span></span><br><span class="line">yarn application -list -appStates RUNNING</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查集群资源使用情况</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"5. 集群资源使用情况："</span></span><br><span class="line">yarn top</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查关键服务进程</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"6. 关键服务进程检查："</span></span><br><span class="line">jps | grep -E <span class="string">"NameNode|DataNode|ResourceManager|NodeManager"</span></span><br></pre></td></tr></tbody></table></figure>

<h2 id="（三）性能调优与故障排除"><a href="#（三）性能调优与故障排除" class="headerlink" title="（三）性能调优与故障排除"></a>（三）性能调优与故障排除</h2><ul>
<li><p><strong>HDFS性能调优</strong></p>
<ul>
<li>块大小优化：根据文件大小调整块大小</li>
<li>副本数量：根据可靠性需求调整副本数</li>
<li>压缩配置：启用数据压缩减少存储和网络开销</li>
<li>缓存配置：配置HDFS缓存提高读性能</li>
</ul>
</li>
<li><p><strong>YARN性能调优</strong></p>
<ul>
<li>内存配置：合理配置容器内存大小</li>
<li>CPU配置：启用CPU资源调度</li>
<li>调度器优化：选择合适的调度器和配置</li>
<li>本地化：提高数据本地性减少网络传输</li>
</ul>
</li>
<li><p><strong>常见故障排除</strong></p>
<ul>
<li>NameNode故障：检查内存、磁盘空间、网络</li>
<li>DataNode故障：检查磁盘健康、网络连接</li>
<li>作业失败：检查日志、资源配置、数据格式</li>
<li>性能问题：分析瓶颈、优化配置、硬件升级</li>
</ul>
</li>
</ul>
<hr>
<p><strong>总结</strong>：Hadoop作为大数据处理的基础平台，提供了可靠的分布式存储和计算能力。HDFS解决了大规模数据存储问题，MapReduce提供了简单易用的分布式计算模型，YARN实现了资源的统一管理和调度。</p>
<p>随着大数据技术的发展，Hadoop生态系统不断丰富，Spark、Flink等新一代计算引擎在某些场景下提供了更好的性能。但Hadoop作为大数据的基石，其稳定性、可靠性和成熟的生态系统仍然使其在企业级大数据应用中占据重要地位。</p>
<p>学习Hadoop需要理解分布式系统的基本概念，掌握HDFS、MapReduce、YARN的核心原理，熟悉生态系统中各组件的使用。在实际应用中，要根据业务需求选择合适的技术栈，合理规划集群架构，做好性能调优和运维管理。掌握Hadoop技术，将为您在大数据领域的发展奠定坚实的基础。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://hexo.blog.uwakeme.tech/about/">Wake</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://hexo.blog.uwakeme.tech/2025/07/30/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E3%80%90%E5%A4%A7%E6%95%B0%E6%8D%AE%E3%80%91Hadoop%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%EF%BC%9A%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E4%B8%8E%E8%AE%A1%E7%AE%97%E7%9A%84%E6%8A%80%E6%9C%AF%E5%9F%BA%E7%9F%B3/">https://hexo.blog.uwakeme.tech/2025/07/30/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E3%80%90%E5%A4%A7%E6%95%B0%E6%8D%AE%E3%80%91Hadoop%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%EF%BC%9A%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E4%B8%8E%E8%AE%A1%E7%AE%97%E7%9A%84%E6%8A%80%E6%9C%AF%E5%9F%BA%E7%9F%B3/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://hexo.blog.uwakeme.tech" target="_blank">Uwakeme</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a><a class="post-meta__tags" href="/tags/Hadoop/">Hadoop</a><a class="post-meta__tags" href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/">分布式系统</a><a class="post-meta__tags" href="/tags/HDFS/">HDFS</a><a class="post-meta__tags" href="/tags/MapReduce/">MapReduce</a><a class="post-meta__tags" href="/tags/YARN/">YARN</a><a class="post-meta__tags" href="/tags/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/">数据处理</a></div><div class="post-share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/sharejs/dist/js/social-share.min.js" defer=""></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/reward/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/img/reward/wechat.jpg" alt="微信"></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/reward/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/img/reward/alipay.jpg" alt="支付宝"></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/08/06/%E5%89%8D%E7%AB%AF/%E3%80%90%E5%89%8D%E7%AB%AF%E3%80%91JavaScript%E8%8A%82%E6%B5%81%E4%B8%8E%E9%98%B2%E6%8A%96%E8%AF%A6%E8%A7%A3%EF%BC%9A%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/" title="【前端】JavaScript节流与防抖详解：性能优化的核心技术"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">【前端】JavaScript节流与防抖详解：性能优化的核心技术</div></div><div class="info-2"><div class="info-item-1">前言在前端开发中，我们经常遇到需要处理高频事件的场景，比如用户快速点击按钮、滚动页面、调整窗口大小、输入搜索关键词等。如果不加以控制，这些高频事件会导致函数被频繁调用，造成性能问题，甚至可能导致页面卡顿或浏览器崩溃。 节流（Throttle）和防抖（Debounce）是两种重要的性能优化技术，它们通过控制函数的执行频率来解决高频事件带来的性能问题。虽然这两种技术都能限制函数的执行次数，但它们的实现原理和适用场景有所不同。 本文将详细介绍节流和防抖的概念、实现原理、使用场景，并提供完整的代码示例和最佳实践。 一、防抖（Debounce）详解（一）什么是防抖防抖是指在事件被触发n秒后再执行回调函数，如果在这n秒内又被触发，则重新计时。简单来说，防抖就是”等你不触发了，我再执行”。 生活中的比喻： 就像电梯等人一样，如果有人进电梯，电梯会等待几秒钟，如果在等待期间又有人进来，就重新开始等待，直到没有人进来了才关门启动。 （二）防抖的实现原理防抖的核心思想是使用定时器延迟执行函数，如果在延迟期间再次触发事件，就清除之前的定时器并重新设置。 基础版防抖实现12345678910111213...</div></div></div></a><a class="pagination-related" href="/2025/07/28/%E5%89%8D%E7%AB%AF/%E3%80%90%E5%89%8D%E7%AB%AF%E3%80%91%E8%B7%A8%E5%9F%9F%E9%97%AE%E9%A2%98%E8%AF%A6%E8%A7%A3%EF%BC%9A%E4%BB%8E%E5%90%8C%E6%BA%90%E7%AD%96%E7%95%A5%E5%88%B0%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E7%9A%84%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/" title="【前端】跨域问题详解：从同源策略到解决方案的完整指南"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">【前端】跨域问题详解：从同源策略到解决方案的完整指南</div></div><div class="info-2"><div class="info-item-1">前言跨域问题是前端开发中最常遇到的问题之一，它源于浏览器的同源策略安全机制。随着现代Web应用架构的复杂化，前后端分离、微服务架构的普及，跨域问题变得更加突出。本文将从同源策略的基本概念出发，深入分析跨域问题的本质，并详细介绍各种跨域解决方案的原理、实现方式和适用场景。 一、同源策略基础（一）什么是同源策略同源策略（Same-Origin Policy）是浏览器的一个重要安全机制，它限制了从一个源加载的文档或脚本如何与来自另一个源的资源进行交互。这是一个用于隔离潜在恶意文档的重要安全机制。 1. 源的定义一个源由三个部分组成：  协议（Protocol）：如 http:// 或 https:// 域名（Domain）：如 example.com 或 api.example.com 端口（Port）：如 :80、:443、:3000  1234567891011// 同源示例const currentOrigin = 'https://www.example.com:443';// 以下URL与当前源的对比：const urls = [    'htt...</div></div></div></a></nav><hr class="custom-hr"><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="/img/avatar.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"></div><div class="author-info-name">Wake</div><div class="author-info-description">一起学习，一起进步</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">252</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">688</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">28</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/uwakeme"><i class="fab fa-github"></i><span>关注我</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/uwakeme" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:wakemeup2025@126.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到我的博客！这里分享技术知识、学习心得和生活感悟。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E3%80%81Hadoop%E6%A6%82%E8%BF%B0%E4%B8%8E%E6%9E%B6%E6%9E%84"><span class="toc-text">一、Hadoop概述与架构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%B8%80%EF%BC%89Hadoop%E7%AE%80%E4%BB%8B"><span class="toc-text">（一）Hadoop简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%BA%8C%EF%BC%89Hadoop%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6"><span class="toc-text">（二）Hadoop核心组件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%B8%89%EF%BC%89Hadoop%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F"><span class="toc-text">（三）Hadoop生态系统</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E3%80%81HDFS%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F"><span class="toc-text">二、HDFS分布式文件系统</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%B8%80%EF%BC%89HDFS%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1"><span class="toc-text">（一）HDFS架构设计</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%BA%8C%EF%BC%89NameNode%E8%AF%A6%E8%A7%A3"><span class="toc-text">（二）NameNode详解</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%B8%89%EF%BC%89DataNode%E8%AF%A6%E8%A7%A3"><span class="toc-text">（三）DataNode详解</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89%E3%80%81MapReduce%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6"><span class="toc-text">三、MapReduce计算框架</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%B8%80%EF%BC%89MapReduce%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B"><span class="toc-text">（一）MapReduce编程模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%BA%8C%EF%BC%89MapReduce%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B"><span class="toc-text">（二）MapReduce执行流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%B8%89%EF%BC%89MapReduce%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF"><span class="toc-text">（三）MapReduce优化技术</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B%E3%80%81YARN%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86"><span class="toc-text">四、YARN资源管理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%B8%80%EF%BC%89YARN%E6%9E%B6%E6%9E%84%E7%BB%84%E4%BB%B6"><span class="toc-text">（一）YARN架构组件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%BA%8C%EF%BC%89YARN%E8%B0%83%E5%BA%A6%E5%99%A8"><span class="toc-text">（二）YARN调度器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%B8%89%EF%BC%89YARN%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91"><span class="toc-text">（三）YARN应用开发</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94%E3%80%81Hadoop%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F%E7%BB%84%E4%BB%B6"><span class="toc-text">五、Hadoop生态系统组件</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%B8%80%EF%BC%89Hive%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93"><span class="toc-text">（一）Hive数据仓库</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%BA%8C%EF%BC%89HBase-NoSQL%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-text">（二）HBase NoSQL数据库</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%B8%89%EF%BC%89Spark%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E"><span class="toc-text">（三）Spark计算引擎</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%AD%E3%80%81Hadoop%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E4%B8%8E%E7%AE%A1%E7%90%86"><span class="toc-text">六、Hadoop集群部署与管理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%B8%80%EF%BC%89%E9%9B%86%E7%BE%A4%E8%A7%84%E5%88%92%E4%B8%8E%E9%83%A8%E7%BD%B2"><span class="toc-text">（一）集群规划与部署</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%BA%8C%EF%BC%89%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7%E4%B8%8E%E7%AE%A1%E7%90%86"><span class="toc-text">（二）集群监控与管理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%88%E4%B8%89%EF%BC%89%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E4%B8%8E%E6%95%85%E9%9A%9C%E6%8E%92%E9%99%A4"><span class="toc-text">（三）性能调优与故障排除</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2026/02/13/%E7%AE%97%E6%B3%95/%E3%80%90%E7%AE%97%E6%B3%95%E3%80%91%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8C%B9%E9%85%8D%E7%AE%97%E6%B3%95%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90%EF%BC%9AKMP%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0/" title="【算法】字符串匹配算法深入解析：KMP算法原理与实现"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【算法】字符串匹配算法深入解析：KMP算法原理与实现"></a><div class="content"><a class="title" href="/2026/02/13/%E7%AE%97%E6%B3%95/%E3%80%90%E7%AE%97%E6%B3%95%E3%80%91%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8C%B9%E9%85%8D%E7%AE%97%E6%B3%95%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90%EF%BC%9AKMP%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0/" title="【算法】字符串匹配算法深入解析：KMP算法原理与实现">【算法】字符串匹配算法深入解析：KMP算法原理与实现</a><time datetime="2026-02-13T09:06:00.000Z" title="发表于 2026-02-13 17:06:00">2026-02-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2026/01/22/BUG/%E3%80%90BUG%E3%80%91Claude%20Code%E8%B7%B3%E8%BF%87%E5%BC%BA%E5%88%B6%E7%99%BB%E5%BD%95%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/" title="【BUG】Claude Code跳过强制登录解决方法"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="https://raw.githubusercontent.com/uwakeme/personal-image-repository/master/images/20260122114551679.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【BUG】Claude Code跳过强制登录解决方法"></a><div class="content"><a class="title" href="/2026/01/22/BUG/%E3%80%90BUG%E3%80%91Claude%20Code%E8%B7%B3%E8%BF%87%E5%BC%BA%E5%88%B6%E7%99%BB%E5%BD%95%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/" title="【BUG】Claude Code跳过强制登录解决方法">【BUG】Claude Code跳过强制登录解决方法</a><time datetime="2026-01-22T03:45:00.000Z" title="发表于 2026-01-22 11:45:00">2026-01-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2026/01/15/%E7%AE%97%E6%B3%95/%E3%80%90%E7%AE%97%E6%B3%95%E3%80%91%E3%80%8A%E7%AE%97%E6%B3%95%E5%9B%BE%E8%A7%A3%E3%80%8B%E7%AC%AC%E5%9B%9B%E7%AB%A0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E5%88%86%E8%80%8C%E6%B2%BB%E4%B9%8B%E4%B8%8E%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/" title="【算法】《算法图解》第四章学习笔记：分而治之与快速排序"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【算法】《算法图解》第四章学习笔记：分而治之与快速排序"></a><div class="content"><a class="title" href="/2026/01/15/%E7%AE%97%E6%B3%95/%E3%80%90%E7%AE%97%E6%B3%95%E3%80%91%E3%80%8A%E7%AE%97%E6%B3%95%E5%9B%BE%E8%A7%A3%E3%80%8B%E7%AC%AC%E5%9B%9B%E7%AB%A0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E5%88%86%E8%80%8C%E6%B2%BB%E4%B9%8B%E4%B8%8E%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/" title="【算法】《算法图解》第四章学习笔记：分而治之与快速排序">【算法】《算法图解》第四章学习笔记：分而治之与快速排序</a><time datetime="2026-01-15T08:48:25.264Z" title="发表于 2026-01-15 16:48:25">2026-01-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2026/01/15/%E7%AE%97%E6%B3%95/%E3%80%90%E7%AE%97%E6%B3%95%E3%80%91%E3%80%8A%E7%AE%97%E6%B3%95%E5%9B%BE%E8%A7%A3%E3%80%8B%E7%AC%AC%E5%8D%81%E7%AB%A0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E8%B4%AA%E5%A9%AA%E7%AE%97%E6%B3%95/" title="【算法】《算法图解》第十章学习笔记：贪婪算法"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【算法】《算法图解》第十章学习笔记：贪婪算法"></a><div class="content"><a class="title" href="/2026/01/15/%E7%AE%97%E6%B3%95/%E3%80%90%E7%AE%97%E6%B3%95%E3%80%91%E3%80%8A%E7%AE%97%E6%B3%95%E5%9B%BE%E8%A7%A3%E3%80%8B%E7%AC%AC%E5%8D%81%E7%AB%A0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E8%B4%AA%E5%A9%AA%E7%AE%97%E6%B3%95/" title="【算法】《算法图解》第十章学习笔记：贪婪算法">【算法】《算法图解》第十章学习笔记：贪婪算法</a><time datetime="2026-01-15T08:48:25.264Z" title="发表于 2026-01-15 16:48:25">2026-01-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2026/01/15/%E7%AE%97%E6%B3%95/%E3%80%90%E7%AE%97%E6%B3%95%E3%80%91%E3%80%8A%E7%AE%97%E6%B3%95%E5%9B%BE%E8%A7%A3%E3%80%8B%E7%AC%AC%E5%8D%81%E4%B8%89%E7%AB%A0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E6%8E%A5%E4%B8%8B%E6%9D%A5%E5%A6%82%E4%BD%95%E5%81%9A/" title="【算法】《算法图解》第十三章学习笔记：接下来如何做"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0OCIgaGVpZ2h0PSI0OCIgdmlld0JveD0iMCAwIDI0IDI0Ij48Y2lyY2xlIGN4PSI0IiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgaWQ9InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAiIGF0dHJpYnV0ZU5hbWU9InIiIGJlZ2luPSIwO3N2Z1NwaW5uZXJzM0RvdHNTY2FsZTEuZW5kLTAuMjVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjMiIGZpbGw9ImN1cnJlbnRDb2xvciI+PGFuaW1hdGUgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNnMiIGR1cj0iMC43NXMiIHZhbHVlcz0iMzsuMjszIi8+PC9jaXJjbGU+PGNpcmNsZSBjeD0iMjAiIGN5PSIxMiIgcj0iMyIgZmlsbD0iY3VycmVudENvbG9yIj48YW5pbWF0ZSBpZD0ic3ZnU3Bpbm5lcnMzRG90c1NjYWxlMSIgYXR0cmlidXRlTmFtZT0iciIgYmVnaW49InN2Z1NwaW5uZXJzM0RvdHNTY2FsZTAuZW5kLTAuNDVzIiBkdXI9IjAuNzVzIiB2YWx1ZXM9IjM7LjI7MyIvPjwvY2lyY2xlPjwvc3ZnPg==" data-original="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【算法】《算法图解》第十三章学习笔记：接下来如何做"></a><div class="content"><a class="title" href="/2026/01/15/%E7%AE%97%E6%B3%95/%E3%80%90%E7%AE%97%E6%B3%95%E3%80%91%E3%80%8A%E7%AE%97%E6%B3%95%E5%9B%BE%E8%A7%A3%E3%80%8B%E7%AC%AC%E5%8D%81%E4%B8%89%E7%AB%A0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E6%8E%A5%E4%B8%8B%E6%9D%A5%E5%A6%82%E4%BD%95%E5%81%9A/" title="【算法】《算法图解》第十三章学习笔记：接下来如何做">【算法】《算法图解》第十三章学习笔记：接下来如何做</a><time datetime="2026-01-15T08:48:25.263Z" title="发表于 2026-01-15 16:48:25">2026-01-15</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">©2024 - 2026 By Wake</span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="https://cdn.jsdelivr.net/npm/hexo-theme-butterfly@5.4.2/source/js/utils.min.js"></script><script src="https://cdn.jsdelivr.net/npm/hexo-theme-butterfly@5.4.2/source/js/main.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@19.1.3/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = `%%{init:{ 'theme':'${theme}'}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (true) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid@11.8.0/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const commentCount = n => {
    const isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
    if (isCommentCount) {
      isCommentCount.textContent= n
    }
  }

  const initGitalk = (el, path) => {
    if (isShuoshuo) {
      window.shuoshuoComment.destroyGitalk = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }

    const gitalk = new Gitalk({
      clientID: 'Ov23liBiL0mPM9ywqcV3',
      clientSecret: '99811bbc42de51338a2ef755501651c968f3240c',
      repo: 'gitalk-pool',
      owner: 'uwakeme',
      admin: ['uwakeme'],
      updateCountCallback: commentCount,
      ...option,
      id: isShuoshuo ? path : (option && option.id) || '0697f4c258d15f1c20dc132bb5a9840c'
    })

    gitalk.render('gitalk-container')
  }

  const loadGitalk = async(el, path) => {
    if (typeof Gitalk === 'function') initGitalk(el, path)
    else {
      await btf.getCSS('https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css')
      await btf.getScript('https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js')
      initGitalk(el, path)
    }
  }

  if (isShuoshuo) {
    'Gitalk' === 'Gitalk'
      ? window.shuoshuoComment = { loadComment: loadGitalk }
      : window.loadOtherComment = loadGitalk
    return
  }

  if ('Gitalk' === 'Gitalk' || !false) {
    if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
    else loadGitalk()
  } else {
    window.loadOtherComment = loadGitalk
  }
})()</script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/dist/fireworks.min.js"></script><script defer="defer" id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/dist/canvas-fluttering-ribbon.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/dist/canvas-nest.min.js"></script><script async="" data-pjax="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="请输入搜索内容" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/hexo-theme-butterfly@5.4.2/source/js/search/local-search.min.js"></script></div></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script src="/bundle.js"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});;

            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        ;
window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});;
(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body></html>