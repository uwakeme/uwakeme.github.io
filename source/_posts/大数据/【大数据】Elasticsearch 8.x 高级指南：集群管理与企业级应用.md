---
title: ã€å¤§æ•°æ®ã€‘Elasticsearch 8.x é«˜çº§æŒ‡å—ï¼šé›†ç¾¤ç®¡ç†ä¸ä¼ä¸šçº§åº”ç”¨
categories: å¤§æ•°æ®
date: 2025-01-19 18:15:00
tags:
  - Elasticsearch
  - é›†ç¾¤ç®¡ç†
  - ä¼ä¸šçº§åº”ç”¨
  - æ€§èƒ½è°ƒä¼˜
  - å®‰å…¨é…ç½®
  - ç›‘æ§è¿ç»´
series: Elasticsearchç³»åˆ—
cover: 
---

# å‰è¨€

æœ¬æ–‡æ˜¯Elasticsearchç³»åˆ—çš„é«˜çº§æ•™ç¨‹ï¼Œé¢å‘æœ‰ä¸°å¯ŒElasticsearchä½¿ç”¨ç»éªŒçš„å¼€å‘è€…å’Œè¿ç»´å·¥ç¨‹å¸ˆã€‚å°†æ·±å…¥æ¢è®¨é›†ç¾¤æ¶æ„è®¾è®¡ã€æ€§èƒ½æ·±åº¦è°ƒä¼˜ã€ä¼ä¸šçº§å®‰å…¨é…ç½®ã€ç›‘æ§è¿ç»´ç­‰é«˜çº§ä¸»é¢˜ï¼Œå¸®åŠ©æ‚¨æ„å»ºç”Ÿäº§çº§çš„Elasticsearchè§£å†³æ–¹æ¡ˆã€‚

> **ç³»åˆ—æ–‡ç« å¯¼èˆª**
> - ğŸŸ¢ [å…¥é—¨çº§ï¼šåŸºç¡€æ¦‚å¿µä¸ç¯å¢ƒæ­å»º](./ã€å¤§æ•°æ®ã€‘Elasticsearch%208.x%20å…¥é—¨æŒ‡å—ï¼šåŸºç¡€æ¦‚å¿µä¸ç¯å¢ƒæ­å»º.md)
> - ğŸŸ¡ [è¿›é˜¶çº§ï¼šå¤æ‚æŸ¥è¯¢ä¸èšåˆåˆ†æ](./ã€å¤§æ•°æ®ã€‘Elasticsearch%208.x%20è¿›é˜¶æŒ‡å—ï¼šå¤æ‚æŸ¥è¯¢ä¸èšåˆåˆ†æ.md)
> - ğŸ”´ **é«˜çº§**ï¼šé›†ç¾¤ç®¡ç†ä¸ä¼ä¸šçº§åº”ç”¨ï¼ˆæœ¬æ–‡ï¼‰

# ä¸€ã€é›†ç¾¤æ¶æ„è®¾è®¡

## ï¼ˆä¸€ï¼‰é›†ç¾¤è§„åˆ’ä¸è®¾è®¡

### 1. èŠ‚ç‚¹è§’è‰²è§„åˆ’

```yaml
# master-eligibleèŠ‚ç‚¹é…ç½®
# elasticsearch.yml
cluster.name: production-cluster
node.name: master-node-1
node.roles: [master]
discovery.seed_hosts: ["master-node-1", "master-node-2", "master-node-3"]
cluster.initial_master_nodes: ["master-node-1", "master-node-2", "master-node-3"]

# ç½‘ç»œé…ç½®
network.host: 0.0.0.0
http.port: 9200
transport.port: 9300

# å†…å­˜é…ç½®
bootstrap.memory_lock: true

# å®‰å…¨é…ç½®
xpack.security.enabled: true
xpack.security.transport.ssl.enabled: true
xpack.security.http.ssl.enabled: true
```

```yaml
# æ•°æ®èŠ‚ç‚¹é…ç½®
# elasticsearch.yml
cluster.name: production-cluster
node.name: data-node-1
node.roles: [data, data_content, data_hot, data_warm, data_cold]
discovery.seed_hosts: ["master-node-1", "master-node-2", "master-node-3"]

# æ•°æ®è·¯å¾„é…ç½®
path.data: ["/data1/elasticsearch", "/data2/elasticsearch"]
path.logs: "/var/log/elasticsearch"

# å†…å­˜å’Œæ€§èƒ½é…ç½®
bootstrap.memory_lock: true
indices.memory.index_buffer_size: 30%
indices.breaker.total.limit: 70%
```

```yaml
# åè°ƒèŠ‚ç‚¹é…ç½®
# elasticsearch.yml
cluster.name: production-cluster
node.name: coordinating-node-1
node.roles: []  # ä»…ä½œä¸ºåè°ƒèŠ‚ç‚¹
discovery.seed_hosts: ["master-node-1", "master-node-2", "master-node-3"]

# ç½‘ç»œé…ç½®
network.host: 0.0.0.0
http.port: 9200

# æœç´¢çº¿ç¨‹æ± é…ç½®
thread_pool.search.size: 16
thread_pool.search.queue_size: 1000
```

### 2. ç¡¬ä»¶èµ„æºè§„åˆ’

```bash
# ç”Ÿäº§ç¯å¢ƒç¡¬ä»¶æ¨èé…ç½®

# MasterèŠ‚ç‚¹
# CPU: 4-8æ ¸
# å†…å­˜: 8-16GB
# å­˜å‚¨: SSD 100GB+
# ç½‘ç»œ: 1Gbps+

# æ•°æ®èŠ‚ç‚¹
# CPU: 16-32æ ¸
# å†…å­˜: 64-128GB (å †å†…å­˜ä¸è¶…è¿‡32GB)
# å­˜å‚¨: SSD 1TB+ (å¤šç£ç›˜RAIDé…ç½®)
# ç½‘ç»œ: 10Gbps+

# åè°ƒèŠ‚ç‚¹
# CPU: 8-16æ ¸
# å†…å­˜: 32-64GB
# å­˜å‚¨: SSD 200GB+
# ç½‘ç»œ: 10Gbps+
```

## ï¼ˆäºŒï¼‰é›†ç¾¤éƒ¨ç½²ä¸ç®¡ç†

### 1. Docker Composeé›†ç¾¤éƒ¨ç½²

```yaml
# docker-compose.yml
version: '3.8'

services:
  es-master-1:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.0
    container_name: es-master-1
    environment:
      - node.name=es-master-1
      - cluster.name=production-cluster
      - node.roles=master
      - discovery.seed_hosts=es-master-1,es-master-2,es-master-3
      - cluster.initial_master_nodes=es-master-1,es-master-2,es-master-3
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms4g -Xmx4g"
      - xpack.security.enabled=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - master1_data:/usr/share/elasticsearch/data
      - ./config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml
    ports:
      - "9201:9200"
    networks:
      - elastic

  es-master-2:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.0
    container_name: es-master-2
    environment:
      - node.name=es-master-2
      - cluster.name=production-cluster
      - node.roles=master
      - discovery.seed_hosts=es-master-1,es-master-2,es-master-3
      - cluster.initial_master_nodes=es-master-1,es-master-2,es-master-3
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms4g -Xmx4g"
      - xpack.security.enabled=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - master2_data:/usr/share/elasticsearch/data
    ports:
      - "9202:9200"
    networks:
      - elastic

  es-master-3:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.0
    container_name: es-master-3
    environment:
      - node.name=es-master-3
      - cluster.name=production-cluster
      - node.roles=master
      - discovery.seed_hosts=es-master-1,es-master-2,es-master-3
      - cluster.initial_master_nodes=es-master-1,es-master-2,es-master-3
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms4g -Xmx4g"
      - xpack.security.enabled=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - master3_data:/usr/share/elasticsearch/data
    ports:
      - "9203:9200"
    networks:
      - elastic

  es-data-1:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.0
    container_name: es-data-1
    environment:
      - node.name=es-data-1
      - cluster.name=production-cluster
      - node.roles=data,data_content,data_hot,data_warm
      - discovery.seed_hosts=es-master-1,es-master-2,es-master-3
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms16g -Xmx16g"
      - xpack.security.enabled=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - data1_data:/usr/share/elasticsearch/data
    networks:
      - elastic

  es-data-2:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.0
    container_name: es-data-2
    environment:
      - node.name=es-data-2
      - cluster.name=production-cluster
      - node.roles=data,data_content,data_hot,data_warm
      - discovery.seed_hosts=es-master-1,es-master-2,es-master-3
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms16g -Xmx16g"
      - xpack.security.enabled=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - data2_data:/usr/share/elasticsearch/data
    networks:
      - elastic

  kibana:
    image: docker.elastic.co/kibana/kibana:8.12.0
    container_name: kibana
    environment:
      - ELASTICSEARCH_HOSTS=http://es-master-1:9200,http://es-master-2:9200,http://es-master-3:9200
      - SERVER_NAME=kibana
      - SERVER_HOST=0.0.0.0
    ports:
      - "5601:5601"
    networks:
      - elastic
    depends_on:
      - es-master-1
      - es-master-2
      - es-master-3

volumes:
  master1_data:
  master2_data:
  master3_data:
  data1_data:
  data2_data:

networks:
  elastic:
    driver: bridge
```

### 2. é›†ç¾¤ç®¡ç†API

```java
@Service
public class ClusterManagementService {
    
    @Autowired
    private ElasticsearchClient client;
    
    /**
     * è·å–é›†ç¾¤å¥åº·çŠ¶æ€
     */
    public ClusterHealthResponse getClusterHealth() throws IOException {
        ClusterHealthRequest request = ClusterHealthRequest.of(h -> h
            .waitForStatus(HealthStatus.Yellow)
            .timeout(t -> t.time("30s"))
        );
        
        return client.cluster().health(request);
    }
    
    /**
     * è·å–é›†ç¾¤ç»Ÿè®¡ä¿¡æ¯
     */
    public ClusterStatsResponse getClusterStats() throws IOException {
        ClusterStatsRequest request = ClusterStatsRequest.of(s -> s);
        return client.cluster().stats(request);
    }
    
    /**
     * è·å–èŠ‚ç‚¹ä¿¡æ¯
     */
    public NodesInfoResponse getNodesInfo() throws IOException {
        NodesInfoRequest request = NodesInfoRequest.of(n -> n
            .metric(NodeInfoMetric.Os, NodeInfoMetric.Process, NodeInfoMetric.Jvm)
        );
        
        return client.nodes().info(request);
    }
    
    /**
     * è·å–èŠ‚ç‚¹ç»Ÿè®¡ä¿¡æ¯
     */
    public NodesStatsResponse getNodesStats() throws IOException {
        NodesStatsRequest request = NodesStatsRequest.of(n -> n
            .metric(NodeStatsMetric.Os, NodeStatsMetric.Process, NodeStatsMetric.Jvm, 
                   NodeStatsMetric.Indices, NodeStatsMetric.ThreadPool)
        );
        
        return client.nodes().stats(request);
    }
    
    /**
     * é›†ç¾¤è®¾ç½®ç®¡ç†
     */
    public void updateClusterSettings(Map<String, String> persistentSettings, 
                                    Map<String, String> transientSettings) throws IOException {
        ClusterPutSettingsRequest request = ClusterPutSettingsRequest.of(s -> s
            .persistent(persistentSettings)
            .transient_(transientSettings)
        );
        
        ClusterPutSettingsResponse response = client.cluster().putSettings(request);
        System.out.println("é›†ç¾¤è®¾ç½®æ›´æ–°ç»“æœ: " + response.acknowledged());
    }
}
```

## ï¼ˆä¸‰ï¼‰åˆ†ç‰‡ç®¡ç†ä¸è·¯ç”±

### 1. åˆ†ç‰‡åˆ†é…ç­–ç•¥

```java
/**
 * åˆ†ç‰‡åˆ†é…ç®¡ç†æœåŠ¡
 */
@Service
public class ShardAllocationService {
    
    @Autowired
    private ElasticsearchClient client;
    
    /**
     * è®¾ç½®åˆ†ç‰‡åˆ†é…ç­–ç•¥
     */
    public void configureShardAllocation() throws IOException {
        Map<String, String> settings = new HashMap<>();
        
        // åˆ†ç‰‡åˆ†é…ç­–ç•¥
        settings.put("cluster.routing.allocation.enable", "all");
        settings.put("cluster.routing.allocation.node_concurrent_recoveries", "4");
        settings.put("cluster.routing.allocation.cluster_concurrent_rebalance", "2");
        
        // ç£ç›˜æ°´ä½çº¿è®¾ç½®
        settings.put("cluster.routing.allocation.disk.threshold_enabled", "true");
        settings.put("cluster.routing.allocation.disk.watermark.low", "85%");
        settings.put("cluster.routing.allocation.disk.watermark.high", "90%");
        settings.put("cluster.routing.allocation.disk.watermark.flood_stage", "95%");
        
        // åˆ†ç‰‡å¤§å°é™åˆ¶
        settings.put("cluster.routing.allocation.total_shards_per_node", "1000");
        
        ClusterPutSettingsRequest request = ClusterPutSettingsRequest.of(s -> s
            .persistent(settings)
        );
        
        client.cluster().putSettings(request);
    }
    
    /**
     * æ‰‹åŠ¨åˆ†ç‰‡é‡åˆ†é…
     */
    public void reallocateShards(String indexName, int shardNumber, String fromNode, String toNode) throws IOException {
        ClusterRerouteRequest request = ClusterRerouteRequest.of(r -> r
            .commands(c -> c
                .move(m -> m
                    .index(indexName)
                    .shard(shardNumber)
                    .fromNode(fromNode)
                    .toNode(toNode)
                )
            )
        );
        
        ClusterRerouteResponse response = client.cluster().reroute(request);
        System.out.println("åˆ†ç‰‡é‡åˆ†é…ç»“æœ: " + response.acknowledged());
    }
    
    /**
     * è·å–åˆ†ç‰‡åˆ†é…è§£é‡Š
     */
    public void explainShardAllocation(String indexName, int shardNumber, boolean primary) throws IOException {
        ClusterAllocationExplainRequest request = ClusterAllocationExplainRequest.of(e -> e
            .index(indexName)
            .shard(shardNumber)
            .primary(primary)
        );
        
        ClusterAllocationExplainResponse response = client.cluster().allocationExplain(request);
        System.out.println("åˆ†ç‰‡åˆ†é…è§£é‡Š: " + response);
    }
}
```

### 2. è‡ªå®šä¹‰è·¯ç”±ç­–ç•¥

```java
/**
 * è‡ªå®šä¹‰è·¯ç”±æœåŠ¡
 */
@Service
public class CustomRoutingService {
    
    @Autowired
    private ElasticsearchClient client;
    
    /**
     * åŸºäºç”¨æˆ·IDçš„è·¯ç”±ç´¢å¼•
     */
    public void indexWithCustomRouting(String indexName, Document document, String userId) throws IOException {
        IndexRequest<Document> request = IndexRequest.of(i -> i
            .index(indexName)
            .id(document.getId())
            .routing(userId)  // ä½¿ç”¨ç”¨æˆ·IDä½œä¸ºè·¯ç”±
            .document(document)
        );
        
        IndexResponse response = client.index(request);
        System.out.println("æ–‡æ¡£ç´¢å¼•ç»“æœ: " + response.result());
    }
    
    /**
     * åŸºäºè·¯ç”±çš„æœç´¢
     */
    public List<Document> searchWithRouting(String indexName, String query, String routing) throws IOException {
        SearchRequest request = SearchRequest.of(s -> s
            .index(indexName)
            .routing(routing)  // æŒ‡å®šè·¯ç”±ï¼Œåªæœç´¢ç‰¹å®šåˆ†ç‰‡
            .query(q -> q
                .multiMatch(m -> m
                    .query(query)
                    .fields("title", "content")
                )
            )
        );
        
        SearchResponse<Document> response = client.search(request, Document.class);
        return response.hits().hits().stream()
                .map(Hit::source)
                .collect(Collectors.toList());
    }
    
    /**
     * åˆ›å»ºå¸¦è·¯ç”±çš„ç´¢å¼•æ¨¡æ¿
     */
    public void createRoutingTemplate() throws IOException {
        PutIndexTemplateRequest request = PutIndexTemplateRequest.of(t -> t
            .name("user_data_template")
            .indexPatterns("user_data_*")
            .template(template -> template
                .settings(s -> s
                    .numberOfShards("6")
                    .numberOfReplicas("1")
                    .routingPartitionSize(2)  // è·¯ç”±åˆ†åŒºå¤§å°
                )
                .mappings(m -> m
                    .properties("userId", p -> p.keyword(k -> k))
                    .properties("title", p -> p.text(t2 -> t2))
                    .properties("content", p -> p.text(t2 -> t2))
                    .properties("createTime", p -> p.date(d -> d))
                    .routing(r -> r.required(true))  // è¦æ±‚å¿…é¡»æä¾›è·¯ç”±
                )
            )
        );
        
        PutIndexTemplateResponse response = client.indices().putIndexTemplate(request);
        System.out.println("ç´¢å¼•æ¨¡æ¿åˆ›å»ºç»“æœ: " + response.acknowledged());
    }
}
```

# äºŒã€æ·±åº¦æ€§èƒ½è°ƒä¼˜

## ï¼ˆä¸€ï¼‰JVMè°ƒä¼˜

### 1. JVMå‚æ•°é…ç½®

```bash
# jvm.options
# å †å†…å­˜è®¾ç½®ï¼ˆä¸è¶…è¿‡32GBï¼Œæ¨èç‰©ç†å†…å­˜çš„50%ï¼‰
-Xms16g
-Xmx16g

# åƒåœ¾å›æ”¶å™¨è®¾ç½®ï¼ˆæ¨èG1GCï¼‰
-XX:+UseG1GC
-XX:G1HeapRegionSize=16m
-XX:+G1UseAdaptiveIHOP
-XX:G1MixedGCCountTarget=8
-XX:G1MixedGCLiveThresholdPercent=85

# GCæ—¥å¿—é…ç½®
-Xlog:gc*,gc+heap=info,safepoint:gc.log:time,level,tags
-XX:+UseGCLogFileRotation
-XX:NumberOfGCLogFiles=32
-XX:GCLogFileSize=64m

# å†…å­˜æ˜ å°„è®¾ç½®
-XX:+AlwaysPreTouch
-Djava.awt.headless=true

# ç½‘ç»œè®¾ç½®
-Djava.net.preferIPv4Stack=true

# ä¸´æ—¶ç›®å½•
-Djava.io.tmpdir=/tmp

# é”™è¯¯å¤„ç†
-XX:+HeapDumpOnOutOfMemoryError
-XX:HeapDumpPath=/var/lib/elasticsearch
-XX:ErrorFile=/var/log/elasticsearch/hs_err_pid%p.log
```

### 2. JVMç›‘æ§æœåŠ¡

```java
@Service
public class JVMMonitoringService {
    
    @Autowired
    private ElasticsearchClient client;
    
    /**
     * è·å–JVMç»Ÿè®¡ä¿¡æ¯
     */
    public JVMStats getJVMStats() throws IOException {
        NodesStatsRequest request = NodesStatsRequest.of(n -> n
            .metric(NodeStatsMetric.Jvm)
        );
        
        NodesStatsResponse response = client.nodes().stats(request);
        
        JVMStats jvmStats = new JVMStats();
        
        response.nodes().forEach((nodeId, nodeStats) -> {
            if (nodeStats.jvm() != null) {
                JvmStats jvm = nodeStats.jvm();
                
                // å †å†…å­˜ä¿¡æ¯
                if (jvm.mem() != null && jvm.mem().heapUsed() != null) {
                    jvmStats.addHeapUsed(nodeId, jvm.mem().heapUsed().bytes());
                    jvmStats.addHeapMax(nodeId, jvm.mem().heapMax().bytes());
                }
                
                // GCä¿¡æ¯
                if (jvm.gc() != null && jvm.gc().collectors() != null) {
                    jvm.gc().collectors().forEach((gcName, gcStats) -> {
                        jvmStats.addGCCount(nodeId, gcName, gcStats.collectionCount());
                        jvmStats.addGCTime(nodeId, gcName, gcStats.collectionTime().millis());
                    });
                }
                
                // çº¿ç¨‹æ± ä¿¡æ¯
                if (nodeStats.threadPool() != null) {
                    nodeStats.threadPool().forEach((poolName, poolStats) -> {
                        jvmStats.addThreadPoolStats(nodeId, poolName, 
                            poolStats.active(), poolStats.queue(), poolStats.rejected());
                    });
                }
            }
        });
        
        return jvmStats;
    }
    
    /**
     * JVMæ€§èƒ½åˆ†æ
     */
    public JVMPerformanceReport analyzeJVMPerformance() throws IOException {
        JVMStats stats = getJVMStats();
        JVMPerformanceReport report = new JVMPerformanceReport();
        
        // åˆ†æå †å†…å­˜ä½¿ç”¨ç‡
        stats.getHeapUsage().forEach((nodeId, usage) -> {
            double usagePercent = (double) usage.getUsed() / usage.getMax() * 100;
            if (usagePercent > 85) {
                report.addWarning(nodeId, "å †å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: " + String.format("%.2f%%", usagePercent));
            }
        });
        
        // åˆ†æGCé¢‘ç‡
        stats.getGcStats().forEach((nodeId, gcStats) -> {
            gcStats.forEach((gcName, gcInfo) -> {
                if (gcInfo.getCount() > 100) {  // å‡è®¾é˜ˆå€¼
                    report.addWarning(nodeId, gcName + " GCæ¬¡æ•°è¿‡å¤š: " + gcInfo.getCount());
                }
                if (gcInfo.getTime() > 10000) {  // å‡è®¾é˜ˆå€¼10ç§’
                    report.addWarning(nodeId, gcName + " GCæ—¶é—´è¿‡é•¿: " + gcInfo.getTime() + "ms");
                }
            });
        });
        
        return report;
    }
}
```

## ï¼ˆäºŒï¼‰ç´¢å¼•æ€§èƒ½ä¼˜åŒ–

### 1. ç´¢å¼•æ¨¡æ¿ä¼˜åŒ–

```java
/**
 * é«˜æ€§èƒ½ç´¢å¼•æ¨¡æ¿æœåŠ¡
 */
@Service
public class OptimizedIndexTemplateService {
    
    @Autowired
    private ElasticsearchClient client;
    
    /**
     * åˆ›å»ºé«˜æ€§èƒ½ç´¢å¼•æ¨¡æ¿
     */
    public void createHighPerformanceTemplate() throws IOException {
        PutIndexTemplateRequest request = PutIndexTemplateRequest.of(t -> t
            .name("high_performance_template")
            .indexPatterns("logs-*", "metrics-*")
            .template(template -> template
                .settings(s -> s
                    // åˆ†ç‰‡è®¾ç½®
                    .numberOfShards("3")
                    .numberOfReplicas("1")
                    
                    // åˆ·æ–°è®¾ç½®
                    .refreshInterval(t2 -> t2.time("30s"))  // é™ä½åˆ·æ–°é¢‘ç‡
                    
                    // åˆå¹¶è®¾ç½®
                    .put("index.merge.policy.max_merge_at_once", JsonData.of(5))
                    .put("index.merge.policy.segments_per_tier", JsonData.of(5))
                    .put("index.merge.policy.max_merged_segment", JsonData.of("5gb"))
                    
                    // å­˜å‚¨è®¾ç½®
                    .put("index.store.type", JsonData.of("fs"))
                    .put("index.store.fs.fs_lock", JsonData.of("simple"))
                    
                    // ç¼“å­˜è®¾ç½®
                    .put("index.queries.cache.enabled", JsonData.of(true))
                    .put("index.requests.cache.enable", JsonData.of(true))
                    
                    // å‹ç¼©è®¾ç½®
                    .put("index.codec", JsonData.of("best_compression"))
                    
                    // äº‹åŠ¡æ—¥å¿—è®¾ç½®
                    .put("index.translog.flush_threshold_size", JsonData.of("1gb"))
                    .put("index.translog.sync_interval", JsonData.of("30s"))
                    .put("index.translog.durability", JsonData.of("async"))
                )
                .mappings(m -> m
                    // åŠ¨æ€æ˜ å°„è®¾ç½®
                    .dynamic(DynamicMapping.Strict)
                    
                    // å­—æ®µæ˜ å°„
                    .properties("timestamp", p -> p.date(d -> d
                        .format("yyyy-MM-dd HH:mm:ss")
                        .store(false)
                    ))
                    .properties("level", p -> p.keyword(k -> k
                        .store(false)
                        .docValues(true)
                    ))
                    .properties("message", p -> p.text(t2 -> t2
                        .analyzer("standard")
                        .store(false)
                        .norms(false)  // ç¦ç”¨è¯„åˆ†è§„èŒƒåŒ–
                    ))
                    .properties("host", p -> p.keyword(k -> k
                        .store(false)
                        .docValues(true)
                    ))
                    .properties("service", p -> p.keyword(k -> k
                        .store(false)
                        .docValues(true)
                    ))
                    
                    // å…ƒæ•°æ®å­—æ®µè®¾ç½®
                    .source(src -> src.enabled(true).compress(true))
                    .fieldNames(fn -> fn.enabled(false))  // ç¦ç”¨_field_names
                )
            )
            .priority(100L)
        );
        
        PutIndexTemplateResponse response = client.indices().putIndexTemplate(request);
        System.out.println("é«˜æ€§èƒ½ç´¢å¼•æ¨¡æ¿åˆ›å»ºç»“æœ: " + response.acknowledged());
    }
    
    /**
     * åˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®æ¨¡æ¿
     */
    public void createTimeSeriesTemplate() throws IOException {
        PutIndexTemplateRequest request = PutIndexTemplateRequest.of(t -> t
            .name("timeseries_template")
            .indexPatterns("timeseries-*")
            .template(template -> template
                .settings(s -> s
                    .numberOfShards("1")  // æ—¶é—´åºåˆ—æ•°æ®é€šå¸¸å•åˆ†ç‰‡
                    .numberOfReplicas("1")
                    
                    // æ—¶é—´åºåˆ—ä¼˜åŒ–è®¾ç½®
                    .put("index.mode", JsonData.of("time_series"))
                    .put("index.routing_path", JsonData.of(List.of("host", "service")))
                    .put("index.time_series.start_time", JsonData.of("2024-01-01T00:00:00Z"))
                    .put("index.time_series.end_time", JsonData.of("2024-12-31T23:59:59Z"))
                    
                    // æ€§èƒ½ä¼˜åŒ–
                    .refreshInterval(t2 -> t2.time("60s"))
                    .put("index.translog.durability", JsonData.of("async"))
                    .put("index.translog.sync_interval", JsonData.of("60s"))
                )
                .mappings(m -> m
                    .properties("@timestamp", p -> p.date(d -> d
                        .format("strict_date_optional_time")
                    ))
                    .properties("host", p -> p.keyword(k -> k
                        .timeSeriesDimension(true)  // æ—¶é—´åºåˆ—ç»´åº¦
                    ))
                    .properties("service", p -> p.keyword(k -> k
                        .timeSeriesDimension(true)
                    ))
                    .properties("cpu_usage", p -> p.double_(d -> d
                        .timeSeriesMetric(TimeSeriesMetricType.Gauge)  // æ—¶é—´åºåˆ—æŒ‡æ ‡
                    ))
                    .properties("memory_usage", p -> p.double_(d -> d
                        .timeSeriesMetric(TimeSeriesMetricType.Gauge)
                    ))
                )
            )
        );
        
        client.indices().putIndexTemplate(request);
    }
}
```

### 2. æ‰¹é‡æ“ä½œä¼˜åŒ–

```java
/**
 * é«˜æ€§èƒ½æ‰¹é‡æ“ä½œæœåŠ¡
 */
@Service
public class HighPerformanceBulkService {
    
    @Autowired
    private ElasticsearchClient client;
    
    private final ExecutorService executorService = Executors.newFixedThreadPool(10);
    
    /**
     * å¹¶è¡Œæ‰¹é‡ç´¢å¼•
     */
    public CompletableFuture<BulkResult> parallelBulkIndex(String indexName, 
                                                          List<Document> documents) {
        return CompletableFuture.supplyAsync(() -> {
            try {
                return performOptimizedBulk(indexName, documents);
            } catch (IOException e) {
                throw new RuntimeException("æ‰¹é‡ç´¢å¼•å¤±è´¥", e);
            }
        }, executorService);
    }
    
    private BulkResult performOptimizedBulk(String indexName, List<Document> documents) throws IOException {
        final int OPTIMAL_BATCH_SIZE = 1000;  // ä¼˜åŒ–çš„æ‰¹æ¬¡å¤§å°
        final int MAX_BULK_SIZE_MB = 10;      // æœ€å¤§æ‰¹æ¬¡å¤§å°ï¼ˆMBï¼‰
        
        BulkResult result = new BulkResult();
        
        // åˆ†æ‰¹å¤„ç†
        for (int i = 0; i < documents.size(); i += OPTIMAL_BATCH_SIZE) {
            int endIndex = Math.min(i + OPTIMAL_BATCH_SIZE, documents.size());
            List<Document> batch = documents.subList(i, endIndex);
            
            // æ£€æŸ¥æ‰¹æ¬¡å¤§å°
            long batchSizeBytes = estimateBatchSize(batch);
            if (batchSizeBytes > MAX_BULK_SIZE_MB * 1024 * 1024) {
                // å¦‚æœæ‰¹æ¬¡è¿‡å¤§ï¼Œè¿›ä¸€æ­¥åˆ†å‰²
                processBatchBySize(indexName, batch, MAX_BULK_SIZE_MB * 1024 * 1024, result);
            } else {
                processSingleBatch(indexName, batch, result);
            }
            
            // æ‰¹æ¬¡é—´ä¼‘æ¯ï¼Œé¿å…è¿‡è½½
            if (endIndex < documents.size()) {
                try {
                    Thread.sleep(50);
                } catch (InterruptedException e) {
                    Thread.currentThread().interrupt();
                    break;
                }
            }
        }
        
        return result;
    }
    
    private void processSingleBatch(String indexName, List<Document> batch, BulkResult result) throws IOException {
        BulkRequest.Builder bulkBuilder = new BulkRequest.Builder()
            .timeout(t -> t.time("60s"))
            .refresh(Refresh.False);  // ä¸ç«‹å³åˆ·æ–°
        
        for (Document doc : batch) {
            bulkBuilder.operations(op -> op
                .index(idx -> idx
                    .index(indexName)
                    .id(doc.getId())
                    .document(doc)
                )
            );
        }
        
        BulkResponse response = client.bulk(bulkBuilder.build());
        
        result.addProcessed(batch.size());
        
        if (response.errors()) {
            int errorCount = 0;
            for (BulkResponseItem item : response.items()) {
                if (item.error() != null) {
                    errorCount++;
                    result.addError(item.error().reason());
                }
            }
            result.addErrors(errorCount);
        }
        
        result.addSuccessful(batch.size() - (response.errors() ? 
            (int) response.items().stream().filter(item -> item.error() != null).count() : 0));
    }
    
    private long estimateBatchSize(List<Document> batch) {
        // ç®€å•ä¼°ç®—æ‰¹æ¬¡å¤§å°
        return batch.stream()
                .mapToLong(doc -> {
                    // ä¼°ç®—æ–‡æ¡£å¤§å°ï¼ˆå­—èŠ‚ï¼‰
                    return (doc.getTitle() != null ? doc.getTitle().length() : 0) +
                           (doc.getContent() != null ? doc.getContent().length() : 0) +
                           100; // å…¶ä»–å­—æ®µä¼°ç®—
                })
                .sum();
    }
    
    private void processBatchBySize(String indexName, List<Document> batch, 
                                  long maxSizeBytes, BulkResult result) throws IOException {
        List<Document> currentBatch = new ArrayList<>();
        long currentSize = 0;
        
        for (Document doc : batch) {
            long docSize = estimateDocumentSize(doc);
            
            if (currentSize + docSize > maxSizeBytes && !currentBatch.isEmpty()) {
                processSingleBatch(indexName, currentBatch, result);
                currentBatch.clear();
                currentSize = 0;
            }
            
            currentBatch.add(doc);
            currentSize += docSize;
        }
        
        if (!currentBatch.isEmpty()) {
            processSingleBatch(indexName, currentBatch, result);
        }
    }
    
    private long estimateDocumentSize(Document doc) {
        return (doc.getTitle() != null ? doc.getTitle().length() : 0) +
               (doc.getContent() != null ? doc.getContent().length() : 0) +
               100;
    }
}

/**
 * æ‰¹é‡æ“ä½œç»“æœ
 */
public class BulkResult {
    private int processed = 0;
    private int successful = 0;
    private int errors = 0;
    private List<String> errorMessages = new ArrayList<>();
    
    public void addProcessed(int count) { this.processed += count; }
    public void addSuccessful(int count) { this.successful += count; }
    public void addErrors(int count) { this.errors += count; }
    public void addError(String message) { this.errorMessages.add(message); }
    
    // Getteræ–¹æ³•
    public int getProcessed() { return processed; }
    public int getSuccessful() { return successful; }
    public int getErrors() { return errors; }
    public List<String> getErrorMessages() { return errorMessages; }
    public double getSuccessRate() { 
        return processed > 0 ? (double) successful / processed * 100 : 0; 
    }
}
```

## ï¼ˆä¸‰ï¼‰æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–

### 1. æŸ¥è¯¢ç¼“å­˜ä¼˜åŒ–

```java
/**
 * æŸ¥è¯¢ç¼“å­˜ä¼˜åŒ–æœåŠ¡
 */
@Service
public class QueryCacheOptimizationService {
    
    @Autowired
    private ElasticsearchClient client;
    
    /**
     * ä¼˜åŒ–çš„ç¼“å­˜æŸ¥è¯¢
     */
    public SearchResult optimizedCachedSearch(SearchParams params) throws IOException {
        SearchRequest request = SearchRequest.of(s -> s
            .index(params.getIndexName())
            .query(q -> q
                .bool(b -> b
                    // ä½¿ç”¨filterè¿›è¡Œå¯ç¼“å­˜çš„è¿‡æ»¤
                    .filter(filter -> filter
                        .bool(fb -> {
                            BoolQuery.Builder filterBuilder = new BoolQuery.Builder();
                            
                            // æ—¶é—´èŒƒå›´è¿‡æ»¤ï¼ˆé«˜ç¼“å­˜å‘½ä¸­ç‡ï¼‰
                            if (params.getStartTime() != null || params.getEndTime() != null) {
                                filterBuilder.must(must -> must
                                    .range(r -> {
                                        RangeQuery.Builder rangeBuilder = new RangeQuery.Builder()
                                            .field("createTime");
                                        if (params.getStartTime() != null) {
                                            rangeBuilder.gte(JsonData.of(params.getStartTime()));
                                        }
                                        if (params.getEndTime() != null) {
                                            rangeBuilder.lte(JsonData.of(params.getEndTime()));
                                        }
                                        return rangeBuilder.build();
                                    })
                                );
                            }
                            
                            // åˆ†ç±»è¿‡æ»¤ï¼ˆé«˜ç¼“å­˜å‘½ä¸­ç‡ï¼‰
                            if (StringUtils.hasText(params.getCategory())) {
                                filterBuilder.must(must -> must
                                    .term(t -> t
                                        .field("category.keyword")
                                        .value(params.getCategory())
                                    )
                                );
                            }
                            
                            // æ ‡ç­¾è¿‡æ»¤ï¼ˆä¸­ç­‰ç¼“å­˜å‘½ä¸­ç‡ï¼‰
                            if (params.getTags() != null && !params.getTags().isEmpty()) {
                                filterBuilder.must(must -> must
                                    .terms(t -> t
                                        .field("tags")
                                        .terms(TermsQueryField.of(tf -> tf.value(
                                            params.getTags().stream()
                                                .map(FieldValue::of)
                                                .collect(Collectors.toList())
                                        )))
                                    )
                                );
                            }
                            
                            return filterBuilder.build();
                        })
                    )
                    
                    // ä½¿ç”¨mustè¿›è¡Œè¯„åˆ†æŸ¥è¯¢ï¼ˆä¸ç¼“å­˜ï¼‰
                    .must(must -> must
                        .multiMatch(m -> m
                            .query(params.getKeyword())
                            .fields("title^3", "content")
                            .type(TextQueryType.BestFields)
                            .minimumShouldMatch("75%")
                        )
                    )
                )
            )
            .sort(so -> so
                .score(sc -> sc.order(SortOrder.Desc))
                .field(f -> f.field("createTime").order(SortOrder.Desc))
            )
            .from(params.getFrom())
            .size(params.getSize())
            .source(src -> src
                .filter(f -> f
                    .includes("id", "title", "summary", "createTime", "category", "tags")
                    .excludes("content")  // æ’é™¤å¤§å­—æ®µ
                )
            )
            .requestCache(true)  // å¯ç”¨è¯·æ±‚ç¼“å­˜
        );
        
        SearchResponse<Document> response = client.search(request, Document.class);
        
        List<SearchResultItem> items = response.hits().hits().stream()
                .map(hit -> {
                    SearchResultItem item = new SearchResultItem();
                    item.setDocument(hit.source());
                    item.setScore(hit.score());
                    return item;
                })
                .collect(Collectors.toList());
        
        return new SearchResult(
            items,
            response.hits().total().value(),
            params.getFrom() / params.getSize(),
            params.getSize()
        );
    }
    
    /**
     * é¢„çƒ­æŸ¥è¯¢ç¼“å­˜
     */
    public void warmupQueryCache(String indexName) throws IOException {
        // å¸¸ç”¨çš„è¿‡æ»¤æŸ¥è¯¢é¢„çƒ­
        List<String> commonCategories = Arrays.asList("tech", "business", "science");
        List<String> commonTimeRanges = Arrays.asList("now-1d", "now-7d", "now-30d");
        
        for (String category : commonCategories) {
            for (String timeRange : commonTimeRanges) {
                SearchRequest warmupRequest = SearchRequest.of(s -> s
                    .index(indexName)
                    .size(0)  // ä¸è¿”å›ç»“æœï¼Œåªé¢„çƒ­ç¼“å­˜
                    .query(q -> q
                        .bool(b -> b
                            .filter(filter -> filter
                                .term(t -> t
                                    .field("category.keyword")
                                    .value(category)
                                )
                            )
                            .filter(filter -> filter
                                .range(r -> r
                                    .field("createTime")
                                    .gte(JsonData.of(timeRange))
                                )
                            )
                        )
                    )
                    .requestCache(true)
                );
                
                client.search(warmupRequest, Void.class);
            }
        }
        
        System.out.println("æŸ¥è¯¢ç¼“å­˜é¢„çƒ­å®Œæˆ");
    }
}
```

# ä¸‰ã€ä¼ä¸šçº§å®‰å…¨é…ç½®

## ï¼ˆä¸€ï¼‰èº«ä»½è®¤è¯ä¸æˆæƒ

### 1. ç”¨æˆ·å’Œè§’è‰²ç®¡ç†

```java
/**
 * å®‰å…¨ç®¡ç†æœåŠ¡
 */
@Service
public class SecurityManagementService {
    
    @Autowired
    private ElasticsearchClient client;
    
    /**
     * åˆ›å»ºç”¨æˆ·
     */
    public void createUser(String username, String password, List<String> roles) throws IOException {
        PutUserRequest request = PutUserRequest.of(u -> u
            .username(username)
            .password(password)
            .roles(roles)
            .enabled(true)
            .fullName(username)
        );
        
        PutUserResponse response = client.security().putUser(request);
        System.out.println("ç”¨æˆ·åˆ›å»ºç»“æœ: " + response.created());
    }
    
    /**
     * åˆ›å»ºè§’è‰²
     */
    public void createRole(String roleName, List<String> clusterPrivileges, 
                          List<IndexPrivileges> indexPrivileges) throws IOException {
        PutRoleRequest request = PutRoleRequest.of(r -> r
            .name(roleName)
            .cluster(clusterPrivileges)
            .indices(indexPrivileges)
        );
        
        PutRoleResponse response = client.security().putRole(request);
        System.out.println("è§’è‰²åˆ›å»ºç»“æœ: " + response.role().created());
    }
    
    /**
     * åˆ›å»ºAPIå¯†é’¥
     */
    public ApiKeyResponse createApiKey(String name, List<String> roleDescriptors, 
                                     String expiration) throws IOException {
        CreateApiKeyRequest request = CreateApiKeyRequest.of(k -> k
            .name(name)
            .expiration(t -> t.time(expiration))
            .roleDescriptors(roleDescriptors.stream()
                .collect(Collectors.toMap(
                    role -> role,
                    role -> RoleDescriptor.of(rd -> rd
                        .cluster(List.of("monitor"))
                        .indices(List.of(IndexPrivileges.of(ip -> ip
                            .names("logs-*")
                            .privileges("read", "view_index_metadata")
                        )))
                    )
                ))
            )
        );
        
        return client.security().createApiKey(request);
    }
    
    /**
     * è·å–ç”¨æˆ·æƒé™
     */
    public GetUserPrivilegesResponse getUserPrivileges() throws IOException {
        GetUserPrivilegesRequest request = GetUserPrivilegesRequest.of(p -> p);
        return client.security().getUserPrivileges(request);
    }
}
```

### 2. SSL/TLSé…ç½®

```yaml
# elasticsearch.yml SSLé…ç½®
xpack.security.enabled: true

# HTTP SSLé…ç½®
xpack.security.http.ssl.enabled: true
xpack.security.http.ssl.keystore.path: certs/elastic-certificates.p12
xpack.security.http.ssl.truststore.path: certs/elastic-certificates.p12

# ä¼ è¾“å±‚SSLé…ç½®
xpack.security.transport.ssl.enabled: true
xpack.security.transport.ssl.verification_mode: certificate
xpack.security.transport.ssl.keystore.path: certs/elastic-certificates.p12
xpack.security.transport.ssl.truststore.path: certs/elastic-certificates.p12

# å®¡è®¡æ—¥å¿—é…ç½®
xpack.security.audit.enabled: true
xpack.security.audit.logfile.events.include: 
  - access_granted
  - access_denied
  - anonymous_access_denied
  - authentication_failed
  - connection_granted
  - connection_denied
  - tampered_request
  - run_as_granted
  - run_as_denied
```

## ï¼ˆäºŒï¼‰æ•°æ®åŠ å¯†ä¸å®¡è®¡

### 1. å­—æ®µçº§åŠ å¯†

```java
/**
 * æ•°æ®åŠ å¯†æœåŠ¡
 */
@Service
public class DataEncryptionService {
    
    private final AESUtil aesUtil = new AESUtil();
    
    /**
     * åŠ å¯†æ•æ„Ÿæ•°æ®
     */
    public Document encryptSensitiveFields(Document document) {
        Document encryptedDoc = new Document();
        
        // å¤åˆ¶éæ•æ„Ÿå­—æ®µ
        encryptedDoc.setId(document.getId());
        encryptedDoc.setTitle(document.getTitle());
        encryptedDoc.setCreateTime(document.getCreateTime());
        
        // åŠ å¯†æ•æ„Ÿå­—æ®µ
        if (document.getEmail() != null) {
            encryptedDoc.setEmail(aesUtil.encrypt(document.getEmail()));
        }
        
        if (document.getPhone() != null) {
            encryptedDoc.setPhone(aesUtil.encrypt(document.getPhone()));
        }
        
        if (document.getIdCard() != null) {
            encryptedDoc.setIdCard(aesUtil.encrypt(document.getIdCard()));
        }
        
        return encryptedDoc;
    }
    
    /**
     * è§£å¯†æ•æ„Ÿæ•°æ®
     */
    public Document decryptSensitiveFields(Document encryptedDocument) {
        Document decryptedDoc = new Document();
        
        // å¤åˆ¶éæ•æ„Ÿå­—æ®µ
        decryptedDoc.setId(encryptedDocument.getId());
        decryptedDoc.setTitle(encryptedDocument.getTitle());
        decryptedDoc.setCreateTime(encryptedDocument.getCreateTime());
        
        // è§£å¯†æ•æ„Ÿå­—æ®µ
        if (encryptedDocument.getEmail() != null) {
            decryptedDoc.setEmail(aesUtil.decrypt(encryptedDocument.getEmail()));
        }
        
        if (encryptedDocument.getPhone() != null) {
            decryptedDoc.setPhone(aesUtil.decrypt(encryptedDocument.getPhone()));
        }
        
        if (encryptedDocument.getIdCard() != null) {
            decryptedDoc.setIdCard(aesUtil.decrypt(encryptedDocument.getIdCard()));
        }
        
        return decryptedDoc;
    }
}

/**
 * AESåŠ å¯†å·¥å…·ç±»
 */
public class AESUtil {
    private static final String ALGORITHM = "AES/GCM/NoPadding";
    private static final String KEY_ALGORITHM = "AES";
    private static final int GCM_IV_LENGTH = 12;
    private static final int GCM_TAG_LENGTH = 16;
    
    private final SecretKey secretKey;
    
    public AESUtil() {
        this.secretKey = generateKey();
    }
    
    private SecretKey generateKey() {
        try {
            KeyGenerator keyGenerator = KeyGenerator.getInstance(KEY_ALGORITHM);
            keyGenerator.init(256);
            return keyGenerator.generateKey();
        } catch (NoSuchAlgorithmException e) {
            throw new RuntimeException("AESå¯†é’¥ç”Ÿæˆå¤±è´¥", e);
        }
    }
    
    public String encrypt(String plainText) {
        try {
            Cipher cipher = Cipher.getInstance(ALGORITHM);
            
            byte[] iv = new byte[GCM_IV_LENGTH];
            SecureRandom.getInstanceStrong().nextBytes(iv);
            GCMParameterSpec parameterSpec = new GCMParameterSpec(GCM_TAG_LENGTH * 8, iv);
            
            cipher.init(Cipher.ENCRYPT_MODE, secretKey, parameterSpec);
            
            byte[] encryptedData = cipher.doFinal(plainText.getBytes(StandardCharsets.UTF_8));
            
            byte[] encryptedWithIv = new byte[iv.length + encryptedData.length];
            System.arraycopy(iv, 0, encryptedWithIv, 0, iv.length);
            System.arraycopy(encryptedData, 0, encryptedWithIv, iv.length, encryptedData.length);
            
            return Base64.getEncoder().encodeToString(encryptedWithIv);
        } catch (Exception e) {
            throw new RuntimeException("åŠ å¯†å¤±è´¥", e);
        }
    }
    
    public String decrypt(String encryptedText) {
        try {
            byte[] decodedData = Base64.getDecoder().decode(encryptedText);
            
            byte[] iv = new byte[GCM_IV_LENGTH];
            System.arraycopy(decodedData, 0, iv, 0, iv.length);
            
            byte[] encryptedData = new byte[decodedData.length - GCM_IV_LENGTH];
            System.arraycopy(decodedData, GCM_IV_LENGTH, encryptedData, 0, encryptedData.length);
            
            Cipher cipher = Cipher.getInstance(ALGORITHM);
            GCMParameterSpec parameterSpec = new GCMParameterSpec(GCM_TAG_LENGTH * 8, iv);
            cipher.init(Cipher.DECRYPT_MODE, secretKey, parameterSpec);
            
            byte[] decryptedData = cipher.doFinal(encryptedData);
            
            return new String(decryptedData, StandardCharsets.UTF_8);
        } catch (Exception e) {
            throw new RuntimeException("è§£å¯†å¤±è´¥", e);
        }
    }
}
```

### 2. å®¡è®¡æ—¥å¿—åˆ†æ

```java
/**
 * å®¡è®¡æ—¥å¿—åˆ†ææœåŠ¡
 */
@Service
public class AuditLogAnalysisService {
    
    @Autowired
    private ElasticsearchClient client;
    
    private static final String AUDIT_INDEX = ".security-audit-*";
    
    /**
     * åˆ†æç™»å½•å¤±è´¥äº‹ä»¶
     */
    public List<SecurityEvent> analyzeFailedLogins(String timeRange) throws IOException {
        SearchRequest request = SearchRequest.of(s -> s
            .index(AUDIT_INDEX)
            .query(q -> q
                .bool(b -> b
                    .must(must -> must
                        .term(t -> t
                            .field("event_type")
                            .value("authentication_failed")
                        )
                    )
                    .filter(filter -> filter
                        .range(r -> r
                            .field("@timestamp")
                            .gte(JsonData.of(timeRange))
                        )
                    )
                )
            )
            .aggregations("failed_by_user", a -> a
                .terms(t -> t
                    .field("user.name")
                    .size(20)
                )
            )
            .aggregations("failed_by_ip", a -> a
                .terms(t -> t
                    .field("origin.address")
                    .size(20)
                )
            )
            .sort(so -> so
                .field(f -> f.field("@timestamp").order(SortOrder.Desc))
            )
            .size(100)
        );
        
        SearchResponse<SecurityEvent> response = client.search(request, SecurityEvent.class);
        
        return response.hits().hits().stream()
                .map(Hit::source)
                .collect(Collectors.toList());
    }
    
    /**
     * æ£€æµ‹å¼‚å¸¸è®¿é—®æ¨¡å¼
     */
    public List<AnomalousActivity> detectAnomalousActivity() throws IOException {
        List<AnomalousActivity> anomalies = new ArrayList<>();
        
        // æ£€æµ‹çŸ­æ—¶é—´å†…å¤§é‡å¤±è´¥ç™»å½•
        SearchRequest failedLoginsRequest = SearchRequest.of(s -> s
            .index(AUDIT_INDEX)
            .size(0)
            .query(q -> q
                .bool(b -> b
                    .must(must -> must
                        .term(t -> t
                            .field("event_type")
                            .value("authentication_failed")
                        )
                    )
                    .filter(filter -> filter
                        .range(r -> r
                            .field("@timestamp")
                            .gte(JsonData.of("now-1h"))
                        )
                    )
                )
            )
            .aggregations("failed_by_ip", a -> a
                .terms(t -> t
                    .field("origin.address")
                    .size(50)
                    .minDocCount(10)  // è‡³å°‘10æ¬¡å¤±è´¥
                )
            )
        );
        
        SearchResponse<Void> failedResponse = client.search(failedLoginsRequest, Void.class);
        
        StringTermsAggregate failedByIp = failedResponse.aggregations()
                .get("failed_by_ip")
                .sterms();
        
        for (StringTermsBucket bucket : failedByIp.buckets().array()) {
            if (bucket.docCount() >= 10) {
                anomalies.add(new AnomalousActivity(
                    "BRUTE_FORCE_ATTACK",
                    "IPåœ°å€ " + bucket.key().stringValue() + " åœ¨1å°æ—¶å†…å¤±è´¥ç™»å½• " + bucket.docCount() + " æ¬¡",
                    bucket.key().stringValue(),
                    "HIGH"
                ));
            }
        }
        
        // æ£€æµ‹å¼‚å¸¸æ—¶é—´è®¿é—®
        SearchRequest offHoursRequest = SearchRequest.of(s -> s
            .index(AUDIT_INDEX)
            .size(0)
            .query(q -> q
                .bool(b -> b
                    .must(must -> must
                        .term(t -> t
                            .field("event_type")
                            .value("access_granted")
                        )
                    )
                    .filter(filter -> filter
                        .range(r -> r
                            .field("@timestamp")
                            .gte(JsonData.of("now-24h"))
                        )
                    )
                    .filter(filter -> filter
                        .script(sc -> sc
                            .source("doc['@timestamp'].value.getHour() < 6 || doc['@timestamp'].value.getHour() > 22")
                        )
                    )
                )
            )
            .aggregations("off_hours_by_user", a -> a
                .terms(t -> t
                    .field("user.name")
                    .size(20)
                    .minDocCount(5)
                )
            )
        );
        
        SearchResponse<Void> offHoursResponse = client.search(offHoursRequest, Void.class);
        
        StringTermsAggregate offHoursByUser = offHoursResponse.aggregations()
                .get("off_hours_by_user")
                .sterms();
        
        for (StringTermsBucket bucket : offHoursByUser.buckets().array()) {
            anomalies.add(new AnomalousActivity(
                "OFF_HOURS_ACCESS",
                "ç”¨æˆ· " + bucket.key().stringValue() + " åœ¨éå·¥ä½œæ—¶é—´è®¿é—®ç³»ç»Ÿ " + bucket.docCount() + " æ¬¡",
                bucket.key().stringValue(),
                "MEDIUM"
            ));
        }
        
        return anomalies;
    }
    
    /**
     * ç”Ÿæˆå®‰å…¨æŠ¥å‘Š
     */
    public SecurityReport generateSecurityReport(String timeRange) throws IOException {
        SecurityReport report = new SecurityReport();
        
        // ç»Ÿè®¡æ€»ä½“è®¿é—®æƒ…å†µ
        SearchRequest totalAccessRequest = SearchRequest.of(s -> s
            .index(AUDIT_INDEX)
            .size(0)
            .query(q -> q
                .bool(b -> b
                    .filter(filter -> filter
                        .range(r -> r
                            .field("@timestamp")
                            .gte(JsonData.of(timeRange))
                        )
                    )
                )
            )
            .aggregations("by_event_type", a -> a
                .terms(t -> t
                    .field("event_type")
                    .size(20)
                )
            )
        );
        
        SearchResponse<Void> response = client.search(totalAccessRequest, Void.class);
        
        StringTermsAggregate byEventType = response.aggregations()
                .get("by_event_type")
                .sterms();
        
        Map<String, Long> eventCounts = new HashMap<>();
        for (StringTermsBucket bucket : byEventType.buckets().array()) {
            eventCounts.put(bucket.key().stringValue(), bucket.docCount());
        }
        
        report.setEventCounts(eventCounts);
        report.setAnomalousActivities(detectAnomalousActivity());
        report.setGeneratedAt(LocalDateTime.now());
        
        return report;
    }
}
```

# å››ã€ç›‘æ§ä¸è¿ç»´

## ï¼ˆä¸€ï¼‰é›†ç¾¤ç›‘æ§

### 1. ç›‘æ§æŒ‡æ ‡æ”¶é›†

```java
/**
 * é›†ç¾¤ç›‘æ§æœåŠ¡
 */
@Service
public class ClusterMonitoringService {
    
    @Autowired
    private ElasticsearchClient client;
    
    /**
     * æ”¶é›†é›†ç¾¤å¥åº·æŒ‡æ ‡
     */
    public ClusterHealthMetrics collectHealthMetrics() throws IOException {
        ClusterHealthResponse health = client.cluster().health(
            ClusterHealthRequest.of(h -> h.timeout(t -> t.time("30s")))
        );
        
        ClusterStatsResponse stats = client.cluster().stats(
            ClusterStatsRequest.of(s -> s)
        );
        
        NodesStatsResponse nodesStats = client.nodes().stats(
            NodesStatsRequest.of(n -> n
                .metric(NodeStatsMetric.Os, NodeStatsMetric.Process, 
                       NodeStatsMetric.Jvm, NodeStatsMetric.Indices)
            )
        );
        
        ClusterHealthMetrics metrics = new ClusterHealthMetrics();
        
        // é›†ç¾¤å¥åº·çŠ¶æ€
        metrics.setClusterStatus(health.status().toString());
        metrics.setActiveShards(health.activeShards());
        metrics.setRelocatingShards(health.relocatingShards());
        metrics.setInitializingShards(health.initializingShards());
        metrics.setUnassignedShards(health.unassignedShards());
        metrics.setNumberOfNodes(health.numberOfNodes());
        metrics.setNumberOfDataNodes(health.numberOfDataNodes());
        
        // é›†ç¾¤ç»Ÿè®¡
        if (stats.indices() != null) {
            metrics.setTotalDocuments(stats.indices().count().longValue());
            metrics.setTotalIndexSize(stats.indices().store().sizeInBytes());
        }
        
        // èŠ‚ç‚¹ç»Ÿè®¡
        Map<String, NodeMetrics> nodeMetrics = new HashMap<>();
        nodesStats.nodes().forEach((nodeId, nodeStats) -> {
            NodeMetrics nodeMetric = new NodeMetrics();
            nodeMetric.setNodeId(nodeId);
            nodeMetric.setNodeName(nodeStats.name());
            
            // JVMæŒ‡æ ‡
            if (nodeStats.jvm() != null) {
                JvmStats jvm = nodeStats.jvm();
                if (jvm.mem() != null) {
                    nodeMetric.setHeapUsedPercent(jvm.mem().heapUsedPercent());
                    nodeMetric.setHeapUsed(jvm.mem().heapUsed().bytes());
                    nodeMetric.setHeapMax(jvm.mem().heapMax().bytes());
                }
                
                if (jvm.gc() != null && jvm.gc().collectors() != null) {
                    long totalGcTime = jvm.gc().collectors().values().stream()
                            .mapToLong(gc -> gc.collectionTime().millis())
                            .sum();
                    nodeMetric.setGcTime(totalGcTime);
                }
            }
            
            // æ“ä½œç³»ç»ŸæŒ‡æ ‡
            if (nodeStats.os() != null) {
                OsStats os = nodeStats.os();
                if (os.cpu() != null) {
                    nodeMetric.setCpuPercent(os.cpu().percent());
                }
                if (os.mem() != null) {
                    nodeMetric.setMemoryUsedPercent(os.mem().usedPercent());
                }
            }
            
            // ç´¢å¼•æŒ‡æ ‡
            if (nodeStats.indices() != null) {
                IndicesStats indices = nodeStats.indices();
                if (indices.indexing() != null) {
                    nodeMetric.setIndexingRate(indices.indexing().indexTotal());
                }
                if (indices.search() != null) {
                    nodeMetric.setSearchRate(indices.search().queryTotal());
                }
            }
            
            nodeMetrics.put(nodeId, nodeMetric);
        });
        
        metrics.setNodeMetrics(nodeMetrics);
        metrics.setTimestamp(LocalDateTime.now());
        
        return metrics;
    }
    
    /**
     * æ£€æŸ¥é›†ç¾¤å¥åº·çŠ¶å†µ
     */
    public List<HealthAlert> checkClusterHealth() throws IOException {
        List<HealthAlert> alerts = new ArrayList<>();
        ClusterHealthMetrics metrics = collectHealthMetrics();
        
        // æ£€æŸ¥é›†ç¾¤çŠ¶æ€
        if ("red".equals(metrics.getClusterStatus())) {
            alerts.add(new HealthAlert(
                "CLUSTER_RED",
                "é›†ç¾¤çŠ¶æ€ä¸ºçº¢è‰²ï¼Œå­˜åœ¨ä¸å¯ç”¨çš„ä¸»åˆ†ç‰‡",
                "CRITICAL"
            ));
        } else if ("yellow".equals(metrics.getClusterStatus())) {
            alerts.add(new HealthAlert(
                "CLUSTER_YELLOW",
                "é›†ç¾¤çŠ¶æ€ä¸ºé»„è‰²ï¼Œå­˜åœ¨æœªåˆ†é…çš„å‰¯æœ¬åˆ†ç‰‡",
                "WARNING"
            ));
        }
        
        // æ£€æŸ¥æœªåˆ†é…åˆ†ç‰‡
        if (metrics.getUnassignedShards() > 0) {
            alerts.add(new HealthAlert(
                "UNASSIGNED_SHARDS",
                "å­˜åœ¨ " + metrics.getUnassignedShards() + " ä¸ªæœªåˆ†é…çš„åˆ†ç‰‡",
                "WARNING"
            ));
        }
        
        // æ£€æŸ¥èŠ‚ç‚¹å¥åº·
        metrics.getNodeMetrics().forEach((nodeId, nodeMetric) -> {
            // æ£€æŸ¥å †å†…å­˜ä½¿ç”¨ç‡
            if (nodeMetric.getHeapUsedPercent() > 85) {
                alerts.add(new HealthAlert(
                    "HIGH_HEAP_USAGE",
                    "èŠ‚ç‚¹ " + nodeMetric.getNodeName() + " å †å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: " + 
                    nodeMetric.getHeapUsedPercent() + "%",
                    nodeMetric.getHeapUsedPercent() > 95 ? "CRITICAL" : "WARNING"
                ));
            }
            
            // æ£€æŸ¥CPUä½¿ç”¨ç‡
            if (nodeMetric.getCpuPercent() > 80) {
                alerts.add(new HealthAlert(
                    "HIGH_CPU_USAGE",
                    "èŠ‚ç‚¹ " + nodeMetric.getNodeName() + " CPUä½¿ç”¨ç‡è¿‡é«˜: " + 
                    nodeMetric.getCpuPercent() + "%",
                    "WARNING"
                ));
            }
            
            // æ£€æŸ¥å†…å­˜ä½¿ç”¨ç‡
            if (nodeMetric.getMemoryUsedPercent() > 90) {
                alerts.add(new HealthAlert(
                    "HIGH_MEMORY_USAGE",
                    "èŠ‚ç‚¹ " + nodeMetric.getNodeName() + " å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: " + 
                    nodeMetric.getMemoryUsedPercent() + "%",
                    "WARNING"
                ));
            }
        });
        
        return alerts;
    }
}

/**
 * é›†ç¾¤å¥åº·æŒ‡æ ‡
 */
public class ClusterHealthMetrics {
    private String clusterStatus;
    private int activeShards;
    private int relocatingShards;
    private int initializingShards;
    private int unassignedShards;
    private int numberOfNodes;
    private int numberOfDataNodes;
    private long totalDocuments;
    private long totalIndexSize;
    private Map<String, NodeMetrics> nodeMetrics;
    private LocalDateTime timestamp;
    
    // Getterå’ŒSetteræ–¹æ³•
    public String getClusterStatus() { return clusterStatus; }
    public void setClusterStatus(String clusterStatus) { this.clusterStatus = clusterStatus; }
    
    public int getActiveShards() { return activeShards; }
    public void setActiveShards(int activeShards) { this.activeShards = activeShards; }
    
    public int getUnassignedShards() { return unassignedShards; }
    public void setUnassignedShards(int unassignedShards) { this.unassignedShards = unassignedShards; }
    
    public Map<String, NodeMetrics> getNodeMetrics() { return nodeMetrics; }
    public void setNodeMetrics(Map<String, NodeMetrics> nodeMetrics) { this.nodeMetrics = nodeMetrics; }
    
    // å…¶ä»–getter/setteræ–¹æ³•...
}

/**
 * èŠ‚ç‚¹æŒ‡æ ‡
 */
public class NodeMetrics {
    private String nodeId;
    private String nodeName;
    private int heapUsedPercent;
    private long heapUsed;
    private long heapMax;
    private long gcTime;
    private int cpuPercent;
    private int memoryUsedPercent;
    private long indexingRate;
    private long searchRate;
    
    // Getterå’ŒSetteræ–¹æ³•
    public String getNodeName() { return nodeName; }
    public void setNodeName(String nodeName) { this.nodeName = nodeName; }
    
    public int getHeapUsedPercent() { return heapUsedPercent; }
    public void setHeapUsedPercent(int heapUsedPercent) { this.heapUsedPercent = heapUsedPercent; }
    
    public int getCpuPercent() { return cpuPercent; }
    public void setCpuPercent(int cpuPercent) { this.cpuPercent = cpuPercent; }
    
    public int getMemoryUsedPercent() { return memoryUsedPercent; }
    public void setMemoryUsedPercent(int memoryUsedPercent) { this.memoryUsedPercent = memoryUsedPercent; }
    
    // å…¶ä»–getter/setteræ–¹æ³•...
}

/**
 * å¥åº·å‘Šè­¦
 */
public class HealthAlert {
    private String type;
    private String message;
    private String severity;
    private LocalDateTime timestamp;
    
    public HealthAlert(String type, String message, String severity) {
        this.type = type;
        this.message = message;
        this.severity = severity;
        this.timestamp = LocalDateTime.now();
    }
    
    // Getteræ–¹æ³•
    public String getType() { return type; }
    public String getMessage() { return message; }
    public String getSeverity() { return severity; }
    public LocalDateTime getTimestamp() { return timestamp; }
}
```

### 2. æ€§èƒ½ç›‘æ§ä¸å‘Šè­¦

```java
/**
 * æ€§èƒ½ç›‘æ§æœåŠ¡
 */
@Service
public class PerformanceMonitoringService {
    
    @Autowired
    private ElasticsearchClient client;
    
    /**
     * ç›‘æ§ç´¢å¼•æ€§èƒ½
     */
    public IndexPerformanceMetrics monitorIndexPerformance(String indexName) throws IOException {
        IndicesStatsRequest request = IndicesStatsRequest.of(s -> s
            .index(indexName)
            .metric(IndicesStatsMetric.Indexing, IndicesStatsMetric.Search, 
                   IndicesStatsMetric.Store, IndicesStatsMetric.Merge)
        );
        
        IndicesStatsResponse response = client.indices().stats(request);
        
        IndexPerformanceMetrics metrics = new IndexPerformanceMetrics();
        metrics.setIndexName(indexName);
        
        if (response.indices().containsKey(indexName)) {
            IndexStats indexStats = response.indices().get(indexName);
            
            // ç´¢å¼•æ€§èƒ½æŒ‡æ ‡
            if (indexStats.total().indexing() != null) {
                IndexingStats indexing = indexStats.total().indexing();
                metrics.setIndexTotal(indexing.indexTotal());
                metrics.setIndexTimeInMillis(indexing.indexTimeInMillis());
                metrics.setIndexCurrent(indexing.indexCurrent());
                
                if (indexing.indexTotal() > 0) {
                    metrics.setAvgIndexTime(indexing.indexTimeInMillis() / indexing.indexTotal());
                }
            }
            
            // æœç´¢æ€§èƒ½æŒ‡æ ‡
            if (indexStats.total().search() != null) {
                SearchStats search = indexStats.total().search();
                metrics.setQueryTotal(search.queryTotal());
                metrics.setQueryTimeInMillis(search.queryTimeInMillis());
                metrics.setQueryCurrent(search.queryCurrent());
                
                if (search.queryTotal() > 0) {
                    metrics.setAvgQueryTime(search.queryTimeInMillis() / search.queryTotal());
                }
            }
            
            // å­˜å‚¨æŒ‡æ ‡
            if (indexStats.total().store() != null) {
                StoreStats store = indexStats.total().store();
                metrics.setStoreSizeInBytes(store.sizeInBytes());
            }
            
            // åˆå¹¶æŒ‡æ ‡
            if (indexStats.total().merge() != null) {
                MergeStats merge = indexStats.total().merge();
                metrics.setMergeTotal(merge.total());
                metrics.setMergeTimeInMillis(merge.totalTimeInMillis());
                metrics.setMergeCurrent(merge.current());
            }
        }
        
        metrics.setTimestamp(LocalDateTime.now());
        return metrics;
    }
    
    /**
     * æ£€æµ‹æ€§èƒ½å¼‚å¸¸
     */
    public List<PerformanceAlert> detectPerformanceAnomalies(String indexName) throws IOException {
        List<PerformanceAlert> alerts = new ArrayList<>();
        IndexPerformanceMetrics metrics = monitorIndexPerformance(indexName);
        
        // æ£€æŸ¥å¹³å‡ç´¢å¼•æ—¶é—´
        if (metrics.getAvgIndexTime() > 1000) {  // è¶…è¿‡1ç§’
            alerts.add(new PerformanceAlert(
                "SLOW_INDEXING",
                "ç´¢å¼• " + indexName + " å¹³å‡ç´¢å¼•æ—¶é—´è¿‡é•¿: " + metrics.getAvgIndexTime() + "ms",
                "WARNING"
            ));
        }
        
        // æ£€æŸ¥å¹³å‡æŸ¥è¯¢æ—¶é—´
        if (metrics.getAvgQueryTime() > 5000) {  // è¶…è¿‡5ç§’
            alerts.add(new PerformanceAlert(
                "SLOW_QUERY",
                "ç´¢å¼• " + indexName + " å¹³å‡æŸ¥è¯¢æ—¶é—´è¿‡é•¿: " + metrics.getAvgQueryTime() + "ms",
                "WARNING"
            ));
        }
        
        // æ£€æŸ¥å½“å‰ç´¢å¼•é˜Ÿåˆ—
        if (metrics.getIndexCurrent() > 100) {
            alerts.add(new PerformanceAlert(
                "HIGH_INDEX_QUEUE",
                "ç´¢å¼• " + indexName + " å½“å‰ç´¢å¼•é˜Ÿåˆ—è¿‡é•¿: " + metrics.getIndexCurrent(),
                "WARNING"
            ));
        }
        
        // æ£€æŸ¥å½“å‰æŸ¥è¯¢é˜Ÿåˆ—
        if (metrics.getQueryCurrent() > 50) {
            alerts.add(new PerformanceAlert(
                "HIGH_QUERY_QUEUE",
                "ç´¢å¼• " + indexName + " å½“å‰æŸ¥è¯¢é˜Ÿåˆ—è¿‡é•¿: " + metrics.getQueryCurrent(),
                "WARNING"
            ));
        }
        
        return alerts;
    }
}
```

## ï¼ˆäºŒï¼‰æ—¥å¿—ç®¡ç†ä¸åˆ†æ

### 1. æ—¥å¿—æ”¶é›†é…ç½®

```yaml
# filebeat.yml
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/elasticsearch/*.log
  fields:
    service: elasticsearch
    environment: production
  fields_under_root: true
  multiline.pattern: '^\['
  multiline.negate: true
  multiline.match: after

- type: log
  enabled: true
  paths:
    - /var/log/application/*.log
  fields:
    service: application
    environment: production
  fields_under_root: true

processors:
- add_host_metadata:
    when.not.contains.tags: forwarded
- add_docker_metadata: ~
- add_kubernetes_metadata: ~

output.elasticsearch:
  hosts: ["elasticsearch-master-1:9200", "elasticsearch-master-2:9200"]
  index: "logs-%{[service]}-%{+yyyy.MM.dd}"
  template.name: "logs"
  template.pattern: "logs-*"
  template.settings:
    index.number_of_shards: 3
    index.number_of_replicas: 1
    index.refresh_interval: "30s"

logging.level: info
logging.to_files: true
logging.files:
  path: /var/log/filebeat
  name: filebeat
  keepfiles: 7
  permissions: 0644
```

### 2. æ—¥å¿—åˆ†ææœåŠ¡

```java
/**
 * æ—¥å¿—åˆ†ææœåŠ¡
 */
@Service
public class LogAnalysisService {
    
    @Autowired
    private ElasticsearchClient client;
    
    /**
     * åˆ†æé”™è¯¯æ—¥å¿—
     */
    public ErrorLogAnalysis analyzeErrorLogs(String timeRange) throws IOException {
        SearchRequest request = SearchRequest.of(s -> s
            .index("logs-*")
            .query(q -> q
                .bool(b -> b
                    .must(must -> must
                        .terms(t -> t
                            .field("level")
                            .terms(TermsQueryField.of(tf -> tf.value(
                                List.of(FieldValue.of("ERROR"), FieldValue.of("FATAL"))
                            )))
                        )
                    )
                    .filter(filter -> filter
                        .range(r -> r
                            .field("@timestamp")
                            .gte(JsonData.of(timeRange))
                        )
                    )
                )
            )
            .aggregations("errors_by_service", a -> a
                .terms(t -> t
                    .field("service")
                    .size(20)
                )
                .aggregations("errors_over_time", sub -> sub
                    .dateHistogram(dh -> dh
                        .field("@timestamp")
                        .calendarInterval(CalendarInterval.Hour)
                    )
                )
            )
            .aggregations("top_error_messages", a -> a
                .terms(t -> t
                    .field("message.keyword")
                    .size(10)
                )
            )
            .size(100)
            .sort(so -> so
                .field(f -> f.field("@timestamp").order(SortOrder.Desc))
            )
        );
        
        SearchResponse<LogEntry> response = client.search(request, LogEntry.class);
        
        ErrorLogAnalysis analysis = new ErrorLogAnalysis();
        analysis.setTotalErrors(response.hits().total().value());
        analysis.setTimeRange(timeRange);
        
        // æŒ‰æœåŠ¡ç»Ÿè®¡é”™è¯¯
        StringTermsAggregate errorsByService = response.aggregations()
                .get("errors_by_service")
                .sterms();
        
        Map<String, Long> serviceErrors = new HashMap<>();
        for (StringTermsBucket bucket : errorsByService.buckets().array()) {
            serviceErrors.put(bucket.key().stringValue(), bucket.docCount());
        }
        analysis.setErrorsByService(serviceErrors);
        
        // çƒ­é—¨é”™è¯¯æ¶ˆæ¯
        StringTermsAggregate topErrorMessages = response.aggregations()
                .get("top_error_messages")
                .sterms();
        
        List<ErrorPattern> errorPatterns = new ArrayList<>();
        for (StringTermsBucket bucket : topErrorMessages.buckets().array()) {
            errorPatterns.add(new ErrorPattern(
                bucket.key().stringValue(),
                bucket.docCount()
            ));
        }
        analysis.setTopErrorPatterns(errorPatterns);
        
        // æœ€æ–°é”™è¯¯æ—¥å¿—
        List<LogEntry> recentErrors = response.hits().hits().stream()
                .map(Hit::source)
                .collect(Collectors.toList());
        analysis.setRecentErrors(recentErrors);
        
        return analysis;
    }
    
    /**
     * æ€§èƒ½æ—¥å¿—åˆ†æ
     */
    public PerformanceLogAnalysis analyzePerformanceLogs(String timeRange) throws IOException {
        SearchRequest request = SearchRequest.of(s -> s
            .index("logs-application-*")
            .query(q -> q
                .bool(b -> b
                    .must(must -> must
                        .exists(e -> e.field("response_time"))
                    )
                    .filter(filter -> filter
                        .range(r -> r
                            .field("@timestamp")
                            .gte(JsonData.of(timeRange))
                        )
                    )
                )
            )
            .aggregations("avg_response_time", a -> a
                .avg(avg -> avg.field("response_time"))
            )
            .aggregations("response_time_percentiles", a -> a
                .percentiles(p -> p
                    .field("response_time")
                    .percents(50.0, 90.0, 95.0, 99.0)
                )
            )
            .aggregations("slow_requests", a -> a
                .filter(f -> f
                    .range(r -> r
                        .field("response_time")
                        .gte(JsonData.of(5000))  // è¶…è¿‡5ç§’çš„è¯·æ±‚
                    )
                )
                .aggregations("slow_by_endpoint", sub -> sub
                    .terms(t -> t
                        .field("endpoint.keyword")
                        .size(10)
                    )
                )
            )
            .aggregations("response_time_over_time", a -> a
                .dateHistogram(dh -> dh
                    .field("@timestamp")
                    .calendarInterval(CalendarInterval.Minute)
                )
                .aggregations("avg_response_time", sub -> sub
                    .avg(avg -> avg.field("response_time"))
                )
            )
            .size(0)
        );
        
        SearchResponse<Void> response = client.search(request, Void.class);
        
        PerformanceLogAnalysis analysis = new PerformanceLogAnalysis();
        
        // å¹³å‡å“åº”æ—¶é—´
        AvgAggregate avgResponseTime = response.aggregations()
                .get("avg_response_time")
                .avg();
        analysis.setAverageResponseTime(avgResponseTime.value());
        
        // å“åº”æ—¶é—´ç™¾åˆ†ä½æ•°
        PercentilesAggregate percentiles = response.aggregations()
                .get("response_time_percentiles")
                .percentiles();
        
        Map<String, Double> responseTimePercentiles = new HashMap<>();
        percentiles.values().forEach((key, value) -> {
            responseTimePercentiles.put("p" + key, value);
        });
        analysis.setResponseTimePercentiles(responseTimePercentiles);
        
        // æ…¢è¯·æ±‚åˆ†æ
        FilterAggregate slowRequests = response.aggregations()
                .get("slow_requests")
                .filter();
        analysis.setSlowRequestCount(slowRequests.docCount());
        
        if (slowRequests.aggregations().containsKey("slow_by_endpoint")) {
            StringTermsAggregate slowByEndpoint = slowRequests.aggregations()
                    .get("slow_by_endpoint")
                    .sterms();
            
            Map<String, Long> slowEndpoints = new HashMap<>();
            for (StringTermsBucket bucket : slowByEndpoint.buckets().array()) {
                slowEndpoints.put(bucket.key().stringValue(), bucket.docCount());
            }
            analysis.setSlowEndpoints(slowEndpoints);
        }
        
        return analysis;
    }
}
```

## ï¼ˆä¸‰ï¼‰å¤‡ä»½ä¸æ¢å¤

### 1. å¿«ç…§å¤‡ä»½ç­–ç•¥

```java
/**
 * å¤‡ä»½æ¢å¤æœåŠ¡
 */
@Service
public class BackupRestoreService {
    
    @Autowired
    private ElasticsearchClient client;
    
    /**
     * åˆ›å»ºå¿«ç…§ä»“åº“
     */
    public void createSnapshotRepository(String repositoryName, String location) throws IOException {
        PutRepositoryRequest request = PutRepositoryRequest.of(r -> r
            .name(repositoryName)
            .type("fs")
            .settings(s -> s
                .put("location", JsonData.of(location))
                .put("compress", JsonData.of(true))
                .put("chunk_size", JsonData.of("1gb"))
                .put("max_restore_bytes_per_sec", JsonData.of("100mb"))
                .put("max_snapshot_bytes_per_sec", JsonData.of("100mb"))
            )
        );
        
        PutRepositoryResponse response = client.snapshot().createRepository(request);
        System.out.println("å¿«ç…§ä»“åº“åˆ›å»ºç»“æœ: " + response.acknowledged());
    }
    
    /**
     * åˆ›å»ºå¿«ç…§
     */
    public void createSnapshot(String repositoryName, String snapshotName, 
                             List<String> indices) throws IOException {
        CreateSnapshotRequest request = CreateSnapshotRequest.of(s -> s
            .repository(repositoryName)
            .snapshot(snapshotName)
            .indices(indices)
            .ignoreUnavailable(true)
            .includeGlobalState(false)
            .waitForCompletion(false)  // å¼‚æ­¥æ‰§è¡Œ
            .metadata(Map.of(
                "created_by", "backup_service",
                "created_at", LocalDateTime.now().toString()
            ))
        );
        
        CreateSnapshotResponse response = client.snapshot().create(request);
        System.out.println("å¿«ç…§åˆ›å»ºç»“æœ: " + response.snapshot().state());
    }
    
    /**
     * æ¢å¤å¿«ç…§
     */
    public void restoreSnapshot(String repositoryName, String snapshotName, 
                              List<String> indices, String renamePattern, 
                              String renameReplacement) throws IOException {
        RestoreRequest request = RestoreRequest.of(r -> r
            .repository(repositoryName)
            .snapshot(snapshotName)
            .indices(indices)
            .ignoreUnavailable(true)
            .includeGlobalState(false)
            .renamePattern(renamePattern)
            .renameReplacement(renameReplacement)
            .waitForCompletion(false)
            .indexSettings(is -> is
                .put("index.number_of_replicas", JsonData.of(1))
            )
        );
        
        RestoreResponse response = client.snapshot().restore(request);
        System.out.println("å¿«ç…§æ¢å¤ç»“æœ: " + response.snapshot().shards().successful());
    }
    
    /**
     * è‡ªåŠ¨å¤‡ä»½è°ƒåº¦
     */
    @Scheduled(cron = "0 0 2 * * ?")  // æ¯å¤©å‡Œæ™¨2ç‚¹æ‰§è¡Œ
    public void scheduledBackup() {
        try {
            String repositoryName = "daily_backup";
            String snapshotName = "snapshot_" + LocalDate.now().format(DateTimeFormatter.ofPattern("yyyy_MM_dd"));
            List<String> indices = List.of("logs-*", "metrics-*", "user_data-*");
            
            createSnapshot(repositoryName, snapshotName, indices);
            
            // æ¸…ç†æ—§å¿«ç…§ï¼ˆä¿ç•™30å¤©ï¼‰
            cleanupOldSnapshots(repositoryName, 30);
            
        } catch (IOException e) {
            System.err.println("è‡ªåŠ¨å¤‡ä»½å¤±è´¥: " + e.getMessage());
        }
    }
    
    /**
     * æ¸…ç†æ—§å¿«ç…§
     */
    public void cleanupOldSnapshots(String repositoryName, int retentionDays) throws IOException {
        GetSnapshotRequest getRequest = GetSnapshotRequest.of(g -> g
            .repository(repositoryName)
            .snapshot("*")
        );
        
        GetSnapshotResponse getResponse = client.snapshot().get(getRequest);
        
        LocalDateTime cutoffDate = LocalDateTime.now().minusDays(retentionDays);
        
        for (SnapshotInfo snapshot : getResponse.snapshots()) {
            if (snapshot.startTime() != null) {
                LocalDateTime snapshotTime = LocalDateTime.ofInstant(
                    Instant.ofEpochMilli(snapshot.startTime()), 
                    ZoneId.systemDefault()
                );
                
                if (snapshotTime.isBefore(cutoffDate)) {
                    DeleteSnapshotRequest deleteRequest = DeleteSnapshotRequest.of(d -> d
                        .repository(repositoryName)
                        .snapshot(snapshot.snapshot())
                    );
                    
                    client.snapshot().delete(deleteRequest);
                    System.out.println("åˆ é™¤æ—§å¿«ç…§: " + snapshot.snapshot());
                }
            }
        }
    }
    
    /**
     * è·å–å¿«ç…§çŠ¶æ€
     */
    public List<SnapshotStatus> getSnapshotStatus(String repositoryName) throws IOException {
        SnapshotStatusRequest request = SnapshotStatusRequest.of(s -> s
            .repository(repositoryName)
        );
        
        SnapshotStatusResponse response = client.snapshot().status(request);
        
        return response.snapshots().stream()
                .map(snapshot -> {
                    SnapshotStatus status = new SnapshotStatus();
                    status.setSnapshotName(snapshot.snapshot());
                    status.setState(snapshot.state());
                    status.setStartTime(snapshot.stats().startTimeInMillis());
                    status.setTotalSize(snapshot.stats().totalSize());
                    status.setProcessedSize(snapshot.stats().processedSize());
                    return status;
                })
                .collect(Collectors.toList());
    }
}
```

# äº”ã€æ€»ç»“ä¸æœ€ä½³å®è·µ

## ï¼ˆä¸€ï¼‰ä¼ä¸šçº§éƒ¨ç½²æœ€ä½³å®è·µ

**é›†ç¾¤æ¶æ„è®¾è®¡åŸåˆ™**
- é‡‡ç”¨ä¸“ç”¨ä¸»èŠ‚ç‚¹æ¶æ„ï¼Œç¡®ä¿é›†ç¾¤ç¨³å®šæ€§
- åˆç†è§„åˆ’æ•°æ®èŠ‚ç‚¹å’Œåè°ƒèŠ‚ç‚¹ï¼Œé¿å…å•ç‚¹æ•…éšœ
- æ ¹æ®æ•°æ®é‡å’ŒæŸ¥è¯¢è´Ÿè½½åˆç†è®¾ç½®åˆ†ç‰‡æ•°é‡
- å®æ–½å†·çƒ­æ•°æ®åˆ†ç¦»ç­–ç•¥ï¼Œä¼˜åŒ–å­˜å‚¨æˆæœ¬

**æ€§èƒ½ä¼˜åŒ–ç­–ç•¥**
- JVMå †å†…å­˜ä¸è¶…è¿‡32GBï¼Œæ¨èè®¾ç½®ä¸ºç‰©ç†å†…å­˜çš„50%
- ä½¿ç”¨G1åƒåœ¾å›æ”¶å™¨ï¼Œä¼˜åŒ–GCæ€§èƒ½
- åˆç†è®¾ç½®åˆ·æ–°é—´éš”å’Œåˆå¹¶ç­–ç•¥
- å®æ–½æŸ¥è¯¢ç¼“å­˜å’Œè¯·æ±‚ç¼“å­˜ä¼˜åŒ–

**å®‰å…¨é…ç½®è¦ç‚¹**
- å¯ç”¨X-Packå®‰å…¨åŠŸèƒ½ï¼Œé…ç½®SSL/TLSåŠ å¯†
- å®æ–½åŸºäºè§’è‰²çš„è®¿é—®æ§åˆ¶ï¼ˆRBACï¼‰
- é…ç½®å®¡è®¡æ—¥å¿—ï¼Œç›‘æ§å®‰å…¨äº‹ä»¶
- å¯¹æ•æ„Ÿæ•°æ®è¿›è¡Œå­—æ®µçº§åŠ å¯†

## ï¼ˆäºŒï¼‰è¿ç»´ç›‘æ§ä½“ç³»

**ç›‘æ§æŒ‡æ ‡ä½“ç³»**
- é›†ç¾¤å¥åº·çŠ¶æ€ï¼šçº¢/é»„/ç»¿çŠ¶æ€ç›‘æ§
- èŠ‚ç‚¹èµ„æºç›‘æ§ï¼šCPUã€å†…å­˜ã€ç£ç›˜ä½¿ç”¨ç‡
- æ€§èƒ½æŒ‡æ ‡ç›‘æ§ï¼šç´¢å¼•é€Ÿåº¦ã€æŸ¥è¯¢å»¶è¿Ÿã€ååé‡
- ä¸šåŠ¡æŒ‡æ ‡ç›‘æ§ï¼šæ–‡æ¡£æ•°é‡ã€ç´¢å¼•å¤§å°ã€é”™è¯¯ç‡

**å‘Šè­¦ç­–ç•¥**
- è®¾ç½®å¤šçº§å‘Šè­¦é˜ˆå€¼ï¼šè­¦å‘Šã€ä¸¥é‡ã€ç´§æ€¥
- å®æ–½æ™ºèƒ½å‘Šè­¦ï¼Œé¿å…å‘Šè­¦é£æš´
- å»ºç«‹å‘Šè­¦å¤„ç†æµç¨‹å’Œå‡çº§æœºåˆ¶
- å®šæœŸå›é¡¾å’Œä¼˜åŒ–å‘Šè­¦è§„åˆ™

**å¤‡ä»½æ¢å¤ç­–ç•¥**
- å®æ–½å®šæœŸè‡ªåŠ¨å¤‡ä»½ï¼Œä¿ç•™åˆç†çš„å¤‡ä»½å‘¨æœŸ
- æµ‹è¯•å¤‡ä»½æ¢å¤æµç¨‹ï¼Œç¡®ä¿æ•°æ®å¯æ¢å¤æ€§
- å®æ–½è·¨åœ°åŸŸå¤‡ä»½ï¼Œæé«˜ç¾éš¾æ¢å¤èƒ½åŠ›
- å»ºç«‹å¤‡ä»½ç›‘æ§å’Œå‘Šè­¦æœºåˆ¶

## ï¼ˆä¸‰ï¼‰ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®

1. **æ·±å…¥å­¦ä¹ Elasticsearchå†…æ ¸æœºåˆ¶**
   - ç†è§£Luceneåº•å±‚åŸç†
   - æŒæ¡åˆ†ç‰‡è·¯ç”±å’Œåˆ†é…ç®—æ³•
   - å­¦ä¹ æŸ¥è¯¢æ‰§è¡Œè®¡åˆ’ä¼˜åŒ–

2. **æ‰©å±•ç”Ÿæ€ç³»ç»Ÿ**
   - å­¦ä¹ ELK Stackå®Œæ•´è§£å†³æ–¹æ¡ˆ
   - æŒæ¡Beatsæ•°æ®é‡‡é›†å·¥å…·
   - äº†è§£Elastic APMåº”ç”¨æ€§èƒ½ç›‘æ§

3. **äº‘åŸç”Ÿéƒ¨ç½²**
   - å­¦ä¹ Kubernetesä¸Šçš„Elasticsearchéƒ¨ç½²
   - æŒæ¡Elastic CloudæœåŠ¡
   - äº†è§£å®¹å™¨åŒ–æœ€ä½³å®è·µ

4. **æœºå™¨å­¦ä¹ å’ŒAIé›†æˆ**
   - å­¦ä¹ Elasticsearchæœºå™¨å­¦ä¹ åŠŸèƒ½
   - æŒæ¡å¼‚å¸¸æ£€æµ‹å’Œé¢„æµ‹åˆ†æ
   - äº†è§£å‘é‡æœç´¢å’Œè¯­ä¹‰æœç´¢

é€šè¿‡æœ¬ç³»åˆ—æ•™ç¨‹çš„å­¦ä¹ ï¼Œæ‚¨å·²ç»æŒæ¡äº†ä»ElasticsearchåŸºç¡€æ¦‚å¿µåˆ°ä¼ä¸šçº§åº”ç”¨çš„å®Œæ•´çŸ¥è¯†ä½“ç³»ã€‚ç»§ç»­å®è·µå’Œæ·±å…¥å­¦ä¹ ï¼Œæ‚¨å°†èƒ½å¤Ÿæ„å»ºå’Œç»´æŠ¤å¤§è§„æ¨¡ã€é«˜æ€§èƒ½çš„Elasticsearché›†ç¾¤ï¼Œä¸ºä¼ä¸šæä¾›å¼ºå¤§çš„æœç´¢å’Œåˆ†æèƒ½åŠ›ã€‚

---

> **ç›¸å…³æ–‡ç« **
> - [å…¥é—¨çº§ï¼šåŸºç¡€æ¦‚å¿µä¸ç¯å¢ƒæ­å»º](./ã€å¤§æ•°æ®ã€‘Elasticsearch%208.x%20å…¥é—¨æŒ‡å—ï¼šåŸºç¡€æ¦‚å¿µä¸ç¯å¢ƒæ­å»º.md)
> - [è¿›é˜¶çº§ï¼šå¤æ‚æŸ¥è¯¢ä¸èšåˆåˆ†æ](./ã€å¤§æ•°æ®ã€‘Elasticsearch%208.x%20è¿›é˜¶æŒ‡å—ï¼šå¤æ‚æŸ¥è¯¢ä¸èšåˆåˆ†æ.md)
> - [ç³»åˆ—å¯¼èˆªï¼šElasticsearchå®Œæ•´å­¦ä¹ è·¯å¾„](./ã€å¤§æ•°æ®ã€‘Elasticsearch%208.x%20å…¨é¢å®æˆ˜æŒ‡å—ï¼šä»å…¥é—¨åˆ°ä¼ä¸šçº§åº”ç”¨.md)